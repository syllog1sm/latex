% vim: set textwidth=78 fo+=t :

\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{amsmath, amsthm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{tikz-dependency}
\usepackage{placeins}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{cancel}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}
\newcommand{\edittrans}{\textsc{edit}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\newcommand{\tuple}[1]{$\langle#1\rangle$}
\newcommand{\maybe}[1]{\textcolor{gray}{#1}}
\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\state}{\mathcal{S}}
\newcommand{\nmae}{\textsc{nmae}\xspace}
\newcommand{\pcfg}{\textsc{pcfg}\xspace}

\newcommand{\szero}{S0\xspace}
\newcommand{\nzero}{N0\xspace}

\newcommand{\szeroH}{S0$_h$\xspace}
\newcommand{\szeroHH}{S0$_{h2}$\xspace}
\newcommand{\szeroL}{S0$_L$\xspace}
\newcommand{\szeroLL}{S0$_{L2}$\xspace}
\newcommand{\szeroR}{S0$_R$\xspace}
\newcommand{\szeroRR}{S0$_{R2}$\xspace}
\newcommand{\szeroLzero}{S0$_{L0}$\xspace}
\newcommand{\szeroRzero}{S0$_{R0}$\xspace}

\newcommand{\nzeroL}{N0$_L$\xspace}
\newcommand{\nzeroLL}{N0$_{LL}$\xspace}
\newcommand{\nzeroLzero}{N0$_{L0}$\xspace}

\newcommand{\szeroRedge}{S0$_{re}$\xspace}
\newcommand{\szeroLedge}{S0$_{le}$\xspace}
\newcommand{\nzeroLedge}{N0$_{le}$\xspace}

\newcommand{\sparseval}{\textsc{sparseval}\xspace}

\title{Joint incremental disfluency detection and dependency parsing}

\author{
	Anonymous\\
  	Department\\
  	Institution\\
  	Address\\
  {\tt \small email }\\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

    We present the first joint model of dependency parsing and
    disfluency detection. An incremental beam-search model
    proves well suited to the task, achieving much better parsing and
    disfluency detection scores than previous joint models.

We then improve the model further, by introducing features targetting disfluency
detection, and a novel non-monotonic transition system designed for speech repairs.
Our final model achieves TODO\% $F$-measure at marking edited words; comparable
to the current state-of-the-art and better than previous incremental models.

\end{abstract}

% P1
\section{Introduction}

Most unscripted speech contains filled pauses (ums and uhs) and errors which are
usually edited on-the-fly by the speaker. Disfluency detection is the task of
marking these infelicities in spoken language transcripts. The task has some
immediate value, as disfluencies have been shown to make speech recognition output
much more difficult to read \citep{jones:03}, but has also been motivated as
a module in a natural language understanding pipeline, because disfluencies have
proven problematic for \pcfg parsing models.

One reason for this difficulty could be the effect of disfluencies on the size
of the grammar that the parser has to learn: even though only 5\% of the tokens
in the Switchboard treebank \citep{marcus:93} are disfluent, they increase
the number of production rules in the corpus by over 50\%.\footnote{With
no nodes labelled \textsc{edted}, \textsc{uh} or \textsc{prn}, there are
2,570 unique productions in sections 2 and 3 of Switchboard. With them,
there are 19,015. Punctuation and metadata tokens were removed before both counts.}

%The underlying problem may be that a \pcfg model incorporates assumptions that
%seem a poor fit for learning disfluent sentences, which are examples of linguistic
%performance, rather than linguistic competence.

Instead of a pipeline approach, we adopt a parsing model that does not rely on
a grammar and makes fewer linguistic assumptions.
The only previous work on dependency parsing of unscripted speech that we are aware
of is from \citet{jorgensen:07}, who ran the \textsc{malt} parser on the Switchboard
corpus. Since then, dependency parsers have improved in accuracy substantially,
and now rival the best \pcfg models.
%\footnote{The parsers described by
%\citet{zhang:11} and \citet{Charniak05a} both recover unlabelled Stanford basic
%dependencies with about 93.5\% accuracy on the Wall Street Journal.}

Our first experiment is to apply a state-of-the-art incremental dependency parser
`out-of-the-box', with disfluencies simply marked by giving them a distinct
dependency label. A structured perceptron parser with beam-search
\citep{zhang:cl11,zhang:11} predicted the heads of fluent words
with 89.7\% accuracy, and detected edited words with 75.5\% $F$-measure.
We then describe methods of customising the feature set and transition system
of the parser to deal with disfluencies, which together improve these scores
substantially.

Our disfluency features were designed to capture the
`rough copy' structure of some speech repairs, which motivates the
\citet{Johnson04a} noisy channel model.  These features improve edit detection
$F$-measure to 81.1\%.  The ease with which incremental dependency parsers can
incorporate arbitrary features is one of their principal advantages.

Our second improvement is a novel non-monotonic extension to the transition
system. It allows the parser to create temporary edges, in order to deal with
speech repairs where the head of an apparent constituent is disfluent, but some
of its apparent leftward children are not.

The joint system achieves competitive accuracy at both disfluency detection and
parsing. It also has other important advantages:

\begin{itemize}
    \item Because the parser is incremental, it does not require sentence segmentation
        as a pre-process. Segmenting spoken utterances is a difficult unsolved
        problem, that will probably benefit from syntactic information. Our parser
        is the first to make this possible.

    \item The system is also quite efficient, processing 488 tokens per second.
        Little work has been done on making disfluency detection systems efficient,
        which has greatly limited their use. Previous models have required either
        large ngram language models \citep{zwarts:11}, a \pcfg syntactic language
        model \citep{Johnson04a}, or several passes over the data \citep{qian:13}.

    \item Finally, by casting the problem of disfluency detection as a minimally
           different dependency parsing task, our system will be able to exploit
           future improvements to incremental dependency parsing —-- an area of
           research that is currently very active.
\end{itemize}

\section{Switchboard disfluency annotations}
\label{sec:swbd}
The Switchboard portion of the Penn Treebank \citep{marcus:93} consists of
telephone conversations between strangers about
an assigned topic.  Two annotation layers are provided: one for syntactic
bracketing (\textsc{mrg} files),
and one for disfluencies (\textsc{dps} files).  The disfluency layer marks
elements with little or no syntactic function, such as filled pauses and discourse
markers, and annotates speech repairs using the \citet{shriberg:94} system of
reparandum/interregnum/repair.

In the syntactic annotation, edited words are covered by a special node labelled
\textsc{edited}.
The idea behind this annotation is to mark text which, if
excised, will lead to a grammatical sentence.  The \textsc{mrg} files do not
mark other types of disfluencies, such as filled pauses (e.g. \emph{um}, \emph{uh}),
parentheticals (e.g. \emph{you know}) or explicit editing terms (e.g. \emph{i mean}).
We follow most previous work in restricting our attention to speech repairs.

The two layers have high but imperfect agreement over
what tokens they mark as repaired: the \textsc{dps} files call
32,310 tokens part of a reparandum, while 32,742 are under \textsc{edited} nodes in
the syntactic annotation, with 33,720 tokens marked disfluent in at least one layer.
We use the \textsc{mrg} layer, as we need syntactic annotations to train and
evaluate our joint syntactic parser.

\subsection{Dependency conversion}
\label{sec:deps}
As is standard in recent statistical dependency parsing work, we acquire our
gold-standard dependencies by converting phrase-structure trees.
We used the 2013-04-05 version of the Stanford dependency converter \citep{stanford_deps}.
At first we feared that the filled pauses, disfluencies and meta-data tokens in
the Switchboard corpus might disrupt the conversion process, by making it more
difficult for the converter to recognise the underlying production rules.

To test this, we prepared two versions of the corpus, both with oracle pre-processing:
one where \textsc{edited} nodes, filled pauses and meta-data were cleaned before
the trees were sent through the Stanford converter, and one where the disfluency
cleaning was performed after the dependency conversion. The resulting corpora
were largely identical: 99.54\% of unlabelled and 98.7\% of labelled dependencies
were the same.

The fact that the Stanford converter is quite robust to the disfluencies is good news
for our baseline model. In our \textsc{edit} model, the structure of disfluent
constituents is under-specified; but for the baseline model, we use the structure
output by the Stanford converter.

We follow previous work on disfluency detection in lower-casing the text and
removing punctuation and partial words (words tagged XX and words ending in
`-').  We also filter out one- and two-token sentences.
We found that two additional simple pre-processes improved our results: discarding
all `um' and `uh' tokens; and merging `you know' and `i mean' into single tokens.

These pre-processes can be completed on the input string without losing information:
none of the `um' or `uh' tokens are semantically significant, and
the bigrams \emph{you know} and \emph{you mean} have a dependency between the two
tokens over 99.9\% of the times they occur in the treebank.
%We found that `um'
%and `uh' were not useful clues about when speakers might become disfluent, and
%we doubted that there was any utility in predicting a syntactic head for them.
%More nuanced handling of speech-specific multi-word expressions seems an interesting
%topic for future work.  It would also be interesting to see whether other filler
%words could be detected with high enough accuracy to discard them as a pre-process.

While running our experiments, we noted that our accuracy on dependencies
typed \textsc{dep} was very low, and that these often reflected conversion errors.
More complicated conversion processes, which make use of the \textsc{dps} discourse
annotations, might alleviate this. The only thing we tried along these lines was
to relabel dependencies of words marked as discourse items with the label
\textsc{discourse}. The converter appears to assign this to words tagged \textsc{uh}
which was less consistently assigned than the discourse tags.
%We reserve more nuanced conversion strategies for future work.

\section{Transition-based dependency parsing}

A transition-based parser predicts the syntactic structure of a sentence incrementally,
by making a sequence of classification decisions.  We follow the architecture of
\citet{zhang:cl11}, who use beam-search for decoding, and the averaged
perceptron algorithm for learning.  Despite its simplicity, this type of parser
has produced highly competitive results on the Wall Street Journal: with the
extended feature set described by \citet{zhang:11}, the parser achieves 93.5\%
unlabelled attachment score on Stanford basic dependencies.  The \citet{Charniak05a}
reranking parser produces similar accuracy when its constituency trees are post-processed
by the Stanford dependency converter.

Briefly, the transition-based parser consists of a state (sometimes termed a
configuration) which is sequentially manipulated by actions chosen from a set
termed the transition system. We use the arc-eager transition system
\citep{nivre:03,nivre:cl}.

We use a notation in which the stack items are indicated by S$i$, with S0 being
the top of the stack, S1 the item previous to it and so on. Similarly, buffer
items are indicated as N$i$, with N0 being the first item on the buffer.
In the initial configuration the stack is empty, and the buffer contains the words
of the sentence followed by an artificial \textsc{root} token, as suggested by
\citet{nivre:squib}.
In the final configuration the buffer is empty and the stack contains the 
\textsc{root} token.
%The set of arcs in the final configuration is the parse tree.

There are four parsing actions (\textbf{S}hift, \textbf{L}eft-Arc,
\textbf{R}ight-Arc and Re\textbf{d}uce, abbreviated as S,L,R,D respectively)
that manipulate stack and buffer items. The Shift action pops the first item
from the buffer and pushes it on the stack (the Shift action has a natural
precondition that the buffer is not empty, as well as a precondition that
\textsc{root} can only be pushed to an empty stack). The Right-Arc action is
similar to the Shift action, but it also adds a dependency arc (S0, N0),
with the current top of the stack as the head of the newly pushed item (the Right
action has an additional precondition that the stack is not empty). The Left-Arc
action adds a dependency arc (N0, S0) with the first item in the buffer as the
head of the top of the stack, and pops the stack (with a precondition that the
stack and buffer are not empty, and that S0 is not assigned a head yet). Finally,
the Reduce action pops the stack, with a precondition that the stack is not empty
and that S0 is already assigned a head.

We extend this set with a new transition, \textsc{edit}, which we describe in
Section \ref{sec:edittrans}.  The transition pops S0 from the stack, and marks
it and its rightward subtree as disfluent, but returns its leftward children to
the stack to be reprocessed.  Arcs to and from disfluent words are deleted.

\section{Features for the joint parser}

Our baseline parser uses the feature set described by \citet{zhang:11}.
The feature set contains TODO templates that mostly refer to the properties of
12 \emph{context tokens}: the top of the stack (\szero), its two leftmost and
rightmost children (\szeroL, \szeroLL, \szeroR, \szeroRR), its parent and
grand-parent (\szeroH, \szeroHH), the first word of the buffer and its two leftmost
children (\nzero, \nzeroL, \nzeroLL), and the next two words of the buffer (N1, N2).

Atomic features consist of the word, part-of- speech tag, or dependency label
for these tokens, and feature templates often consist of multiple feature atoms
in combination.  The feature set also considers the string-distance between \szero
and \nzero, and the left and right valencies
(total number of children) for \szero and \nzero, as well as the set of labels
assigned in their subtrees. We restrict the label set features to the first and
last 2 children for implementation efficiency, as we found this had no effect on
accuracy. Numeric features (for distance and valency) are binned by single
increments, with a final bin for 5+.  The only bi-lexical feature template pairs
the words of \szero and \nzero.
There are also ten tri-tag templates, which consider the
\pos tag of \szero, \nzero, and various other context tokens.
We refer the reader to \citet{zhang:11} for the specific feature
templates used.

We follow \citet{zhang:cl11} in subtyping the Left- and Right-Arc transitions
by dependency label.  We added additional label features to the baseline feature
set, as we found that disfluency detection errors often resulted in ungrammatical
label combinations.  The additional templates combine the \pos tag of \szero with
two or three labels from its left and right subtrees. We refer the reader to the
supplementary material for the exact templates added.

\subsection{Brown cluster features}

The Brown clustering algorithm \citep{brown:92} provides a well known source
of semi-supervised features. The clustering algorithm is run over a large sample
of unlabelled data, to generate a type-to-cluster map. This mapping is then used
to generate features that sometimes generalise better than lexical features,
and are helpful for out-of-vocabulary words \citep{turian:10}.

\citet{koo:10} found that Brown cluster features greatly improved the performance
of a graph-based dependency parser. On our transition-based parser, Brown cluster
features bring a small but statistically significant improvement on the \textsc{wsj}
task (0.1-0.3\% \textsc{uas}).  Other developers of transition-based parsers
seem to have found similar results (personal communication).

Since a Brown cluster type-mapping computed by \citet{liang:05} is easily
available\footnote{\url{www.metaoptimize.com/projects/wordreps}}, the features
are very simple to implement and cheap to compute, so we see little reason not to include them
in the parser.

Our templates follow \citet{koo:10} in including features for full clusters as
well as cluster prefixes. We adapt their templates to transition-based parsing
by replacing `head' with `item on top of the stack' and `child' with `first word
of the buffer'. The exact templates can be found in the supplementary material.

\subsection{Rough copy features}

\citet{Johnson04a} point out that in speech repairs, the repair is often a rough
copy of the reparandum.  The simplest case of this is where the repair is a single
word repetition. In more complex cases, some of the wording or grammatical
structure will be the same, and the rest will be different.  Common cases
are for the repair to add a suffix or prefix to the reparandum.

To capture this regularity, we first extend the feature-set to track three new
\emph{context tokens}, by their index in the sentence:\footnote{As is common
in this type of parser, our implementation has a number of vectors for properties
that are defined before parsing, such as word forms, \textsc{pos} tags, Brown
clusters, etc. The index allows features considering any of these
properties to be computed.}
\begin{enumerate}
    \item \szeroRedge : The rightmost edge of \szero 's yield;
    \item \szeroLedge : The leftmost edge of \szero 's yield;
    \item \nzeroLedge : The leftmost edge of \nzero 's yield.
\end{enumerate}

The \emph{yield} of a word is the span of words headed by it.  If a word has
no leftward children, it will be its own left-edge, and similarly it will be
its own rightward edge if it has no rightward children. Note that the token
\szeroRedge is necessarily immediately before \nzeroLedge, unless some of the
tokens between them are disfluent.

These features can be computed naively by traversing the parse tree for each
instance. However, this process would not be linear in complexity with respect
to sentence length.  Instead, we maintain two additional vectors, for left
and right yield edges, and update them during parsing.  To do this, we exploit
the fact that our features only consider the yield-edges of the tokens currently
at S0 and N0, as disfluency processing takes place between these two tokens.
We can therefore let the yield edges for tokens deeper in the stack
or that have been popped from the stack go out of date.  It turns out that the
yield edges for S0 and N0 can be maintained very cheaply:

\begin{itemize}
\item The Left-Arc move makes \szero the leftmost child of \nzero. The left yield-edge of
\nzero is therefore the left yield-edge of \szero.
\item The Right-Arc move makes \nzero the rightmost child of \szero. \nzero cannot have a rightward subtree, so it must
be the rightmost child of \szero. After a Right-Arc, \nzero will also become
new rightmost edge of all ancestors of \szero.
\item When the Reduce move is used, we set the right yield-edge of S1 to be the
      right yield-edge of \szero.
\item The Shift move does not create any dependencies, so yield-edges are unchanged.
\end{itemize}

Having computed the yield edges, we calculate a simple form of rough-copy feature using the \szeroLedge and \nzeroLedge tokens.  The features ask:

\begin{itemize}
    \item How long is the \emph{prefix word match} between \szeroLedge...\szero
          and \nzeroLedge...\nzero?
    \item How long is the \emph{prefix POS tag match} between \szeroLedge...\szero
          and \nzeroLedge...\nzero?
    \item Do the words in \szeroLedge...\szero and \nzeroLedge...\nzero match
          exactly?
    \item Do the POS tags in \szeroLedge...\szero and \nzeroLedge...\nzero match
          exactly?
\end{itemize}

For the prefix-length features, we adopt the simple discretisation strategy of
introducing a different binary-valued feature for each value up to 5, and another
bin for 5+.\footnote{We also tried the more nuanced strategy of having a bin for each
\emph{range} up to 5, so that the bins ask e.g. `Is the prefix \emph{at
least} two words long?', instead of `Is the prefix \emph{exactly} two words long?'.
This turned out to make no difference, so we used the simpler scheme.}

\subsection{Match features}

This class of features ask which pairs of the \emph{context tokens} match, in
word or \pos tag.  The context tokens in the \citet{zhang:11} feature set
are the top of the stack (\szero), its head and grandparent (\szeroH, \szeroHH),
its two left- and rightmost children (\szeroL, \szeroLL, \szeroR, \szeroRR), the
first three words of the buffer (\nzero, N1, N2), and the two leftmost children
of \nzero (\nzeroL, \nzeroLL).
We extend this set with the \szeroLedge, \szeroRedge and \nzeroLedge tokens
described above, and also the first left and right child of \szero and \nzero
(\szeroLzero, \szeroRzero, \nzeroLzero).

All up, there are 18 context tokens, so ${18 \choose 2}~=~153$ token pairs.
For each pair of these tokens, we add two binary features, indicating whether the
two tokens match in word form or \pos tag.  We also have two further classes of
features: if the words do match, a feature is added indicating the word form;
if the tags match, a feature is added indicating the tag. These finer grained
versions help the model adjust for the fact that some words can be duplicated
in grammatical sentences (e.g. `that that'), while most rare words cannot.

\subsection{Edited neighbour features}

Disfluencies are usually
string contiguous, even if they do not form a single constituent.  We help the
parser exploit this by adding features asking whether the word or pair of words
immediately before \nzero were marked disfluent. A similar pair of features is
added for the word or pair of words immediately after \szero.

\section{A non-monotonic Edit transition}
\label{sec:edittrans}
\begin{figure}
    \small
\begin{dependency}[theme=simple, segmented edge]
    \begin{deptext}[column sep=.075cm, row sep=.1ex]
    Pass \& me \& the \& red \& rectangle \& uh I mean \& square \\
    \end{deptext}
    \depedge[edge unit distance=0.9ex]{1}{2}{}
    \depedge[dotted, edge below, edge unit distance=0.8ex]{5}{3}{}
    \depedge[dotted, edge below, edge unit distance=0.8ex]{5}{4}{}
    \depedge[dotted, edge below, edge unit distance=0.8ex]{1}{5}{}
    \depedge[edge unit distance=0.7ex]{7}{3}{}
    \depedge[edge unit distance=0.7ex]{7}{4}{}
    \depedge[edge unit distance=0.6ex]{1}{7}{}
    \end{dependency}
    \caption{\small Example where `temporary edges' between the reparandum and the
    fluent sentence complicate parsing. In order to learn a projective tree
    for the sentence, the parser would have to learn not to assign the dotted
    edges below. We instead make the parsing process non-monotonic.
\label{fig:rectangle}}
\end{figure}

One of the reasons disfluent sentences are hard to parse is that there are often
syntactic relationships between words in the reparandum and the fluent sentence.
When these relations are added to the `true' dependencies between fluent words,
the resulting structure is not necessarily a projective tree.

Figure \ref{fig:rectangle} shows a simple example.
The repair \emph{square} replaces the \emph{reparandum} rectangle, and must be
attached to the rest of the fluent sentence.
The problematic dependencies are shown in dotted lines, below the words.
These arcs are not exactly correct, as they aren't part of the intended semantics,
but they are also not exactly incorrect, in the way that an arc from \emph{a}
to \emph{pass} or \emph{me} would be.

For an incremental model to avoid these arcs, it will have to treat \emph{rectangle}
as disfluent from the time it is first at the head of the buffer.  This seems
very difficult --- the parser is very susceptible to
treating \emph{the red rectangle} as a constituent.  It's not even clear that
that should be considered incorrect.  We suspect an eye-tracking experiment would
reveal that the human processor (which is clearly incremental) would also form
such a constituent.  TODO: citation.

Psycholinguistic models of human sentence processing have long posited
\emph{repair} mechanisms \citep{FrazierRayner1982}.  Recently, \citet{honnibal:13}
showed that a limited amount of `non-monotonic' behaviour can 
improve an incremental parser's accuracy on some types of garden-path constructions.
We here introduce a non-monotonic transition, \textsc{edit}, to handle speech
repairs. 

The \textsc{edit} transition does the following:

\begin{enumerate}
    \item Mark \szero and its \emph{rightward yield} as disfluent;
    \item Delete all arcs to and from \szero
    \item Place the former leftward children of \szero back onto the stack.
    \item Pop \szero from the stack;
\end{enumerate}

Why revisit the leftward children, but not the right? The logic here is that we
are concerned about dependencies which might be mirrored be- tween the reparandum
and the repair. The right-ward subtree of the disfluency might well be incorrect,
but if it is, it would still be incorrect if the word on top of the stack weren't
disfluent --– so we regard these as parsing errors that we will train our model
to avoid. In contrast, avoiding the leftward arcs would require the parser to
predict that the head is disfluent, when it has not necessarily seen any evidence
indicating that.

The transition is \emph{non-monotonic} in the sense that it can delete arcs
that the parser had previously proposed, and replace tokens onto the stack for
further processing.  We will now present a detailed motivating example for the
transition, before describing how the model is trained, and how we accomodate
the fact that transition histories can now be of varying lengths.

\subsection{Motivating example}

\begin{figure*}
    \small
    \centering
    \begin{tabular}{llc}
    % 1 R
        1 & $\langle R \rangle $ & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \& noticed \& that \& $\|$ the \& \emph{monthly} \& \emph{salary} \& starting \& average \& monthly \& \emph{salary} \& salary \& for \& engineers \hfill \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        \wordgroup{1}{3}{3}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}
    \end{dependency}\\[-2.0ex]
    % 2 SS
    2 & $\langle DSS \rangle$ & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]

            I \& noticed \& that \& the \& \emph{monthly} \& $\|$ \emph{salary} \& starting \& average \& monthly \& \emph{salary} \& salary \& for \& engineers \hfill \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        \wordgroup{1}{4}{4}{}
        \wordgroup{1}{5}{5}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}
    \end{dependency}\\[-2.0ex]
 
    % 3 LL
    3 & $\langle LL \rangle$ & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \& noticed \& that \& the \& \emph{monthly} \& $\|$ \emph{salary} \& starting \& average \& monthly \& \emph{salary} \& salary \& for \& engineers \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        %\wordgroup{1}{4}{4}{}
        %\wordgroup{1}{5}{5}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}
        \depedge{6}{5}{}
        \depedge{6}{4}{}
    \end{dependency}\\[-2.0ex]
 
    % 4 R
4 & $\langle R \rangle$ & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \& noticed \& that \& the \& \emph{monthly} \& \emph{salary} \& $\|$ starting \& average \& monthly \& \emph{salary} \& salary \& for \& engineers \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        %\wordgroup{1}{4}{4}{}
        %\wordgroup{1}{5}{5}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}
        \depedge{6}{5}{}
        \depedge[edge unit distance=0.9ex]{6}{4}{}

        \depedge[edge unit distance=0.3ex, edge below]{2}{6}{}
        \wordgroup{1}{6}{6}{}
    \end{dependency}\\[-2.0ex]
 
    % 5 SSSLLL
5 & $\langle SSSLLL \rangle$ & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \& noticed \& that \& the \& \emph{monthly} \& \emph{salary} \& starting \& average \& monthly \& $\|$ \emph{salary} \& salary \& for \& engineers \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        %\wordgroup{1}{4}{4}{}
        %\wordgroup{1}{5}{5}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}
        \depedge{6}{5}{}
        \depedge[edge unit distance=0.9ex]{6}{4}{}

        \depedge[edge unit distance=0.3ex, edge below]{2}{6}{}
        \wordgroup{1}{6}{6}{}

        \depedge[edge unit distance=0.9ex]{10}{9}{}
        \depedge[edge unit distance=0.8ex]{10}{8}{}
        \depedge[edge below, edge unit distance=0.3ex]{10}{7}{}
    \end{dependency}\\[-2.0ex]
    % 6 E
    6 & $\langle \textbf{E} \rangle$ & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \& noticed \& that \& the \& \emph{monthly} \& \cancel{\emph{salary}} \& starting \& average \& monthly \& $\|$ \emph{salary} \& salary \& for \& engineers \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        \wordgroup{1}{4}{4}{}
        \wordgroup{1}{5}{5}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}

        \depedge[edge unit distance=0.9ex]{10}{9}{}
        \depedge[edge unit distance=0.8ex]{10}{8}{}
        \depedge[edge below, edge unit distance=0.3ex]{10}{7}{}
    \end{dependency}\\[-2.0ex]
 
    % 7 E
 7 & $\langle \textbf{E} \rangle$ & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \& noticed \& that \& the \& \cancel{\emph{monthly}} \& \cancel{\emph{salary}} \& starting \& average \& monthly \& $\|$ \emph{salary} \& salary \& for \& engineers \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        \wordgroup{1}{4}{4}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}

        \depedge[edge unit distance=0.9ex]{10}{9}{}
        \depedge[edge unit distance=0.8ex]{10}{8}{}
        \depedge[edge below, edge unit distance=0.3ex]{10}{7}{}
    \end{dependency}\\[-2.0ex]
 
    % 8 LS
 8 & $\langle LS \rangle$ & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \& noticed \& that \& the \& \cancel{\emph{monthly}} \& \cancel{\emph{salary}} \& starting \& average \& monthly \& \emph{salary} \& $\|$ salary \& for \& engineers \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        %\wordgroup{1}{4}{4}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}

        \depedge[edge unit distance=0.9ex]{10}{9}{}
        \depedge[edge unit distance=0.8ex]{10}{8}{}
        \depedge[edge below, edge unit distance=0.3ex]{10}{7}{}
        \depedge[edge below, edge unit distance=0.3ex]{10}{4}{}
        \wordgroup{1}{10}{10}{}
    \end{dependency}\\[-2.0ex]
 
    % 9 E
    9 & $\langle \textbf{E} \rangle$ & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \& noticed \& that \& the \& \cancel{\emph{monthly}} \& \cancel{\emph{salary}} \& starting \& average \& monthly \& \cancel{\emph{salary}} \& $\|$ salary \& for \& engineers \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        \wordgroup{1}{4}{4}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}

        \wordgroup{1}{7}{7}{}
        \wordgroup{1}{8}{8}{}
        \wordgroup{1}{9}{9}{}
        %\depedge[edge unit distance=0.9ex]{10}{9}{}
        %\depedge[edge unit distance=0.8ex]{10}{8}{}
        %\depedge[edge below, edge unit distance=0.3ex]{10}{7}{}
        %\depedge[edge below, edge unit distance=0.3ex]{10}{4}{}
        %\wordgroup{1}{10}{10}{}
    \end{dependency}\\[-2.0ex]
     10 &    & \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \& noticed \& that \& the \& \cancel{\emph{monthly}} \& \cancel{\emph{salary}} \& starting \& average \& monthly \& \cancel{\emph{salary}} \& $\|$ salary \& for \& engineers \\
        \end{deptext}
        \wordgroup{1}{2}{2}{}
        \depedge{2}{1}{}
        \depedge{2}{3}{}

        \depedge[edge unit distance=0.9ex]{11}{9}{}
        \depedge[edge unit distance=0.8ex]{11}{8}{}
        \depedge[edge below, edge unit distance=0.3ex]{11}{7}{}
        \depedge[edge below, edge unit distance=0.3ex]{11}{4}{}
        \depedge[edge unit distance=0.3ex]{2}{11}{}
        \depedge{11}{12}{}
        \depedge{12}{13}{}
        %\wordgroup{1}{10}{10}{}
    \end{dependency}\\[-2.0ex]
 
    \end{tabular}
    \caption{\small Example from the development data where the Edit transition
    improves accuracy. Words on the stack are shown circled, while words marked
    disfluent with the Edit transition are struck-through. The start of the
    buffer is marked with $\|$}.
\end{figure*}

Figure 2 shows part of a sentence from the development data that contains several
difficult disfluencies, of the type the \edittrans transition is designed to address.
Each numbered line shows a state along the final path predicted by the
parser.\footnote{This discussion is intended for illustration, and may not reflect
how our final model actually behaves, or why.  Our model is evaluated
statistically in Section \ref{sec:results}.}
The actions performed to transform the previous state into the current one are
listed in angle brackets. Words on the stack are circled, and dependencies are
shown as arcs from the head to the child. The $\|$ symbol shows where the buffer
begins.

Line 1 shows a state resulting from a Right-Arc (R) from \emph{noticed}
to \emph{that}. Previously, the parser had performed a Left-Arc from \emph{noticed}
to \emph{I}, which is no
longer on the stack. The italicised words in the buffer are disfluent, and will
be deleted using the \edittrans transition. Deletions will be marked by strike-through.

Line 2 shows the state after the Shift move (S)
has pushed \emph{the} and \emph{monthly} onto the stack. It would be correct to
apply the \edittrans transition to this configuration, but that is only one of
several moves that we would consider to be correct, or `zero-cost'.\footnote{As
we explain in Section \ref{sec:oracle}, our training process does not use a single
canonical derivation as its gold-standard.  Instead, our training process is adjusted
for the \emph{spurious ambiguity} we introduce via the non-monotonic transition
system, using a dynamic oracle \citep{goldberg:12}.}

The model selects a different zero-cost action, Left-Arc, which pops \emph{monthly}
and attaches it to \emph{salary}. The parser then performs another Left-Arc,
attaching the fluent word the to the disfluent word salary, resulting in the
state at Line 3.

The critical difference between the \edittrans model and the baseline is that
this type of arc is also zero-cost in the \edittrans model, because we will later
put \emph{the} back onto the stack. Our argument is that from the configuration
in Line 2, the Left-Arcs are the natural choice, and a loss-function that penalises
this move will be very difficult to learn. The state resulting from these two
actions is shown in Line 3.

The parser’s next action is a Right-Arc from the fluent \emph{noticed} to the
disfluent \emph{salary}, pushing \emph{salary} onto the stack. This action is also
zero-cost, even though it creates an erroneous arc, because we’ll later have the chance to
delete the arc via the \edittrans transition. The parser then performs a series of
Shift and Left-Arc moves, attaching the fluent tokens \emph{starting}, \emph{monthly}
and \emph{average} to the second disfluent \emph{salary} token. These arcs are all
also zero-cost, and are further examples of the type of arc that motivates the
\edittrans transition.

The parser is now (Line 4) in a state from which it can confidently predict that
the \emph{salary} token on top of the stack is disfluent, as there is a matching
word at the start of the buffer that isn’t one of the small set of words that are
often repeated in the training data. The word-match features will also find a match
between the tokens S0L0 and N0L0 (first left child of S0 and first left child
of N0).

Line 5 shows the state that results from the \edittrans transition. It marks
\emph{salary} as disfluent, deletes all arcs to and from it, pops it from the
stack, and restores its leftward children to the stack —-- in this case, monthly.
The parser’s next action is to mark monthly as disfluent. A Left-Arc, Shift, or
even Right-Arc would all have been zero-cost from this state, because the word
on top of the stack and the word at the start of the buffer are both disfluent.
Instead, the word-match between S0 and N0L0, combined with the fact that the token
after S0 was marked disfluent, leads the parser to mark monthly as also disfluent.

The parser next applies the Left-Arc transition from the disfluent salary to the
fluent the, again at zero-cost, even though the arc is incorrect. The Shift move
then puts the parser in the state shown on Line 7: the disfluent salary is at
the top of the stack, with several fluent words as leftward chil- dren, and its
repair is at the start of the buffer. The word-match feature between S0 and N0
fires, encouraging the parser to select the EDIT transition, which restores the,
starting, average and monthly to the stack, resulting in the state on Line 9.

Having successfully marked the disfluencies, the parser performs a series of
Left-Arcs, before correctly attaching salary to noticed via the Right-Arc,
and happens to correctly attach the preposi- tional phrase as well.

\subsection{Dynamic oracle training}

An essential component when training a transition-based parser is an oracle which,
given a gold-standard tree, dictates the sequence of moves a parser should make
in order to derive it. Traditionally, these oracles are defined as functions from
trees to sequences, mapping a gold tree to a single sequence of actions deriving it,
even if more than one sequence of actions derives the gold tree. During training,
the model suffers a loss if its prediction departs from this single 
sequence, even if it lies on an alternate path to the gold-standard tree.

The oracle that \citet{goldberg:12} define, which they term a \emph{dynamic} oracle,
instead calculates how many gold-standard arcs will be newly unreachable after a
given action has been performed. This allows the model to be trained with a
loss function that refers to the dependency structure, instead of the transition
history.  

Another way to look at this is to say that the derivation structure is left latent,
instead of being learnt from direct supervision.  
We therefore adapt the learning
algorithm of \citet{zettlemoyer:07}, in order to use the dynamic oracle with
a global model, which has not yet been done.

Briefly, \citeauthor{zettlemoyer:07} train a global perceptron model to predict
lambda calculus representations from sentence strings, with Combinatory
Categorial Grammar derivations as a latent structure.  Ignoring the parts of
their algorithm which deal with the lexicon, and generalising a little, the algorithm is:

\begin{enumerate}
\item Search for the model's best hypothesis given the input.
\item If the hypothesis satisfies the supervision constraints, move on to the
      next example.
\item Otherwise, search again, this time constraining the search to ensure the
    candidate \emph{will} satisfy the supervision criteria.
\item Perform a weight update for the model hypothesis and the gold-standard
      candidate.
\end{enumerate}

For us, the supervision constraint is that each action must be zero-cost.  So,
we first run our beam-search decoder, just as at parse time (Step 1),
except that we consult the gold-standard dependencies and disfluency information
to compute a cost for each candidate in the beam, via the dynamic oracle (for Step 2).
If the top-ranked derivation is zero-cost, we perform no update (Step 2).
Otherwise, we search again, but
ensure that no actions with non-zero cost are introduced into the beam (Step 3).
Our weight update is the standard one, from \citet{collins:02}: count how often
each feature/class pair occurs in the gold-standard and predicted sequences,
and update the weights by their difference.

Note that neither search process is exact, and in fact we may find a gold-standard
candidate that scores higher than the predicted candidate, if the gold candidate
were pruned during search.  If handled naively, this would trigger what \citet{huang:12}
refer to as an `invalid update'.  The early-update strategy of \citet{collins:02}
is one way to avoid this, but we adopt the maximum violation update strategy
described by \citet{huang:12} instead, as it requires fewer training iterations.

\subsection{Path length normalisation}

One problem introduced by the \textsc{edit} transition is that the number of
actionss applied to a sentence is no longer constant --- it is no longer guaranteed
to be $2n-1$, for a sentence of length $n$. When the \textsc{edit} transition is
applied to a word with leftward children, those children are returned to the stack,
and processed again.  This has little to no impact on the algorithm's empirical
efficiency, although its worst-case complexity is no longer linear, but it does
pose a problem for decoding.

The perceptron model tends to assign
high scores to its top prediction.\footnote{Recall that each training instance
involves adding 1.0 to the weights for the correct class, and -1.0 to the weights
for the predicted class, over several iterations. The correct class stays the
same, while the predictions may vary, so the negative weight tends to be split
among multiple classes. This is why the top prediction on an instance almost
always has a high positive weight.}
We thus observed a problem when comparing paths of different lengths, at the end
of the sentence. Candidates that proposed \textsc{edit} transitions were longer,
so the sum of their scores tended to be higher.

The same problem has been observed during incremental \textsc{pcfg} parsing,
by \citet{zhang:13}.  They introduce an additional transition, \textsc{idle},
to ensure that paths are the same length. So long as one candidate in the beam
is still being processed, all other candidates apply the \textsc{idle} transition.

We adopt a simpler solution.  We normalise the figure-of-merit, which is used to rank
candidates in the beam, by the length of the candidates' transition history. The
new figure-of-merit is the
arithmetic mean of the candidate's transition scores.

Interestingly, \citet{zhang:13} report that they tried exactly this, and that it
was less effective than their solution. We found the opposite: the features
associated with the \textsc{idle} transition were uninformative (the state is at
termination, so the stack and buffer are empty), and had nothing to do with how
many edit transitions were earlier applied.

\section{Part-of-speech tagging}

In the absence of an established way of doing joint part-of-speech tagging and
dependency parsing, we adopt the standard pipeline strategy.  Most transition-based
parsers use a structured averaged perceptron model with beam-search for tagging,
as this model achieves competitive accuracy and matches the standard dependency
parsing architecture. Our tagger also uses this architecture.

We performed some additional feature engineering for the tagger, in order to
improve its accuracy given the lack of case distinctions and punctuation in
the data. Our additional features use two sources of unsupervised information.
First, we follow the suggestion of \citet{manning:11} in using Brown cluster
features to improve the tagger's accuracy on unknown words. Second, we compensate
for the lack of case distinctions by including features that ask what percentage
of the time a word form was seen title-cased, upper-cased and lower-cased in the
Google Web1T corpus. 

Where most previous work uses cross-fold training for the tagger, to ensure that the
parser is trained on tags that reflect run-time accuracies, we do online training
of the tagger alongside the parser, using the current tagger model to produce
tags during parser training.  This had no impact on parse accuracy, and made it
slightly easier to develop our tagger alongside the parser.

The tagger achieved 96.5\% accuracy on the development data, but when we ran our
final test experiments, we found its accuracy had dropped to 96.0\%, indicating
some over-fitting during our feature engineering.  On the development data,
our parser accuracy improves by about 1\% when gold-standard tags are used.

\section{Experiments}

We use the Switchboard portion of the Penn Treebank \citep{marcus:93}, as
described in Section \ref{sec:swbd}, to train our joint parsing and disfluency
models and evaluate them on dependency parsing and disfluency detection. The
pre-processing and dependency conversion are described in Section~\ref{sec:deps}.

Our parser evaluation is modelled on the \sparseval metric \citep{sparseval}.
However, we wanted to use the Stanford dependency scheme, to keep the system
close to standard practice for Wall Street Journal dependency parsing, so we
do not use the \sparseval tool itself.  Our evaluation script is distributed
in the supplementary materials.

We follow \sparseval in not including arcs to speech repairs or other disfluent
words in the parser evaluation (i.e. we don't evaluate the heads of disfluent
words).  Links to fluent words incorrectly marked as disfluent are counted as
incorrect.
%A small advantage of this evaluation is that systems are always
%evaluated on the same number of arcs.  This allows us to report our accuracies
%in the same format as the rest of the recent dependency parsing literature,
%instead of introducing precision/recall/$F$-measure scores.

We follow \citet{Johnson04a} in restricting our disfluency evaluation to speech
repairs, which we identify as words that have a node labelled \textsc{edited}
as an ancestor.  Unlike most other disfluency detection research, we train only
on the \textsc{mrg} files, giving us TODO words of training data instead of
the usual TODO.

All parsing models were trained for 15 iterations, using the maximum violation
update strategy described by \citet{huang:12}.  We found this to produce equivalent
model scores to the standard early update strategy \citep{collins:02}, but with
fewer training iterations.

We test for statistical significance in our results by training 20 models for
each experimental configuration, using different random seeds. The random seeds
control how the sentences are shuffled during training, which the perceptron
model is quite sensitive to.  We use the Wilcoxon non-parametric test.

\subsection{Parsing with oracle disfluencies}

\label{sec:oracle}
This experiment investigates how much disfluencies disrupt parsing accuracy,
by training and evaluating a parser on a version of Switchboard with oracle
disfluency detection.  Every word labelled as a disfluency\footnote{That is, those
part of the yield of a node labelled \textsc{edited} in the \textsc{ptb} annotation.}
is removed, prior to training and evaluation.

\subsection{Parsing with a disfluency pipeline}
\label{sec:pipeline}
The accuracy of incremental dependency parsers is well established on the Wall
Street Journal, but there are no dependency parsing results in the literature
that make it easy to put our joint model's parsing accuracy into context.

We therefore compare our joint model to a pipeline approach, using a strong 
syntax-based disfluency detection system.  We used the \citet{Johnson04a}
noisy-channel model, with the \citet{Charniak01b} parser as a syntactic language
model.  We chose this system because it is an extension of the \citet{Charniak01a}
boosting classifier, which is the only disfluency detection system studied as
a pre-processor to a syntactic parser.  The accuracy of the model has improved
since \citet{Johnson04a}, as it now includes the additional features described by
\citet{zwarts:11}.  In fact, its accuracy is higher than the previous state-of-the-art.

The parser is first trained on the cleaned corpus, as described in section
\ref{sec:oracle}. The \citet{Charniak01a} is cross-fold trained on the Switchboard
corpus, and then scores candidates produced by the \citet{Johnson04a} noisy-channel
model.  A MaxEnt reranker then considers the channel model and language model scores,
in addition to features described by \citet{zwarts:11}. We then parse the string
that results from omitting words marked as disfluent by top-ranked hypothesis.

One concern we had with this architecture was that the model is trained on text
with oracle pre-processing, but is run on text that the pipeline has cleaned
imperfectly.  Since cross-training the repairs pipeline is quite computationally
expensive, as a quick test we ran a model trained with no disfluencies over text
that had not been pre-processed at all.  This model was only 0.2\% less accurate
than one trained on the disfluent text.  This convinced us that there would be
little benefit in departing from the \citet{Charniak01a} architecture to do
cross-fold training for the baseline pipeline model.

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr|rr}
        & P & R & F & \textsc{uas} & \textsc{las} \\
        \hline \hline
                    &	P	&	R	&	F	&	U	&	L \\
Beam parser$_{k=16}$ &	79.7	&	69.6	&	74.3	&	89.7	&	86.8 \\
+Clusters            &	79.7	&	69.9	&	74.5	&	89.8	&	86.9 \\
+Features            &	85.3	&	77.2	&	81.0	&	90.2	&	87.2 \\
+Transition          &	91.5	&	80.3	&	85.6	&	90.7	&	87.7 \\
\hline
Pipe (Sec. \ref{sec:pipeline})  & 89.4 & 80.3 & 84.6 & 90.5 & 87.4 \\
\hline       
Oracle (Sec. \ref{sec:oracle})  & 100 & 100 & 100 & 91.6    & 88.5 \\
\hline
    \end{tabular}
\caption{Development accuracies for disfluency detection and parse accuracy.}
\end{table}



\section{Results}
\label{sec:results}

\begin{table}
\centering
\begin{tabular}{l|r}
    System          & \textsc{uas} \\
    \hline \hline
    Pipeline parser & ?? \\
    Joint parser    & ?? \\
    \hline
\end{tabular}
\caption{Parser evaluation on \textsc{swbd} test set. The pipeline parser, described
    in Section \ref{sec:pipeline}, uses a state-of-the-art syntax-based noisy
    channel model, followed by our parser trained on clean text. This yields
    significantly lower parse accuracies than our best joint model, which includes
additional features and the non-monotonic Edit transition.\label{tab:parseval}}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{l|r}
        System                      & Disfluency $F$ \\
        \hline \hline
        Xian and Liu (2013)         & ?? \\
        Noisy channel pipeline      & ?? \\
        Joint parser                & ?? \\
        \hline
    \end{tabular}
    \caption{Disfluency evaluation on the Switchboard test set. Our joint parser-based
             model compares favourably 
             against the noisy channel model used for our pipeline experiments
             (Section \ref{sec:pipeline}) and the previous state-of-the-art system.
         \label{tab:dfl_eval}}
\end{table}

\section{Analysis of Edit behaviour}

In order to understand how the parser applies the \edittrans transition, we
collected some additional statistics over the development data.  Unlike our
parse and disfluency results, we only examined the output of one model, trained
with a single random seed, as robustness was less important.

Over the development data, the parser predicted 2,459 \edittrans transitions,
which together marked 2,702
words disfluent (2,462 correctly). So the model largely assigns disfluency labels
word-by-word, only sometimes marking rightward branches disfluent.

Of the 2,459 Edit transitions, 1554 had no leftward children, 512 returned one word,
and another 393 together returned a total of 1080 words to the stack. Looked at
another way, 1,592 leftward children were returned to the stack, from
905/2,459 Edit transitions.

Of the 1,592 instances of a word being returned to the stack, 121 were cases where
the same word was returned two or more times --- i.e. 1,471 tokens were returned
to the stack at least once, for a total of 1,592 returnings.
The 1,471 tokens break down as follows:

\begin{itemize}
    \item 607 were subsequently marked as disfluent
        \begin{itemize}
            \item 562 true positive
            \item 45 false positive
        \end{itemize}
    \item 864 were marked fluent.
        \begin{itemize}
            \item 699 were attached to their correct head
            \item 165 were attached incorrectly
            \begin{itemize}
                \item 26 were false-negative disfluencies
                \item 139 were just parse errors
            \end{itemize}
        \end{itemize}
\end{itemize}

The first thing we take away from these figures is that the case that the \edittrans
transition is designed to cover is quite common, relative to the number of edited
words.  The parser made 864 provisional left-arcs, for 2,702 words marked as 
edits.  We also note that even though the parser must make multiple decisions for
those words (by returning them to the stack), it has high recall at marking these
words disfluent (only 26 false negatives), and its precision is similar to
the parser's overall disfluency detection score.

\section{Related Work}

The most prominent joint model of parsing and disfluency detection that we are
aware of is from \citet{hale:06}, who investigated the effect of syntactic and
prosodic cues on a baseline disfluency detection system.  Their work is most
closely connected to psycholinguistics and corpus linguistics research on
disfluencies, such as \citet{shriberg:98}. The systems \citeauthor{hale:06}
implemented range in disfluency accuracy from
18-41.7\% $F$-measure, using gold \textsc{pos} tags. They also provide results
from the  \citet{Charniak01a} parser on edit detection, and improved its score
from 57.6 to 70.0.

We follow disfluency detection task definition set out by 
\citet{Charniak01b}, who describe the first `pipeline' architecture for parsing
disfluent speech.  The successor
demonstrated the importance of syntactic features for disfluency detection
\citep{Johnson04a}.

Despite this, most subsequent work has used sequence models, rather than syntactic
parsers.  One reason for this is that most applications of these models will be
over unsegmented text, as segmenting utterances into linguistic units resembling
clauses or sentences is a difficult unsolved problem.  State-of-the-art systems
currently mark segment boundaries at TODO-TODO\%.

We consider the most promising aspect of our system to be that it is naturally
incremental, so it should be straightforward to extend the system to operate
on unsegmented text in subsequent work.  Due to its use of syntactic features,
from the joint model, the system is substantially more accurate than the previous
state-of-the-art in incremental disfluency detection, 77\% \citep{zwarts:10}.

Our modified transition system is reminiscent
of the transition system described by \citet{honnibal:13}, in that it is
non-monotonic.  The non-monotonic behaviour allows the parser to build a graph of dependencies,
even though the final parse is a projective tree. There are thus natural connections
between our transition system and the \textsc{dag}-parsing system of \citet{sagae:08}.
A related idea would be to model the `rough copy' dependencies described by 
\citet{Johnson04a} with a non-projective transition system, either by allowing
arcs between non-adjacent nodes \citep{cohen:11} or by doing online-reordering
\citep{nivre:09}.

\section{Conclusion}


\bibliography{main}
\bibliographystyle{aclnat}

\end{document}
