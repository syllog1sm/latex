% vim: set textwidth=78 fo+=t :

\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{amsmath, amsthm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{tikz-dependency}
\usepackage{placeins}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{cancel}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}
\newcommand{\edittrans}{\textsc{edit}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\newcommand{\tuple}[1]{$\langle#1\rangle$}
\newcommand{\maybe}[1]{\textcolor{gray}{#1}}
\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\state}{\mathcal{S}}
\newcommand{\nmae}{\textsc{nmae}\xspace}
\newcommand{\pcfg}{\textsc{pcfg}\xspace}

\newcommand{\szero}{S0\xspace}
\newcommand{\nzero}{N0\xspace}

\newcommand{\szeroH}{S0$_h$\xspace}
\newcommand{\szeroHH}{S0$_{h2}$\xspace}
\newcommand{\szeroL}{S0$_L$\xspace}
\newcommand{\szeroLL}{S0$_{L2}$\xspace}
\newcommand{\szeroR}{S0$_R$\xspace}
\newcommand{\szeroRR}{S0$_{R2}$\xspace}
\newcommand{\szeroLzero}{S0$_{L0}$\xspace}
\newcommand{\szeroRzero}{S0$_{R0}$\xspace}

\newcommand{\nzeroL}{N0$_L$\xspace}
\newcommand{\nzeroLL}{N0$_{LL}$\xspace}
\newcommand{\nzeroLzero}{N0$_{L0}$\xspace}

\newcommand{\szeroRedge}{S0$_{re}$\xspace}
\newcommand{\szeroLedge}{S0$_{le}$\xspace}
\newcommand{\nzeroLedge}{N0$_{le}$\xspace}

\newcommand{\sparseval}{\textsc{sparseval}\xspace}

\title{Joint incremental disfluency detection and dependency parsing}

\author{
	Anonymous\\
  	Department\\
  	Institution\\
  	Address\\
  {\tt \small email }\\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

We present an incremental dependency parsing model that jointly performs
disfluency detection.  The model handles speech repairs using a novel non-monotonic
transition system, and includes several novel classes of features.  Together, these
additions improve parse accuracy by 1\%.  The model runs in expected linear time,
and processes over 700 sentences a second.

For comparison, we evaluated two pipeline systems, using state-of-the-art disfluency
detectors.  The joint model performed better on both tasks,
with a parse accuracy of 90.5\% and 84.0\% accuracy at disfluency detection.

\end{abstract}

% P1
\section{Introduction}

Most unscripted speech contains filled pauses (\emph{um}s and \emph{uh}s) and
errors which are usually edited on-the-fly by the speaker. Disfluency detection
is the task of detecting these infelicities in spoken language transcripts. The
task has some immediate value, as disfluencies have been shown to make speech
recognition output much more difficult to read \citep{jones:03}, but has also
been motivated as a module in a natural language understanding pipeline, because
disfluencies have proven problematic for \pcfg parsing models.

%One reason for this difficulty could be the effect of disfluencies on the size
%of the grammar that the parser has to learn: even though only 5\% of the tokens
%in the Switchboard treebank \citep{marcus:93} are disfluent, they increase
%the number of production rules in the corpus by over 50\%.\footnote{With
%no nodes labelled \textsc{edted}, \textsc{uh} or \textsc{prn}, there are
%2,570 unique productions in sections 2 and 3 of Switchboard. With them,
%there are 19,015. Punctuation and metadata tokens were removed before both counts.}

%The underlying problem may be that a \pcfg model incorporates assumptions that
%seem a poor fit for learning disfluent sentences, which are examples of linguistic
%performance, rather than linguistic competence.

Instead of a pipeline approach, we build on recent work in transition-based dependency
parsing, to perform the two tasks jointly.
There have been two small studies of dependency parsing on unscripted speech,
both using entirely greedy parsing strategies, without a direct comparison
against a pipeline architecture \citep{jorgensen:07,rasooli:13}.  We go substantially
beyond these pilot studies, and present a system that compares favourably to a
pipeline consisting of state-of-the-art components.

Our parser largely follows the design of \citet{zhang:cl11}. We use a structured averaged
perceptron model \citep{collins:02}, with beam-search decoding. Our feature set
is based on \citet{zhang:cl11}, and our transition-system is based on the arc-eager
system of \citet{nivre:03}.

We exploit the ease with which this model can incorporate arbitrary features,
and design a set of disfluency features that capture the
`rough copy' structure of some speech repairs, which motivated the
\citet{Johnson04a} noisy channel model.
Our second improvement is a novel non-monotonic extension to the transition
system. It allows the parser to delete edges it had previously created, in order
to deal with speech repairs where the head of an apparent constituent is disfluent,
but some of its apparent leftward children are not (e.g. \emph{pass the salt uh pepper}).
Words detected as disfluencies are disconnected from the parse of the fluent sentence,
which is scored with the Sparseval measure \citep{sparseval}.

Our main comparison is against two pipeline systems, using the two current 
state-of-the-art disfluency detections systems: the \citet{Johnson04a} noisy-channel
model, and the \citet{qian:13} cascade of sequence taggers.  The joint model
compared favourably to the pipeline parsers at both tasks, with an unlabelled
attachment score of 90.5\%, and 84.0\% accuracy at detecting speech repairs.
The system is also quite efficient, and has quite promising future prospects.
Because the parser is incremental, it should be well suited
to speech-recogniser output and other unsegmented text.

\section{Switchboard disfluency annotations}
\label{sec:swbd}

\begin{figure}
    \begin{tabular}{l}

        A flight to $\underbrace{\mathrm{um}}_\text{FP} \underbrace{\mathrm{Boston}}_\text{RM} \underbrace{\mathrm{I\;mean}}_\text{IM} \underbrace{\mathrm{Denver}}_\text{RP}$ Tuesday\\

\end{tabular}
\caption{\small A sentence with disfluencies annotated in the style of Shriberg (1994) 
    and the Switchboard corpus.
FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair.\label{fig:shriberg}}
\end{figure}

The Switchboard portion of the Penn Treebank \citep{marcus:93} consists of
telephone conversations between strangers about
an assigned topic.  Two annotation layers are provided: one for syntactic
bracketing (\textsc{mrg} files),
and one for disfluencies (\textsc{dps} files).  The disfluency layer marks
elements with little or no syntactic function, such as filled pauses and discourse
markers, and annotates speech repairs using the \citet{shriberg:94} system of
reparandum/interregnum/repair. An example is shown in Figure \ref{fig:shriberg}.

In the syntactic annotation, edited words are covered by a special node labelled
\textsc{edited}.
The idea behind this annotation is to mark text which, if
excised, will lead to a grammatical sentence.  The \textsc{mrg} files do not
mark other types of disfluencies, such as filled pauses (e.g. \emph{um}, \emph{uh}),
parentheticals (e.g. \emph{you know}) or explicit editing terms (e.g. \emph{i mean}).
We follow most previous work in restricting our attention to speech repairs.

The \textsc{mrg} and \textsc{dps} layers have high but imperfect agreement over
what tokens they mark as speech repairs: of the text annotated with both layers,
the \textsc{dps} files call 32,310 tokens part of a reparandum, while 32,742 are
under \textsc{edited} nodes in the syntactic annotation, with 33,720 tokens marked
disfluent in at least one layer. We use the \textsc{mrg} layer, as we need syntactic
annotations to train and evaluate our joint syntactic parser.

\subsection{Dependency conversion}

\label{sec:deps}
As is standard in statistical dependency parsing of English, we acquire our
gold-standard dependencies from phrase-structure trees.
We used the 2013-04-05 version of the Stanford dependency converter \citep{stanford_deps}.
At first we feared that the filled pauses, disfluencies and meta-data tokens in
the Switchboard corpus might disrupt the conversion process, by making it more
difficult for the converter to recognise the underlying production rules.

To test this, we prepared two versions of the corpus, both with oracle pre-processing:
one where \textsc{edited} nodes, filled pauses and meta-data were removed before
the trees were transformed by the Stanford converter, and one where the disfluency
removal was performed after the dependency conversion. The resulting corpora
were largely identical: 99.54\% of unlabelled and 98.7\% of labelled dependencies
were the same. The fact that the Stanford converter is quite robust to disfluencies
was useful for our baseline joint model, which is
trained on dependency trees that also included governors for disfluent words.

We follow previous work on disfluency detection by lower-casing the text and
removing punctuation and partial words (words tagged XX and words ending in
`-').  We also filter out one- and two-token sentences, as their syntactic
analyses are trivial.
We found that two additional simple pre-processes improved our results: discarding
all `um' and `uh' tokens; and merging `you know' and `i mean' into single tokens.

These pre-processes can be completed on the input string without losing information:
none of the `um' or `uh' tokens are semantically significant, and
the bigrams \emph{you know} and \emph{you mean} have a dependency between the two
tokens over 99.9\% of the times they occur in the treebank, with \emph{you} and \emph{I}
never having any children. This makes it easy to unmerge the tokens deterministically
after parsing:
all incoming and outgoing arcs will point to \emph{know} or \emph{mean}.


\section{Transition-based dependency parsing}

A transition-based parser predicts the syntactic structure of a sentence incrementally,
by making a sequence of classification decisions.  We follow the architecture of
\citet{zhang:cl11}, who use beam-search for decoding, and a structured averaged
perceptron for training.  Despite its simplicity, this type of parser
has produced highly competitive results on the Wall Street Journal: with the
extended feature set described by \citet{zhang:11}, it achieves 93.5\%
unlabelled accuracy on Stanford basic dependencies \citep{stanford_deps}.  Converting
the constituency trees produced by the \citet{Charniak05a} reranking parser
results in similar accuracy.

Briefly, the transition-based parser consists of a configuration (or `state') which
is sequentially manipulated by a set of possible transitions.  A state is a 4-tuple
$c = (\sigma, \beta, A, D)$, where $\sigma$ and $\beta$ are disjoint sets of word
indices termed the \emph{stack} and \emph{buffer} respectively, $A$ is the set of
dependency arcs, and $D$ is the set of word indices marked disfluent.  There are no
arcs to or from members of $D$, so the dependencies and disfluencies can be
implemented as a single vector (we mark a token as disfluent by setting it as its
own governor).

We use the arc-eager transition system \citep{nivre:03,nivre:cl}, which consists
of four parsing actions:  \textbf{S}hift, \textbf{L}eft-Arc,
\textbf{R}ight-Arc and Re\textbf{d}uce.
We denote the stack with its topmost element
to the right, and the buffer with its first element to the left. A vertical bar
is used to indicate concatenation to the stack or buffer, e.g. $\sigma | i$ indicates
a stack with the topmost element $i$ and remaining elements $\sigma$.  
A dependency from a governor $i$ to a child $j$ is denoted $i \rightarrow j$.
The four arc-eager transitions are shown in Figure \ref{fig:ae_notation}.

The Shift action moves the first item of the buffer onto the stack.
The Right-Arc does the same, but also adds an arc, so that the top two items
on the stack are connected. The Reduce move and the Left-Arc both pop the stack,
but the Left-Arc first adds an arc from the first word of the buffer to the word
on top of the stack. Constraints on the Reduce and Left-Arc
moves ensure that every word is assigned exactly one head in the final configuration.
We follow the suggestion of \citet{nivre:squib} and
add a dummy token, \textsc{Root}, to the end of the sentence.  Parsing terminates
when this token is at the start of the buffer, and the stack is empty.
Disfluencies are added to $D$ via the Edit transition, E, which we now define.



\begin{figure}
    \centering
    \small
    \begin{tabular}{lr}
        $(\sigma,i | \beta, A, D) \vdash (\sigma | i, \beta, A, D) $ \hfill & \hfill S \\
        $(\sigma | i,j | \beta, A, D) \vdash ( \sigma, j | \beta, A \cup \{ j \rightarrow i \}, D ) $ \hfill & \hfill L \\
        \multicolumn{2}{c}{Only if $i$ does not have an incoming arc.}\\
        $(\sigma | i,j | \beta, A, D) \vdash ( \sigma | i | j, \beta, A \cup \{ i \rightarrow j \}, D ) $ \hfill & \hfill R \\
        $(\sigma | i, \beta, A, D) \vdash ( \sigma, \beta, A, D )$ \hfill & \hfill  D \\
        \multicolumn{2}{c}{Only if $i$ has an incoming arc.}\\
    \hline
    $(\sigma | i, j | \beta, A, D) \vdash (\sigma | x_1...x_n, j | \beta,A',D')$ & E \\
    Where \\
    $A \setminus A' = \{x \rightarrow y : \forall x,y \in \mathbb{N}\;\mathrm{s.t.}\;x = i\;\mathrm{or}\;y = i\} $\\
    %Rsuch that $x = i$ or $y = i \}$ \\
    $D' \setminus D = \{i...j-1\}$ \\
    $x_1...x_n$ are the former left children of $i$ \\
    \end{tabular}
    \caption{\small Our parser's transition system.  The first four transitions
             are the standard arc-eager system; the fifth is our novel Edit
             transition.\label{fig:ae_notation}}
\end{figure}



\section{A non-monotonic Edit transition}
\label{sec:edittrans}

One of the reasons disfluent sentences are hard to parse is that there often appear
to be syntactic relationships between words in the reparandum and the fluent sentence.
When these relations are considered in addition to the dependencies between fluent words,
the resulting structure is not necessarily a projective tree.

Figure 3 shows a simple example, where the repair {\em square} replaces the
reparandum {\em rectangle}.  An incremental parser could easily become
`garden-pathed' and attach the repair {\em square} to the preceding words,
constructing the dependencies shown dotted in Figure~3.  Rather than attempting
to devise an incremental model that avoids constructing such dependencies, we
allow the parser to construct these dependencies and later delete them if the governor
or child are marked disfluent.

Psycholinguistic models of human sentence processing have long posited
\emph{repair} mechanisms \citep{FrazierRayner1982}.  Recently, \citet{honnibal:13}
showed that a limited amount of `non-monotonic' behaviour can 
improve an incremental parser's accuracy.
We here introduce a non-monotonic transition, Edit, for speech
repairs. 

\begin{figure}
    \small
\begin{dependency}[theme=simple, segmented edge]
    \begin{deptext}[column sep=.075cm, row sep=.1ex]
    Pass \& me \& the \& red \& rectangle \& uh I mean \& square \\
    \end{deptext}
    \depedge[edge unit distance=0.9ex]{1}{2}{}
    \depedge[dotted, edge below, edge unit distance=0.8ex]{5}{3}{}
    \depedge[dotted, edge below, edge unit distance=0.8ex]{5}{4}{}
    \depedge[dotted, edge below, edge unit distance=0.8ex]{1}{5}{}
    \depedge[edge unit distance=0.7ex]{7}{3}{}
    \depedge[edge unit distance=0.7ex]{7}{4}{}
    \depedge[edge unit distance=0.6ex]{1}{7}{}
    \end{dependency}
    \caption{\small Example where `temporary edges' between the reparandum and the
    fluent sentence complicate parsing. In order to learn a projective tree
    for the sentence, the parser would have to learn not to assign the dotted
    edges below. We instead make the parsing process non-monotonic.
\label{fig:rectangle}}
\end{figure}

The Edit transition marks the word $i$ on top of the stack $\sigma | i$ as
disfluent, along with its rightward descendents --- i.e., all words in the
sequence $i...j-1$, where $j$ is the word at the start of the buffer. It then
restores the words both preceding and formerly governed by $i$ to the stack.

In other words, the word on top of the stack and its \emph{rightward descendents}
are all marked as disfluent, and the stack is popped. We then restore its
leftward children to the stack, to be re-processed.
All dependencies to and from words marked disfluent are deleted. 
The transition is \emph{non-monotonic} in the sense that it can delete dependencies
that the parser had previously proposed, and replace tokens onto the stack for
further processing. 

Why revisit the leftward children, but not the right? The logic here is that we
are concerned about dependencies which might be mirrored between the reparandum
and the repair. The rightward subtree of the disfluency might well be incorrect,
but if it is, it would still be incorrect if the word on top of the stack weren't
disfluent --– so we regard these as parsing errors that we will train our model
to avoid. In contrast, avoiding the Left-Arc transitions would require the parser to
predict that the head is disfluent, when it has not necessarily seen any evidence
indicating that.

\subsection{Worked example}

%The Edit transition is particularly useful for this
%type of sentence, where the verb is disfluent but the subject is not.
Figure \ref{fig:bankrupt} shows a gold-standard derivation for a disfluent sentence
from the development data.
Line 1 shows the state resulting from the initial Shift action.  In the next three
states, \emph{His} is Left-Arced to \emph{company}, which is then Shifted onto
the stack, and Left-Arced to \emph{went} in Line 4.

The dependency between \emph{went} and \emph{company} is not part of the gold-standard,
because \emph{went} is disfluent.  The correct governor of \emph{company} is the
second \emph{went} in the sentence.  The Left-Arc move in Line 4 can still
be considered correct, however, because the gold-standard analysis is still
derivable from the resulting configuration, via the Edit transition.
Another non-gold dependency is created in Line 6, between \emph{broke} and \emph{went},
before \emph{broke} is Reduced from the stack in Line 7.

Lines 9 and 10 show the states before and after the Edit transition. The word
on top of the stack in Line 9, \emph{went}, has one leftward child, and one
rightward child.  After the Edit transition is applied, \emph{went} and its 
rightward child \emph{broke} are both marked disfluent, and \emph{company}
is returned to the stack.  All of the previous dependencies to and from \emph{went}
and \emph{broke} are deleted.

Parsing then proceeds as normal, with the correct governor of \emph{company}
being assigned by the Left-Arc in Line 11, and \emph{bankrupt} being Right-Arced
to \emph{went} in Line 12.  To conserve space, we have omitted the dummy
\textsc{Root} token, which is placed at the end of the sentence, following
the suggestion of \citet{nivre:squib}.  The final action will be a Left-Arc
from the \textsc{Root} token to \emph{went}.

\begin{figure}
\small
%\centering
\begin{tabular}{l}
\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    1. $S$ \&    His \& company \& went \& broke \& i\_mean \& went \& bankrupt \\
\end{deptext}
\wordgroup{1}{2}{2}{}
\deproot[edge height=0.3cm, ultra thick]{3}{}
\end{dependency} \\[-1ex]

\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    2. $L$ \&    His \& company \& went \& broke \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\deproot[edge height=0.3cm, ultra thick]{3}{}
\end{dependency} \\[-1ex]

\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    3. $S$ \&    His \& company \& went \& broke \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\wordgroup{1}{3}{3}{}
\deproot[edge height=0.3cm, ultra thick]{4}{}
\end{dependency} \\[-1ex]

\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    4. $L$ \&    His \& company \& went \& broke \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{4}{3}{}
\deproot[edge height=0.3cm, ultra thick]{4}{}
\end{dependency} \\[-1ex]


\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    5. $S$ \&    His \& company \& went \& broke \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{4}{3}{}
\wordgroup{1}{4}{4}{}
\deproot[edge height=0.3cm, ultra thick]{5}{}
\end{dependency} \\[-1ex]


\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    6. $R$ \&    His \& company \& went \& broke \& i \_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{4}{3}{}
\depedge{4}{5}{}
\wordgroup{1}{4}{4}{}
\wordgroup{1}{5}{5}{}
\deproot[edge height=0.3cm, ultra thick]{6}{}
\end{dependency} \\[-1ex]

\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    7. $D$ \&    His \& company \& went \& broke \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{4}{3}{}
\depedge{4}{5}{}
\wordgroup{1}{4}{4}{}
\deproot[edge height=0.3cm, ultra thick]{6}{}
\end{dependency} \\[-1ex]



\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    8. $S$ \&    His \& company \& went \& broke \& i \_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{4}{3}{}
\depedge{4}{5}{}
\wordgroup{1}{4}{4}{}
\wordgroup{1}{6}{6}{}
\deproot[edge height=0.3cm, ultra thick]{7}{}
\end{dependency} \\[-1ex]


\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    9. $L$ \&    His \& company \& went \& broke \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{4}{3}{}
\depedge{4}{5}{}
\depedge{7}{6}{}
\wordgroup{1}{4}{4}{}
\deproot[edge height=0.3cm, ultra thick]{7}{}
\end{dependency} \\[-1ex]


\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    10. \textbf{E} \&    His \& company \& \cancel{went} \& \cancel{broke} \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{7}{6}{}
\wordgroup{1}{3}{3}{}
\deproot[edge height=0.3cm, ultra thick]{7}{}
\end{dependency} \\[-1ex]


\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    11. $L$ \&    His \& company \& \cancel{went} \& \cancel{broke} \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{7}{6}{}
\depedge[edge height=0.8em]{7}{3}{}
\deproot[edge height=0.3cm, ultra thick]{7}{}
\end{dependency} \\[-1ex]

\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    12. $S$ \&    His \& company \& \cancel{went} \& \cancel{broke} \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{7}{6}{}
\depedge[edge height=0.8em]{7}{3}{}
\wordgroup{1}{7}{7}{}
\deproot[edge height=0.3cm, ultra thick]{8}{}
\end{dependency} \\[-1ex]


\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    12. $R$ \&    His \& company \& \cancel{went} \& \cancel{broke} \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{7}{6}{}
\depedge[edge height=0.8em]{7}{3}{}
\depedge{7}{8}{}
\wordgroup{1}{7}{7}{}
\wordgroup{1}{8}{8}{}
\end{dependency} \\[-1ex]


\begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
\begin{deptext}[column sep=0.055cm, row sep=.1ex]
    13. $D$ \&    His \& company \& \cancel{went} \& \cancel{broke} \& i\_mean \& went \& bankrupt \\
\end{deptext}
\depedge{3}{2}{}
\depedge{7}{6}{}
\depedge[edge height=0.8em]{7}{3}{}
\depedge{7}{8}{}
\wordgroup{1}{7}{7}{}
\end{dependency} \\[-1ex]
\end{tabular}
\caption{\small A gold-standard transition sequence using our \edittrans transition.
Each line specifies an action and shows the state resulting from it. Words on the stack
are circled, and the arrow indicates the start of the buffer. Disfluent
words are struck-through.
\label{fig:bankrupt}}

\end{figure}


\subsection{Dynamic oracle training}
\label{sec:dynoracle}


%An essential component when training a transition-based parser is an oracle which,
%given a gold-standard tree, dictates the sequence of moves a parser should make
%in order to derive it. Traditionally, these oracles are defined as functions from
%trees to sequences, mapping a gold tree to a single sequence of actions deriving it,
%even if more than one sequence of actions derives the gold tree. During training,
%the model suffers a loss if its prediction departs from this single 
%sequence, even if it lies on an alternate path to the gold-standard tree.

Our non-monotonic transition system introduces substantial \emph{spurious ambiguity}:
the gold-standard parse can be derived via many different transition sequences.
Recent work has shown that this can be advantageous
\citep{sartorio:13,honnibal:13,goldberg:12}, because difficult decisions can
sometimes be delayed until more information is available.

For instance, consider the state shown in Line 5 of Figure \ref{fig:bankrupt}.
From this configuration, there are multiple actions that could be considered
`correct', in the sense that they do not exclude the gold-standard analysis.
The Edit transition is correct because \emph{went} is disfluent, but the
Left-Arc and even the Right-Arc are also correct, in that there are continuations
from them that lead to the fully correct parse tree and disfluency analysis.

We regard all transition sequences that can result in the correct analysis as
equally valid, and want to avoid stipulating one of them during training.  
We achieve this by following \citet{goldberg:12} in using a
\emph{dynamic oracle} to create partially labelled training data. 
A dynamic oracle is a function that determines the highest-scoring analysis reachable
from a given configuration, which makes it easy to calculate the cost of applying
an action to a configuration.  Our training data is \emph{partially} labelled
in the sense that there can be multiple zero-cost actions for a configuration.

\citet{goldberg:12} describe how a local model can be trained using a dynamic
oracle.  We adapt their algorithm for the structured perceptron with beam-search
decoding as follows.

For each sentence, we maintain two priority queues of
$\langle \mathrm{score}, \mathrm{configuration} \rangle$ tuples: a prediction
beam $C$ and a gold-standard beam $G$.
At each step of the parsing process, we search for the \emph{maximum violation}
\citep{huang:12}, which we track in another priority queue, $V$:

\begin{enumerate}
\itemsep0em
\item Extend the configurations in $C_1...C_k$ with every action, adding the model's
    score for the action to their figure of merit. Push the updated tuples to
      priority queue $C'$. Replace $C$ with $C'$.
\item Extend $G_1...G_k$ in the same way, but only with actions that result in
    a configuration from which the gold-standard analysis can be reached.
      Push the updated tuples to priority queue $G'$. Replace $G$ with $G'$.
  \item If the gold-standard analysis cannot be reached from $C_{1,2}$, 
    set $\delta = C_{1,1} - G_{1,1}$. If $\delta \ge 0$, push the triple
    $\langle \delta, C_{1,2}, G_{1,2} \rangle$ to the priority queue $V$.
\end{enumerate}

Once parsing terminates, we perform a standard structured perceptron update
with the gold and predicted candidates at $V_1$.

The maximum violation update strategy is an improvement on the early update
strategy of \citet{collins:02}.\footnote{Its advantage is mostly in efficiency:
the two update strategies result in similar accuracy, but \emph{maximum violation}
requires fewer training iterations, because its updates are over longer sequences.}
Instead of searching for the point where the
gold-standard analysis falls out of the prediction beam, we find the point
where the divergence in scores is highest.

%Having found the \emph{maximum violation}
%$V$, we perform a standard structured perceptron weight update over the gold and
%predicted state sequences.


\subsection{Path length normalisation}

One problem introduced by the Edit transition is that the number of
actionss applied to a sentence is no longer constant --- it is no longer guaranteed
to be $2n-1$, for a sentence of length $n$. When the Edit transition is
applied to a word with leftward children, those children are returned to the stack,
and processed again.  This has little to no impact on the algorithm's empirical
efficiency, although worst-case complexity is no longer linear, but it does
pose a problem for decoding.

The perceptron model tends to assign
high scores to its top prediction.\footnote{Recall that each training instance
involves adding 1.0 to the weights for the correct class, and -1.0 to the weights
for the predicted class, over several iterations. The correct class stays the
same, while the predictions may vary, so the negative weight tends to be split
among multiple classes. This is why the top prediction on an instance almost
always has a high positive weight.}
We thus observed a problem when comparing paths of different lengths, at the end
of the sentence. Paths that included Edit transitions were longer,
so the sum of their scores tended to be higher.

The same problem has been observed during incremental \textsc{pcfg} parsing,
by \citet{zhang:13}.  They introduce an additional transition, \textsc{idle},
to ensure that paths are the same length. So long as one candidate in the beam
is still being processed, all other candidates apply the \textsc{idle} transition.

We adopt a simpler solution.  We normalise the figure-of-merit, which is used to rank
candidates in the beam, by the length of the candidates' transition history. The
new figure-of-merit is the
arithmetic mean of the candidate's transition scores.

Interestingly, \citet{zhang:13} report that they tried exactly this, and that it
was less effective than their solution. We found that the features
associated with the \textsc{idle} transition were uninformative (the state is at
termination, so the stack and buffer are empty), and had nothing to do with how
many edit transitions were earlier applied.

\section{Features for the joint parser}
\label{sec:features}

Our baseline parser uses the feature set described by \citet{zhang:11}.
The feature set contains 73 templates that mostly refer to the properties of
12 \emph{context tokens}: the top of the stack (\szero), its two leftmost and
rightmost children (\szeroL, \szeroLL, \szeroR, \szeroRR), its parent and
grand-parent (\szeroH, \szeroHH), the first word of the buffer and its two leftmost
children (\nzero, \nzeroL, \nzeroLL), and the next two words of the buffer (N1, N2).

Atomic features consist of the word, part-of- speech tag, or dependency label
for these tokens; and multiple feature atoms are often combined for feature
templates. The feature set also considers the string-distance between \szero
and \nzero, and the left and right valencies
(total number of children) for \szero and \nzero, as well as the set of dependency
labels assigned in their subtrees. We restrict the label set features
to the first and last 2 children for implementation efficiency, as we found this
had no effect on accuracy. Numeric features (for distance and valency) are binned
with the function $\lambda x: \max(x, 5)$.
There is only one bi-lexical feature template, which pairs
the words of \szero and \nzero.
There are also ten tri-tag templates, which consider the
\pos tag of \szero, \nzero, and various other context tokens.

We added additional dependency label features to the baseline feature
set, as we found that disfluency detection errors often resulted in ungrammatical
label combinations.  The additional templates combine the \pos tag of \szero with
two or three labels from its left and right subtrees.  Details can be found in
the supplementary materials.

\subsection{Brown cluster features}

The Brown clustering algorithm \citep{brown:92} is a well known source
of semi-supervised features. The clustering algorithm is run over a large sample
of unlabelled data, to generate a type-to-cluster map. This mapping is then used
to generate features that sometimes generalise better than lexical features,
and are helpful for out-of-vocabulary words \citep{turian:10}.

\citet{koo:10} found that Brown cluster features greatly improved the performance
of a graph-based dependency parser. On our transition-based parser, Brown cluster
features bring a small but statistically significant improvement on the \textsc{wsj}
task (0.1-0.3\% \textsc{uas}).  Other developers of transition-based parsers
seem to have found similar results (personal communication).
Since a Brown cluster mapping computed by \citet{liang:05} is easily
available\footnote{\url{www.metaoptimize.com/projects/wordreps}}, the features
are very simple to implement and cheap to compute.
%so we see little reason not to include them in the parser.

Our templates follow \citet{koo:10} in including features for full clusters as
well as cluster prefixes. We adapt their templates to transition-based parsing
by replacing `head' with `item on top of the stack' and `child' with `first word
of the buffer'. The exact templates can be found in the supplementary material.

The Brown cluster features are used in our `baseline' parser, and in the parsers
we use as part of our pipeline systems. They improved development set accuracy
by 0.4\%.  We experimented with the other feature sets in these parsers, but found
that they did not improve accuracy on fluent text.

\subsection{Rough copy features}

\citet{Johnson04a} point out that in speech repairs, the repair is often a `rough
copy' of the reparandum.  The simplest case of this is where the repair is a single
word repetition. In more complex cases, some of the wording or grammatical
structure will be the same, and the rest will be different.  
It is common for the repair to differ from the reparandum by insertion, deletion
or substitution of one or more words.

To capture this regularity, we first extend the feature-set to track three new
\emph{context tokens}:\footnote{As is common
in this type of parser, our implementation has a number of vectors for properties
that are defined before parsing, such as word forms, \textsc{pos} tags, Brown
clusters, etc. A context token is an index into these vectors, allowing
features considering these properties to be computed.}
\begin{enumerate}
    \itemsep0em
    \item \szeroRedge : The rightmost edge of \szero  descendants;
    \item \szeroLedge : The leftmost edge of \szero  descendants;
    \item \nzeroLedge : The leftmost edge of \nzero  descendants.
\end{enumerate}

If a word has
no leftward children, it will be its own left-edge, and similarly it will be
its own rightward edge if it has no rightward children. Note that the token
\szeroRedge is necessarily immediately before \nzeroLedge, unless some of the
tokens between them are disfluent.

These features can be computed naively by traversing the parse tree for each
instance. However, this process would not be linear in complexity with respect
to sentence length.  Instead, we maintain two additional vectors, for left
and right yield edges, and update them during parsing.  To do this, we exploit
the fact that our features only consider the yield-edges of the tokens currently
at S0 and N0, as disfluency processing takes place between these two tokens.

We calculate a simple kind of rough-copy feature using the \szeroLedge and
\nzeroLedge tokens:

\begin{itemize}
    \itemsep0em
    \item How long is the \emph{prefix word match} between \szeroLedge...\szero
          and \nzeroLedge...\nzero?
    \item How long is the \emph{prefix POS tag match} between \szeroLedge...\szero
          and \nzeroLedge...\nzero?
    \item Do the words in \szeroLedge...\szero and \nzeroLedge...\nzero match
          exactly?
    \item Do the POS tags in \szeroLedge...\szero and \nzeroLedge...\nzero match
          exactly?
\end{itemize}

The prefix-length features are binned using the function $\lambda x: \max(x, 5)$.

\subsection{Match features}

This class of features ask which pairs of the \emph{context tokens} match, in
word or \pos tag.  The context tokens in the \citet{zhang:11} feature set
are the top of the stack (\szero), its head and grandparent (\szeroH, \szeroHH),
its two left- and rightmost children (\szeroL, \szeroLL, \szeroR, \szeroRR), the
first three words of the buffer (\nzero, N1, N2), and the two leftmost children
of \nzero (\nzeroL, \nzeroLL).
We extend this set with the \szeroLedge, \szeroRedge and \nzeroLedge tokens
described above, and also the first left and right child of \szero and \nzero
(\szeroLzero, \szeroRzero, \nzeroLzero).

All up, there are 18 context tokens, so ${18 \choose 2}~=~153$ token pairs.
For each pair of these tokens, we add two binary features, indicating whether the
two tokens match in word form or \pos tag.  We also have two further classes of
features: if the words do match, a feature is added indicating the word form;
if the tags match, a feature is added indicating the tag. These finer grained
versions help the model adjust for the fact that some words can be duplicated
in grammatical sentences (e.g. `that that'), while most rare words cannot.

\subsection{Edited neighbour features}

Disfluencies are usually
string contiguous, even if they do not form a single constituent.  
We enable the parser to exploit this by adding features that fire when the word
or pair of words immediately preceding N0 are already marked disfluent by the parser.
A similar pair of features is
added for the word or pair of words immediately after \szero.


\section{Part-of-speech tagging}

In the absence of an established way of doing joint part-of-speech tagging and
dependency parsing, we adopt the standard pipeline strategy.  Most transition-based
parsers use a structured averaged perceptron model with beam-search for tagging,
as this model achieves competitive accuracy and matches the standard dependency
parsing architecture. Our tagger also uses this architecture.

We performed some additional feature engineering for the tagger, in order to
improve its accuracy given the lack of case distinctions and punctuation in
the data. Our additional features use two sources of unsupervised information.
First, we follow the suggestion of \citet{manning:11} in using Brown cluster
features to improve the tagger's accuracy on unknown words. Second, we compensate
for the lack of case distinctions by including features that ask what percentage
of the time a word form was seen title-cased, upper-cased and lower-cased in the
Google Web1T corpus. 

Where most previous work uses cross-fold training for the tagger, to ensure that the
parser is trained on tags that reflect run-time accuracies, we do online training
of the tagger alongside the parser, using the current tagger model to produce
tags during parser training.  This had no impact on parse accuracy, and made it
slightly easier to develop our tagger alongside the parser.

The tagger achieved 96.5\% accuracy on the development data, but when we ran our
final test experiments, we found its accuracy dropped to 96.0\%, indicating
some over-fitting during our feature engineering.  On the development data,
our parser accuracy improves by about 1\% when gold-standard tags are used.

\section{Experiments}

We use the Switchboard portion of the Penn Treebank \citep{marcus:93}, as
described in Section \ref{sec:swbd}, to train our joint
models and evaluate them on dependency parsing and disfluency detection. The
pre-processing and dependency conversion are described in Section~\ref{sec:deps}.
We use the standard train/dev/test split from \citet{Charniak01a}: Sections 2
and 3 for training, and Section 4 divided into three held-out sections, the first
of which is used for final evaluation.

Our parser evaluation is modelled on the \sparseval metric \citep{sparseval}.
However, we wanted to use a standard dependency converter, so we
do not use the \sparseval tool itself.  Our evaluation script is distributed
in the supplementary materials.

We follow \sparseval in not including arcs to speech repairs or other disfluent
words in the parser evaluation (i.e. we don't evaluate the heads of disfluent
words).  Links to fluent words incorrectly marked as disfluent are counted as
incorrect.  
%A small advantage of this evaluation is that systems are always
%evaluated on the same number of arcs.  This allows us to report our accuracies
%in the same format as the rest of the recent dependency parsing literature,
%instead of introducing precision/recall/$F$-measure scores.

We follow \citet{Johnson04a} and others in restricting our disfluency evaluation
to speech repairs, which we identify as words that have a node labelled \textsc{edited}
as an ancestor.  Unlike most other disfluency detection research, we train only
on the \textsc{mrg} files, giving us 619,236 words of training data instead of
the 1,482,845 used by the pipeline systems.  It may be possible to improve our
system's disfluency detection by leveraging the additional data that does not
have syntactic annotation in some way.

All parsing models were trained for 15 iterations.
We found that optimising the number of iterations on a development set led to
small improvements that did not transfer to a second development set (the middle
third of Section 4, which \citet{Charniak01a} reserved for `future use').

We test for statistical significance in our results by training 20 models for
each experimental configuration, using different random seeds. The random seeds
control how the sentences are shuffled during training, which the perceptron
model is quite sensitive to.  We use the Wilcoxon rank-sums non-parametric test.
The standard deviation in \textsc{uas} for a sample was typically around 0.05,
and 0.5 for disfluency $F$-measure.

All of our models use beam-search decoding, with a beam width of 32. We found that
a beam width of 64 brought a very small accuracy improvement (about 0.1\%), at
the cost of 50\% slower run-time. Wider beams brought no accuracy improvement.
Accuracy seems to converge with slightly narrower beams than on newswire text.
This is probable due to the shorter sentences in Switchboard.

%\label{sec:oracle}
%This experiment investigates how much disfluencies disrupt parsing accuracy,
%by training and evaluating a parser on a version of Switchboard with oracle
%disfluency detection.  Every word labelled as a disfluency\footnote{That is, those
%part of the yield of a node labelled \textsc{edited} in the \textsc{ptb} annotation.}
%is removed, prior to training and evaluation.

\subsection{Comparison with pipeline approach}
\label{sec:pipeline}
The accuracy of incremental dependency parsers is well established on the Wall
Street Journal, but there are no dependency parsing results in the literature
that make it easy to put our joint model's parsing accuracy into context.
We therefore compare our joint model to a pipeline approach, using two state-of-the-art
disfluency detection systems. On the development data, we also evaluate parse
accuracies after oracle disfluency detection.

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr|rr}
        & P & R & F & \textsc{uas} & \textsc{las} \\
        \hline \hline
Beam parser$_{k=32}$  &	79.6	&	70.6	&	74.8	&	89.9	&	86.9 \\
+Features             &	86.0	&	77.6	&	81.6	&	90.3	&	87.4 \\
+Edit transition      &	92.2	&	80.7	&	86.1	&	90.9	&	87.9 \\
\hline       
Oracle pipeline  & 100 & 100 & 100 & 91.6    & 88.6 \\
\hline
    \end{tabular}
\caption{Development parse and disfluency accuracies.
\label{tab:dev}}
\end{table}


The parsers for the pipeline systems were trained on text with all disfluencies
removed, following \citet{Charniak01a}. 
The two disfluency detection systems we used were the \citet{qian:13} sequence-tagging
model, and a version of the \citet{Johnson04a} noisy channel model, using the
\citet{Charniak01b} syntactic language model and the reranking features of
\citet{zwarts:11}. The two systems operate very differently, and achieve high
accuracy.


%One concern we had with this architecture was that the model is trained on text
%with oracle pre-processing, but is run on text that the pipeline has cleaned
%imperfectly. As a quick test we ran a model trained with no disfluencies over text
%that had not been pre-processed at all.  This model was only 0.2\% less accurate
%than one trained on the disfluent text.  This convinced us that there would be
%little benefit in departing from the \citet{Charniak01a} architecture to do
%cross-fold training.  This may be because the treebank annotation is noisy, so
%some portion of disfluent text remains after the \textsc{edited} nodes are discarded.
%We used the \citet{Johnson04a}
%noisy-channel model, with the \citet{Charniak01b} parser as a syntactic language
%model.  We chose this system because it is an extension of the \citet{Charniak01a}
%boosting classifier, which is the only disfluency detection system studied as
%a pre-processor to a syntactic parser.  The accuracy of the model has improved
%since \citet{Johnson04a}, as it now includes the additional features described by
%\citet{zwarts:11}.
%In fact, its accuracy is higher than the previous state-of-the-art.


\section{Results}
\label{sec:results}

\begin{table}
    \small
    \centering
    \begin{tabular}{l|r|r}
        & Disfl. F & \textsc{uas} \\
        \hline \hline
Johnson et al pipeline      & 82.1 & 90.3 \\ 
Qian and Liu  pipeline     & 83.9 & 90.1  \\
\hline
Baseline joint parser & 73.9 & 89.4 \\
Final joint parser    & \textbf{84.0} & \textbf{90.5} \\
\hline
    \end{tabular}
    \caption{Test-set parse and disfluency accuracies.\label{tab:test}}
\vspace*{-1.0ex}
\end{table}

Table \ref{tab:dev} shows the development set accuracies for our joint parser.
Both the disfluency features and the 
\textsc{edit} transition make statistically significant improvements, in both
disfluency $F$-measure, unlabelled attachment score, and labelled attachment score.

The \textbf{Oracle pipeline} system, which uses the gold-standard to clean disfluencies
prior to parsing, shows the total impact of speech-errors on the parser.  The
baseline parser, which uses the same features, scores 1.7\% lower.

When we add the features described in Section \ref{sec:features}, the gap is reduced
to 1.3\% (\textbf{+Features}).  Finally, the improved transition system reduces
the gap further still, to 0.7\% (\textbf{+Edit transition}).

Table \ref{tab:test} shows the final evaluation.  
Our main comparison is with the two pipeline systems, described in Section
\ref{sec:pipeline}. 
The final joint parser is 0.2\% more accurate than the best pipeline system, and
0.4\% higher than the other.
The \citet{Johnson04a} noisy channel model has a lower
disfluency score, but results in a more accurate parser. We attribute this to
the use of a syntactic language model, which seems to make its output more syntactically
consistent.

The Johnson et al pipeline uses a syntactic parser as a pre-process to its
disfluency detector, which is then used as a pre-process to a final syntactic
parser.  We take this as good motivation for joint modelling: if both tasks need
the other as a pre-process, then it makes good sense to do them at once.  The
success of our joint model seems to confirm this.

Our joint model's disfluency detection accuracy is not significantly different from
the current state-of-the-art, \citet{qian:13}, despite having approximately 50\%
as much training data (as we require syntactic annotation).
\footnote{Our scores refer to an updated version of the system
that corrects minor pre-processing problems. We thank the Qian Xian for making
his code available.}
Our significance testing regime involves training multiple
models of our parser with different random seeds, so could not be applied to
the other two disfluency detectors.

Although we did not systematically
optimise on the development set, our test scores are lower than our development
accuracies. Much of the over-fitting seems to be in the \textsc{pos} tagger,
which dropped in accuracy by 0.5\%.

\section{Analysis of Edit behaviour}

In order to understand how the parser applies the \edittrans transition, we
collected some additional statistics over the development data.
The parser predicted 2,459 \edittrans transitions,
which together marked 2,702
words disfluent (2,462 correctly).
%The model largely assigns disfluency labels
%word-by-word, only sometimes marking rightward branches disfluent.

Of the 2,459 Edit transitions, 1554 had no leftward children, 512 returned one word,
and another 393 together returned a total of 1080 words to the stack. Looked at
another way, 1,592 leftward children were returned to the stack, from
2,459 \textsc transitions.

Of the 1,592 instances of a word being returned to the stack, 121 were cases where
the same word was returned two or more times --- i.e. 1,471 tokens were returned
to the stack at least once, for a total of 1,592 returnings.
The 1,471 tokens break down as follows:
\vspace*{-0.5em}
\begin{itemize}
    \itemsep0ex
    \item 607 were subsequently marked as disfluent
        \begin{itemize}
            \itemsep0ex
            \item 562 true positive
            \item 45 false positive
        \end{itemize}
    \item 864 were marked fluent.
        \begin{itemize}
            \itemsep0ex
            \item 699 were attached to their correct head
            \item 165 were attached incorrectly
            \begin{itemize}
                \itemsep0ex
                \item 26 were false-negative disfluencies
                \item 139 were just parse errors
            \end{itemize}
        \end{itemize}
\end{itemize}

The first thing we take away from these figures is that the case that the \edittrans
transition is designed to cover is quite common, relative to the number of edited
words.  The parser made 864 provisional left-arcs, for 2,702 words marked as 
edits.  We also note that even though the parser must make multiple decisions for
those words (by returning them to the stack), it has high recall at marking these
words disfluent (only 26 false negatives), and its precision is similar to
the parser's overall disfluency detection score.

\section{Related Work}

The most similar system to ours was published while this research was being
finalised for publication. \citet{rasooli:13} describe a joint model of dependency
parsing and disfluency detection. They introduce a second classification step,
where they first decide whether to apply a disfluency transition, or a regular
parsing move. Disfluency transitions operate either over a sequence of words
before the start of the buffer, or a sequence of words from the start of the
buffer forward. Instead of the dynamic oracle training method that we employ,
they use a two-stage bootstrap-style process.

Direct comparison between our model and theirs is difficult,
as they use the Penn2MALT scheme, and their parser is greedy, where we use
beam search. The use of beam search may explain much of our performance advantage:
they report an unlabelled attachment score of 88.6, and a disfluency detection
F-measure of 81.4\%.

The most prominent joint model of constituency parsing and disfluency detection
that we are aware of is from \citet{hale:06}, who investigated the effect of
syntactic and prosodic cues on a baseline disfluency detection system.  Their
work is most closely connected to psycholinguistics and corpus linguistics research
on disfluencies, such as \citet{shriberg:98}. The systems \citeauthor{hale:06}
implemented range in disfluency accuracy from
18-41.7\% $F$-measure, using gold \textsc{pos} tags. They also provide results
from the  \citet{Charniak01a} parser on edit detection, and improved its score
from 57.6 to 70.0.

We follow disfluency detection task definition set out by 
\citet{Charniak01b}, who describe the first `pipeline' architecture for parsing
disfluent speech.  The successor
demonstrated the importance of syntactic features for disfluency detection
\citep{Johnson04a}.

Despite this, most subsequent work has used sequence models, rather than syntactic
parsers.  One reason for this is that most applications of these models will be
over unsegmented text, as segmenting unpunctuated text
is a difficult task that benefits from syntactic features \citep{zhang:13}.

We consider the most promising aspect of our system to be that it is naturally
incremental, so it should be straightforward to extend the system to operate
on unsegmented text in subsequent work.  Due to its use of syntactic features,
from the joint model, the system is substantially more accurate than the previous
state-of-the-art in incremental disfluency detection, 77\% \citep{zwarts:10}.

Our modified transition system is reminiscent
of the transition system described by \citet{honnibal:13}, in that it is
non-monotonic.  The non-monotonic behaviour allows the parser to build a graph of dependencies,
even though the final parse is a projective tree. There are thus natural connections
between our transition system and the \textsc{dag}-parsing system of \citet{sagae:08}.
A related idea would be to model the `rough copy' dependencies described by 
\citet{Johnson04a} with a non-projective transition system, either by allowing
arcs between non-adjacent nodes \citep{cohen:11} or by doing online-reordering
\citep{nivre:09}.


\section{Conclusion}

We have presented an efficient and accurate joint model of dependency parsing and
disfluency detection.  The model out-performs pipeline approaches using state-of-the-art
disfluency detectors, and is highly efficient, processing over 700 tokens a second.
Because the system is incremental, it should be straight-forward to apply it
to unsegmented text. The success of an incremental, non-monotonic parser at
disfluent speech parsing may also be of some psycholinguistic interest.

\bibliography{main}
\bibliographystyle{aclnat}

\end{document}
