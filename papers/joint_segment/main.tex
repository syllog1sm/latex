% vim: set textwidth=78 fo+=t :

\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{amsmath, amsthm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{tikz-dependency}
\usepackage{placeins}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{lcovington}
\usepackage{cancel}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}
\newcommand{\edittrans}{\textsc{edit}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\newcommand{\tuple}[1]{$\langle#1\rangle$}
\newcommand{\maybe}[1]{\textcolor{gray}{#1}}
\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\state}{\mathcal{S}}
\newcommand{\nmae}{\textsc{nmae}\xspace}
\newcommand{\pcfg}{\textsc{pcfg}\xspace}

\newcommand{\szero}{S0\xspace}
\newcommand{\nzero}{N0\xspace}

\newcommand{\szeroH}{S0$_h$\xspace}
\newcommand{\szeroHH}{S0$_{h2}$\xspace}
\newcommand{\szeroL}{S0$_L$\xspace}
\newcommand{\szeroLL}{S0$_{L2}$\xspace}
\newcommand{\szeroR}{S0$_R$\xspace}
\newcommand{\szeroRR}{S0$_{R2}$\xspace}
\newcommand{\szeroLzero}{S0$_{L0}$\xspace}
\newcommand{\szeroRzero}{S0$_{R0}$\xspace}

\newcommand{\nzeroL}{N0$_L$\xspace}
\newcommand{\nzeroLL}{N0$_{LL}$\xspace}
\newcommand{\nzeroLzero}{N0$_{L0}$\xspace}

\newcommand{\szeroRedge}{S0$_{re}$\xspace}
\newcommand{\szeroLedge}{S0$_{le}$\xspace}
\newcommand{\nzeroLedge}{N0$_{le}$\xspace}

\newcommand{\sparseval}{\textsc{sparseval}\xspace}

\title{Incremental Parsing of Disfluent, Unsegmented Speech Transcripts}

\author{
	Anonymous\\
  	Department\\
  	Institution\\
  	Address\\
  {\tt \small email }\\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Conversational speech turns are often long, motivating the use of a segmentation
model to pre-process the input into shorter units.  We instead propose a
transition-based model that processes entire turns, jointly performing dependency
parsing and disfluency detection.  Segment boundaries can then be recovered
from the syntactic structure if they are required.

We compare the system against a pipeline approach, where the input is segmented
with a \textsc{crf} model before parsing.  We find that the pipeline approach
suffers from error-propagation, while the joint model achieves
state-of-the-art accuracies.
It achieves 87.9\% parse accuracy, a 2.0\% improvement over the pipeline approach.
\end{abstract}

% P1
\section{Introduction}


Previous speech understanding systems have required that the input be
pre-segmented into sentence-like units.  However, sentence boundaries are not
always clear in the speech input, which lacks case distinctions and punctuation,
and is not reliably segmented by pauses.
For example, the following turn from the
Switchboard conversational speech transcripts is segmented into three utterances,
at the slashes:

\begin{lexample}
\small
uh and really we were really forced into keeping a budget because i 'm i 'm paid
once a month which sort of sort of forces some uh uh restrictions / \\
and you need to make sure all your bills are paid / \\
uh about yourself
\end{lexample}

\noindent An annotator could reasonably omit the first boundary, analysing
the first two units as a single conjoined sentence.  
We suspect this arbitrariness in sentence boundary location
might be problematic for syntactic parsing, since the boundaries could
accidentally constrain the syntactic analyses in ways that are inconsistent with
the parser's expectations.  This motivated us to explore approaches where the
syntactic parser itself is responsible for identifying the locations of sentence
boundaries.  Our approach here is to associate entire discourse turns with a
single syntactic structure that consists of a sequence of sentences.
While the length of these turns makes conventional $O(n^3)$ parsing algorithms
impractical, incremental transition-based parsers now achieve high accuracy,
while running in time linear in the length of the input string.
Our parsing model is based on the system described
by \citet{honnibal:14}, who showed that a joint model of disfluency detection
and parsing produced state-of-the-art accuracy on both tasks, when gold-standard
segment boundaries are available.

We tried two ways of parsing unsegmented inputs.  Our first approach introduces
a new transition, Break, to explicitly predict segment boundaries during parsing.
The second approach leaves the segmentation implicit until
parsing is complete, at which point the boundaries can be read-off from the
predicted parse.

We find that both ways of parsing unsegmented input result in superior accuracy
to a pipeline approach, where the input is pre-segmented by a \textsc{crf} model.
The joint models prove advantageous at all three evaluations: segmentation,
disfluency detection and parsing.
The improvement is particularly substantial for parsing, where the
pipeline approach results in 85.9\% \textsc{uas}, while the best joint model
achieves 87.9\%, a new state-of-the-art.  
Our model is also highly efficient, processing over 900 words per second.


\section{Spoken Language Understanding}

\begin{figure}
    \begin{tabular}{l}
        \small

        A flight to $\underbrace{\mathrm{um}}_\text{FP} \underbrace{\mathrm{Boston}}_\text{RM} \underbrace{\mathrm{I\;mean}}_\text{IM} \underbrace{\mathrm{Denver}}_\text{RP}$ Tuesday\\

\end{tabular}
\caption{\small A sentence with disfluencies annotated in the style of Shriberg (1994) 
    and the Switchboard corpus.
FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair.
%We follow previous work in evaluating the system on the accuracy with which
%it identifies speech-repairs, marked \emph{reparandum} above.
\label{fig:shriberg}}
\end{figure}

\begin{table}
    \centering
    \small
    \begin{tabular}{lc|r}
\hline
Disfluency type & Example & Freq. \\
\hline \hline
Speech-repair & \emph{to } \textbf{Boston} \emph{uh Denver} & 32,310 \\
Filled pauses    & \textbf{um}, \textbf{uh} & 20,502 \\
Edit terms & \textbf{I mean} & 3,447 \\ 
Discourse  & \textbf{well}, \textbf{you know} & 21,412  \\
Segment Conjunctions & \textbf{and}, \textbf{and so} & 25,624 \\
\hline
\end{tabular}
\caption{\small Frequencies of different disfluency types in Sections 2 and 3 of the
Switchboard \textsc{mrg} files.\label{tab:dfl_freqs}}
\vspace*{-4em}
\end{table}


A verbatim, unpunctuated speech transcript has very different linguistic characteristics
from well-edited written text, even in the absence of speech-recognition errors.
given perfect speech recognition.  Speech transcripts pose
two challenges for natural language understanding technologies in particular.
The \textbf{segmentation} problem arises because the speech stream is continuous,
and  pauses are less syntactically informative than careful punctuation \citep{gregory:04}.
The \textbf{disfluency} problem arises due to language performance problems ---
speakers \emph{um} and \emph{uh}, stop and start, and frequently insert parentheticals.

%Unscripted speech has several characteristics that make spoken language understanding
%a distinct problem from understanding written text.  Two such features are that
%it is \emph{continuous}, and that it is frequently \emph{disfluent}.

Well-edited written text can be segmented into sentences easily, using punctuation and
capitalisation.  Sentence boundary detection systems typically achieve accuracies
above 99\% on clean text input, while state-of-the-art speech segmentation systems
achieve only 96-97\% accuracy (i.e. a 3-times higher error-rate).
Segmentation is typically a \emph{pre-}process for parsing, limiting segmentation
systems to ngram-based features, and acoustic cues, which have proven difficult
to utilise \citep{liu:05}.  These local features cannot accurately identify
segment boundaries, if segments are identified with tensed clauses, since they can
contain multiple long-range dependencies.

As well as being continuous, unscripted speech is frequently disfluent.
Figure \ref{fig:shriberg} shows part of a disfluent utterance, annotated according to
\citet{shriberg:94}.
%The words marked \emph{reparandum}
%are corrected by the speaker, often with an explicit editing term in the
%\emph{interregnum}, before being replaced by the text marked \emph{repair}.
Speech repairs are particularly problematic for syntactic
parsers, because of the complicated dependency structures that can arise between the
reparandum, repair and the fluent sentence \citep{Johnson04a}.
%In contrast, other types of disfluencies tend to be lexically restricted, and
%tend to simply interrupt the fluent sentence, with limited syntactic interaction.

\begin{table}
\centering
\small
\begin{tabular}{l|rr}
    \hline
    Length & Segmented & Unsegmented \\
    \hline \hline
    1-2 & 26,140 & 10,225 \\
    2-5 & 15,727 & 6,914 \\
    5-10 & 23,283 & 6,885 \\
    10-20 & 19,738 &  8,934 \\
    20-50 & 6,406 & 9,765 \\
    50-100 & 161 & 2,106 \\
    100-200 & 0 & 257 \\
    200-422 & 0 & 20 \\
    \hline
    Total & 91,455 & 45,106 \\
    \hline
\end{tabular}
\caption{\small Input lengths in the Switchboard training corpus, with and without
    gold-standard segmentation.  For instance, with utterances pre-segmented,
    there are 161 sentences with between 51 and 100 tokens; in the unsegmented
corpus, there are 2,106 sentences with lengths in that range.
\label{tab:seg_freqs}}
%\vspace*{-5em}
\end{table}

\newpage
\subsection{The Switchboard Corpus}
\label{sec:swbd}

The Switchboard portion of the Penn Treebank \citep{marcus:93} contains 1,126
transcripts of telephone calls between strangers on an assigned topic.
Every file has been annotated for speech-repairs and other disfluencies (filled
pauses, parentheticals, discourse markers, etc); these annotations are provided
in the \textsc{dps} files.  Syntactic brackets (\textsc{mrg} files) are available
for 619,236 of the 1,482,845 words in the training sections of the corpus (2 and 3). 
All of the transcripts have also been annotated for various speech \emph{metadata},
including utterance segmentation, speech repairs per \citet{shriberg:94}, and 
non-repair disfluencies, such as filled pauses.  Table \ref{tab:dfl_freqs} shows the
frequency of these disfluencies in the training corpus.

Table \ref{tab:seg_freqs} shows how segmentation affects the length of training
inputs.  Without any utterance segmentation, the inputs consist of whole turns,
using the gold-standard diarisation in the \textsc{mrg} files.
Segmentation doubles the number of segments (and so halves their length, on average);
i.e., on average each turn contains one sentence-medial segment boundary.

%The Switchboard syntactic annotations come in the form of syntactic brackets.
%We follow Honnibal and Johnson (2014) in using
%the 2013-04-05 version of the Stanford dependency converter \citep{stanford_deps},
%using the Basic Dependencies scheme,
%which produces strictly projective representations.

We follow previous work on spoken language understanding by lower-casing the text
and removing punctuation and partial words (words tagged XX and words ending in
`-').  However, we depart from Honnibal and Johnson (2014) in not removing one-token
sentences, not removing filled pauses as a pre-process, and not
re-tokenising the common parentheticals \emph{you know} and \emph{i mean}.
We avoid these extra pre-processing steps in favour of extended disfluency processing,
by subtyping the Edit transition with different disfluency labels.

\section{Joint Disfluency Detection and Parsing}
\label{sec:model}


\begin{figure}
    \centering
    \small
    \begin{tabular}{lr}
        \hline
        $(\sigma, i | \beta, \mathbf{A}, \mathbf{L}) \vdash (\sigma | i, \beta, \mathbf{A}, \mathbf{L})$ & S \\
        $(\sigma | i | j, \beta, \mathbf{A}, \mathbf{L}) \vdash (\sigma | i, \beta, \mathbf{A}(j)=i, \mathbf{L}(j)=l)$ & R$_l$ \\ 
        $(\sigma | i, j | \beta, \mathbf{A}, \mathbf{L}) \vdash (\sigma, j | \beta, \mathbf{A}(i)=j, \mathbf{L}(i)=l)$ & L$_l$ \\
        $(\sigma | i, j | \beta, \mathbf{A}, \mathbf{L}) \vdash (\sigma | y, j | \beta, \mathbf{A}(y)=y, \mathbf{L}(y)=z)$ & E$_l$ \\
        Where \\
        $x_1...x_n$ are the former left children of $i$ \\
        $y = [x_1, x_n]$ \\
        $z$ is the dependency label assigned to reparanda \\
        %A \setminus \{(x, y, l) \;\mathrm{or}\; (y, x, l) : \forall x \in [i, j), \forall y \in \mathbb{N} \}$ \\
        %    $A'' = A' \cup \{(-, x, l) : \forall x \in [i, j) \} $ \\
        \hline
\end{tabular}
\caption{\small Transition system for the parser, with $\sigma$ denoting the
    stack, $\beta$ denoting the buffer, $A$ denoting the set of arcs, and $D$ the
    set of disfluent words.  The transitions are the arc-hybrid \textbf{S}hift,
    \textbf{R}ight and \textbf{L}eft, and the Honnibal and Johnson (2014) \textbf{E}dit.
    The R, L and E transitions are parameterised by label, $l$.
\label{fig:trans}}
\vspace*{-3em}
\end{figure}
Our model is based on the Honnibal and Johnson (2014) joint incremental disfluency
detection and parsing sysm.  This section provides a brief description of
the model, and highlights our departures from it.
Our novel contributions are described from Section 4 onwards.
%we adopt a different
%transition system --- \emph{arc hybrid} \citep{kuhlmann:11}, instead of
%\emph{arc eager} \citep{nivre:03}. 
%
% The change to the arc hybrid
%transition system necessitates some feature engineering, described in Section
%\ref{sec:features}.
%We briefly describe how the system works in this section, before introducing our
%novel contributions from Section 4 onwards.

\subsection{Transition-based Dependency Parsing}

We follow recent work on speech parsing in using an incremental, transition-based
dependency parser \citep{rasooli:13,honnibal:14}.
A transition-based parser \citep{nivre:03} consists of a configuration,
and a set of actions (or `transition system').  Actions are chosen from the
transition-system and applied to the state, until the stack is empty and the
buffer is exhausted.

A configuration $c = (\sigma, \beta, \mathbf{A}, \mathbf{L})$,
where $\sigma$ and $\beta$ are disjoint sets of word
indices termed the \emph{stack} and \emph{buffer} respectively, $\mathbf{A}$ is a
vector of head indices, and $\mathbf{L}$ is a vector of dependency labels.  A
dependency arc from a head $h$ to a dependent $d$ with label $l$ is represented
$\mathbf{A}(d)=h$, $\mathbf{L}(d)=l$.  A word $d$ is marked disfluent by setting
its head to itself, i.e. $\mathbf{A}(d)=d$.  Labels are used to distinguish the
different types of dependencies,
e.g. filled pauses, speech repairs, etc.
A vertical bar is used to denote concatenation
to the stack or buffer, e.g. $\sigma | i$ indicates a stack with the topmost
element $i$ and remaining elements $\sigma$.

\subsection{Arc-Hybrid Transition System}

We depart from Honnibal and Johnson (2014) in using the arc-hybrid system \citep{kuhlmann:11},
instead of the arc-eager system \citep{nivre:03}. The two transition systems achieve
comparable accuracy \citep{goldberg:13}, but we find the arc hybrid system slightly
simpler.
%particularly when designing a training oracle for our extended transition
%system (Section \ref{sec:oracle}).
%The increased simplicity was useful for devising the transition strategies for segmentation
%described in Section \ref{sec:break_trans}. 

The arc-hybrid system, shown in Figure \ref{fig:trans}, defines the Left-Arc in
the same way as the \citet{nivre:03}
arc-eager system, but the Right-Arc creates an arc between the top two words of
the stack, following the arc-standard definition.

Unlike the arc-eager system,
arcs are only created when a word is \emph{popped}.
This means that there are never arcs to words on the stack.
The stack, buffer, and the words that have been assigned heads are three disjoint
sets.
%Another convenient property of the transition-system is that the word
%on top of the stack is the potential child of both Right-Arc and Left-Arc
%dependencies.
%In the arc-standard system, the Left-Arc adds an arc to
%the second word on the stack, and in the arc-eager system, the Right-Arc adds
%an arc to the first word of the buffer.

The arc-hybrid system maintains the simplicity advantages of arc-standard, which
have motivated \citet{huang:10} and others to continue working with it; but allows
training oracles to be defined easily, due to the \emph{arc decomposable}
property that \citet{goldberg:13} show it shares with the arc-eager system.

\subsection{The Edit Transition}

We employ the Edit transition defined by Honnibal and Johnson (2014), to handle
speech repairs.
The Edit transition marks the word $i$ on top of the stack $\sigma | i$ as
disfluent, along with its rightward descendents --- i.e., all words in the
sequence $i...j-1$, where $j$ is the word at the start of the buffer. It then
restores the words both preceding and formerly governed by $i$ to the stack.

In other words, the word on top of the stack and its \emph{rightward descendents}
are all marked as disfluent, and the stack is popped. Its leftward children are
then restored to the stack, and all dependencies to and from words marked
disfluent are deleted.
%\footnote{A word restored to the stack may be popped with either a Left-Arc, or a Right-Arc.
%This flexibility is a potential
%    advantage of the arc-hybrid system for the Edit transition. With the
%arc-eager system, only the Left-Arc could be applied to the replaced words.}

%The arc-hybrid system offers a potential advantage for this definition of the Edit
%transition.  In the arc-eager system, when words
%are replaced on the stack, they can only be attached via the Left-Arc transition.
%The arc-hybrid system allows the Right-Arc to apply to these words as well.


\subsection{Training and Decoding}

We follow Honnibal and Johnson (2014) in employing beam-search decoding, and
use their training strategy: the \citet{sun:09} latent-variable
variant of the \citet{collins:02} structured perceptron, with weight updates
calculated with the \citet{huang:12} \emph{maximum violation} strategy.  We
also employ the path-length normalisation technique that Honnibal and Johnson (2014)
recommend, to deal with the variable-length transition histories the Edit transition
may introduce: 
when calculating the figure-of-merit for the beam, we use the
\emph{mean transition score}, instead of the \emph{total transition score}.

\clearpage

\section{Joint Segmentation and Parsing}

%\begin{figure}
%\centering
%    \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
%    \begin{deptext}
%        Governor \& Governor \& Governor \& \textsc{root} \\
%    \end{deptext}
%    \depedge{4}{1}{}
%    \depedge{4}{2}{}
%    \depedge{4}{3}{}
%\end{dependency}
%\caption{\small Attachment style that arises from the Arc-from-Root segmentation strategy.
%         All segment governors are attached to the \textsc{root} symbol, via the Left-Arc.
%     \label{fig:left_arcs}}
%
%\vspace*{-1em}
%`\end{figure}

\begin{figure}
\begin{dependency}[theme=simple, edge unit distance=1.0ex]
    \begin{deptext}[row sep=2.0ex]
        I \& never \& wear \& heels \& they \& tire \& me \& out \& \textsc{root} \\
        I \& saw \& her \& wear \& heels \& \textsc{root} \\
    \end{deptext}
    \depedge{3}{1}{}
    \depedge{3}{2}{}
    \depedge{3}{4}{}
    \depedge{6}{5}{}
    \depedge{6}{7}{}
    \depedge{6}{8}{}

    \deproot[edge height=0.3cm, ultra thick]{9}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{6}{6}{}
    %\depedge[dashed]{9}{3}{}
    \depedge[dashed]{9}{6}{}


    \depedge[edge below]{2}{1}{}
    \depedge[edge below]{2}{3}{}
    \depedge[edge below]{4}{5}{}
    %\depedge[dashed, edge below]{6}{2}{}
    \depedge[dashed, edge below]{2}{4}{}

    \deproot[edge below, edge height=0.3cm, ultra thick]{6}{}
    \wordgroup{2}{2}{2}{}
    \wordgroup{2}{4}{4}{}
    \end{dependency}
    \vspace*{-2em}
\caption{\small Two parse states, showing a segmentation decision to be made using
    Strategy 1, Arc-from-Root.
    Words remaining on the stack are
    circled, and current arcs are drawn with solid lines.  The arrow indicates
    the start of the buffer.  In the top state, the correct move is a Left-Arc
    as there is no dependency between \emph{wear} and \emph{tired}.  In the
    lower state, the Right-Arc is correct.  The  dependencies that would be
added by these moves are shown with dashed lines.\label{fig:left_state}}
\vspace*{-3em}
\end{figure}

We now describe two ways of encoding utterance segmentation decisions into the
parser's transition system.  In the first strategy, the segmentation is determined
from the parse structure; in the second strategy, a distinct transition, Break,
is added to the transition system to insert segment boundaries.
The two strategies are evaluated in Section \ref{sec:results}.
We find that despite being quite different, the two strategies achieve similar
performance.

\subsection{Strategy 1: Implicit segmentation}

The first strategy we present uses the Left-Arc to attach each segment-governor
to the \textsc{root} symbol, which we place at the end of the input.
This way of encoding the segmentation decisions has the governors accumulate on
the stack, with the final decision to place segment boundaries between them only
made when the buffer is exhausted.  Because the arc-hybrid system is used, rather
than the arc-eager system, the governor of a word is not determined when it is pushed
onto the stack --- only when it is popped.  Thus, even when the buffer is exhausted,
the parser may decide to create a dependency between two words, instead of
using the Left-Arc to attach them to the \textsc{root} symbol.

Figure \ref{fig:left_state} shows two similar parse states, where this decision
arises.  In the first state, the correct decision is \textbf{Left-Arc}, because
a segment boundary should be inserted between \emph{wear} and \emph{tires}.
In the second state, the correct decision is \textbf{Right-Arc}, because
\emph{wear} is an argument of \emph{saw}.  Such decisions may be difficult,
so delaying them until the potential left and right heads can be compared in the
same state may be advantageous.

\subsection{Strategy 2: Explicit Break transition}

\begin{figure}
\begin{dependency}[theme=simple, edge unit distance=1.0ex]
    \begin{deptext}[row sep=2.0ex]
        I \& never \& wear \& heels \& they \& tire \& me \& out \& \textsc{root} \\
    \end{deptext}
    \depedge{3}{1}{}
    \depedge{3}{2}{}
    \depedge{3}{4}{}

    \deproot[edge height=0.3cm, ultra thick]{5}{}
    \wordgroup{1}{3}{3}{}
    \depedge[dashed]{9}{3}{}
    \end{dependency}
    \caption{\small A parse state illustrating segmentation using Strategy 2,
        the Break transition.
        The governor of the previous segment,
        \emph{wear},  is
        the only word on the stack (circled), and the first word of the
        next segment, \emph{they}, is at the start of the buffer (indicated by
        an arrow). After the Break transition is applied, \emph{wear} is popped
    from the stack, and the dashed arc is added from the \textsc{root} symbol.
    \label{fig:break_state}}
\vspace*{-3em}
\end{figure}
The second strategy we present uses a specific Break transition.  The transition
is applied when there is exactly one word on the stack, and the word at the start
of the buffer has no leftward children.  The word on the stack is arced to the
\textsc{root} symbol, and the stack is popped:
\vspace*{-1em}
\begin{center}
    \begin{tabular}{l}
        $(i, j | \beta | n, \mathbf{A}, \mathbf{D}) \vdash (\emptyset, j | \beta | n, \mathbf{A}(i)=n, \mathbf{L}(i)=\textsc{r}) $ \\
    Where $\{x, ..., j : \mathbf{A}(x)=j \} = \emptyset$ \\
    \end{tabular}
\end{center}
The transition
is designed to be applied early, instead of late: it should be applied
when the first word of the new segment is at the start of the buffer.  This is
guaranteed by the pre-condition, which prevents it from being applied if the word at the start of the buffer has any leftward children.

Figure \ref{fig:break_state} shows a state at which the transition should
be applied.  The governor of the previous segment is the only word remaining
on the stack, and the word at the start of the buffer has no children, as it
is the start of a new segment.  In the resulting state (shown below), the governor
has been popped from the stack (it is no longer circled), and it has been arced
to the \textsc{root} symbol.

\subsection{Training Oracle}

We follow \citet{honnibal:14} in training our model using the latent-variable
structured perceptron algorithm \citep{sun:09}.  As each training example is
received, we search for the highest-scoring transition sequence, and the highest-scoring
\emph{gold} transition sequence.  The search for the best gold-standard sequence
requires a training oracle, which maps a parse-state and a gold-standard derivation
to a set of gold-standard actions.  An action is considered gold-standard if
it is a step towards the best possible continuation, i.e. the
transition sequence that will yield the highest-scoring analysis reachable from
the current configuration.

\citet{goldberg:13} give a training oracle for the arc-hybrid system.  The oracle
amounts to the following rules, where $\beta_0$ refers to the first word of the
buffer, $\sigma_0$ refers to the top word of the stack, $\sigma_1$ refers
to the second word on the stack, and $\mathbf{G}(d)=h$ asserts that an arc from $h$
to $d$ is in the gold-standard:

\begin{enumerate}
    \item If the stack is empty, S is the only gold action;
    \item If $\mathbf{G}(\sigma_0)=\sigma_1$, S is a gold action; 
    \item If there are any other arcs between $\beta_0$ and the stack, Shift is 
          not a gold-standard action;
      \item If $\mathbf{G}(\sigma_0)=\sigma_1$, R is a gold action;
      \item If $\mathbf{G}(\sigma_0)=\beta_0$, L is a gold action;
    \item If there are any other arcs between $\sigma_0$ and the buffer, neither
          L nor R are gold actions.
\end{enumerate}

\noindent The \citet{honnibal:14} Edit transition adds the following rules to the oracle
above. Recall that $\mathbf{G}(d)=d$ asserts that a word $d$ is disfluent in the
gold-standard:

\begin{enumerate}
    \item If $\mathbf{G}(\sigma_0) = \sigma_0$, E is a gold action;
    \item If $\mathbf{G}(\sigma_0) \not= \sigma_0$, E is not a gold action;
    \item If $\mathbf{G}(\beta_0) = \beta_0$, both L and S are gold actions;
    \item If $\mathbf{G}(\sigma_1)= \sigma_1$ and $\mathbf{G}(\sigma_0) = \sigma_0$, R
        is a gold action;
    \item If $\mathbf{G}(\sigma_0) = \sigma_0$ but $\mathbf{G}(\beta_0) \not= \beta_0$,
          L is not a gold action;
      \item If $\mathbf{G}(\sigma_0) = \sigma_0$ but $\mathbf{G}(\sigma_1) \not= \sigma_1$,
          R is not a gold action.
\end{enumerate}

\noindent In Strategy 1 (Arc-from-Root), the segment boundaries are inserted
via the standard Left-Arc transition, so no adjustment to the training oracle
is required.  In Strategy 2, a new transition, Break, is used to insert the
segment boundaries. 

The Break transition is gold-standard if and only if its pre-conditions
are met, and the word on top of the stack and the word at the start of the buffer
do not belong to the same segment.  If this is the case, B is the only gold-standard
action.

\section{Experiments}

We use the Switchboard portion of the Penn Treebank \citep{marcus:93}, as
described in Section \ref{sec:swbd}, to train and evaluate our models.  Unfortunately,
this complicates comparison with most of the prior work on utterance segmentation,
which used the RT'04 shared-task data distributed by \textsc{darpa}.  We use
the Switchboard corpus for consistency with previous work on disfluency 
detection and parsing \citep{qian:13,rasooli:13,honnibal:14}.

We follow the pre-processing and dependency conversion steps described in
Section \ref{sec:swbd}: the text was lower-cased, partial words were removed,
and the phrase-structure trees were converted into projective-dependency parses
using the Stanford Dependency Converter \citep{stanford_deps}.
We use the standard train/dev/test split from \citet{Charniak01a}: Sections 2
and 3 for training, and Section 4 divided into three held-out sections, the first
of which is used for final evaluation.
We follow \citet{honnibal:14} in using the \sparseval \citep{sparseval}
metric to evaluate our parser, which measures the labelled and unlabelled dependency
accuracy of gold-standard fluent words.

We follow \citet{Johnson04a} and others in restricting our disfluency evaluation
to speech repairs, which we identify as words that have a node labelled \textsc{edited}
as an ancestor in the Switchboard phrase-structure trees.  We also follow them
in training only on the \textsc{mrg} files, giving us 619,236 words of training
data instead of the 1,482,845 used by other disfluency detection systems, such
as \citet{qian:13}.

We test for statistical significance in our results by training 20 models for
each experimental configuration, using different random seeds. The random seeds
control how the sentences are shuffled during training, which the perceptron
model is quite sensitive to.  We use the Wilcoxon rank-sums non-parametric test.
The standard deviation in \textsc{uas} for a sample was typically around TODO,
and TODO for disfluency $F$-measure.

The main hyper-parameters of our model are the number of training iterations,
and the beam-width, which controls a speed/accuracy trade-off.  We found that
only small accuracy improvements were obtained with beams wider than 8. We set
the beam-width to 12 for all of our experiments, and use 15 training iterations.

\subsection{Features}

We base our feature set on the arc-hybrid features used by
\citet{goldberg:13},
with the additional disfluency features used by \citet{honnibal:14}.  The
specific feature templates used are provided in the supplementary materials.

The templates refer to various combinations of the word-form, part-of-speech tag,
and the Brown cluster \citep{brown:92} of various tokens
in the context.  The tokens are the top three words of the stack, the left and
right subtree of the top word of the stack, the first three words of the buffer,
and the left subtree of the first word of the buffer.  Additionally, we follow
\citet{honnibal:14} in using the leftmost and rightmost edge of the top word
of the stack and the first word of the buffer as contextual tokens.
We used the Brown cluster mapping computed by \citet{liang:05}.
We follow \citet{honnibal:14}
in using 4- and 6-bit prefixes of the clusters in our feature templates, as
initially suggested by \citet{koo:10}.

\subsection{Qian and Liu (2013) Disfluency Detector}

Our parser performs disfluency detection jointly during parsing, using the Edit
transition described by \citet{honnibal:14}.  For comparison, we also trained
and evaluated the disfluency detection system of \citet{qian:13} on both
segmented and unsegmented input.

The \citet{qian:13} system uses a cascade of \textsc{m3n} sequence-tagging
models.  The first pass detects filler-words, the next pass uses the filler-word
predictions as features, and two subsequent passes detect disfluencies.
The system is a good comparison point because it achieves
equivalent, state-of-the-art accuracy to the \citet{honnibal:14} system,
with a very different algorithm.

Because the system does not require syntactically-annotated training data, it
can be trained from the \textsc{dps} files, which make more training data
available than the syntactically-annotated \textsc{mrg} files.  

To promote direct comparison, we trained the system from the \textsc{mrg}
annotations, and followed the syntactic disfluency annotations (i.e., words
were marked disfluent if they were part of the yield of an \textsc{edited} node).
Interestingly, this resulted in slightly higher accuracy than 
\citet{qian:13} report.  It seems that the \textsc{mrg} annotated speech repairs
are easier to detect than the \textsc{dps} annotations.

\subsection{CRF Segmenter}
\label{sec:crf}

To evaluate our joint model, we prepared a sequence-based segmentation system,
following the approach of \citet{liu:05}.  We used the \textsc{crf} implementation
provided by the Wapiti toolkit \citep{wapiti}.
%\footnote{\url{http://wapiti.limsi.fr/}}
The segmentation problem was modelled as a binary classification task, with
segment-initial
%\footnote{We
%found segment-initial encoding slightly better, due to the high frequency of
%conjunctions at the start of segments.}
tokens labelled 1 and all other tokens labelled 0.
Features referred to the word, part-of-speech tag, prefix, and suffix of the
target and surrounding tokens, as well as the previous two labelling decisions.
Weights were learned using the \textsc{l-bfgs} algorithm,
with an elastic-net penalty tuned on the development data.  The pattern file,
which describes the exact feature-templates, is attached in the supporting
materials.

\section{Development Results}

\label{sec:results}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrrr}
        \hline
               & \multicolumn{3}{c}{Segmentation} \\
               & Acc. & $P$ & $R$ & $F$ \\
        \hline
        CRF              & 96.6 & 87.4 & 78.0 & 82.4 \\
        Joint (explicit) & 96.9 & 84.0 & 85.9 & 84.9 \\
        Joint (implicit) & 96.9 & 83.9 & 86.2 & 85.0 \\
        \hline
    \end{tabular}
    \caption{\small Segmentation evaluation on the development set, for
    the CRF model and our joint approaches.\label{tab:sbd_eval}}
    %\vspace*{-5em}
\end{table}

\subsection{Segmentation Accuracy}

Table \ref{tab:sbd_eval} compares the per-token segmentation accuracy of the
three systems, along with their precision, recall and $F$-measure at identifying
segment-initial words.  Low recall indicates under-segmentation, while low precision
indicates over-segmentation.  Our segmentation evaluation excludes disfluent words.
When a disfluent word begins a segment, we move its segment label back to the first
fluent word.

%We evaluate segmentation accuracy as a binary classification task, where each
%word is assigned a label
%indicating whether it is the first word in a segment.  The \textbf{CRF}
%model learns these labels explicitly.  The \textbf{Joint (explicit)} model
%learns to insert segment boundaries as part of the transition system. In
%the \textbf{Joint (implicit)} model, the segmentation is derived deterministically
%from the predicted dependency analysis.

The two syntactic systems achieve equivalent accuracy, with similar precision/recall
bias.  Because the parser is incremental, it is easy to capture the information
available to the \textsc{crf} system, with the added advantage of long-range
syntactic features.  This gives the parsing models a small accuracy advantage
over the \textsc{crf}.

\subsection{Disfluency Detection Accuracy}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr|rrr}
        & \multicolumn{3}{c|}{Segmented}  & \multicolumn{3}{c}{Unsegmented} \\
        System & $P$ & $R$ & $F$ & $P$ & $R$ & $F$ \\
        \hline \hline
        Qian \& Liu '13         & 90.6 & 80.7  & 85.3  & 83.8 & 76.5 & 80.0 \\
        Pipeline                & 92.4 & 76.8  & 84.1  & 81.9 & 73.6 & 77.6 \\ 
        Joint (explicit)        & \multicolumn{3}{c|}{n/a} & 88.6 & 68.9 & 77.6 \\
        Joint (implicit)        & \multicolumn{3}{c|}{n/a} & 88.3 & 68.9 & 77.4 \\
        \hline
   \end{tabular}
    \caption{\small Disfluency detection evaluation on the development set,
             with and without gold segment boundaries on the input.
         \label{tab:dfl}}
%\vspace*{-5em}
\end{table}

Table \ref{tab:dfl} shows the accuracy of the systems at detecting speech-repair
disfluencies.  We first follow previous work in evaluating our model given
gold-standard segment boundaries (\textbf{Segmented}).  The \textbf{Pipeline}
system is the parsing model described in Section \ref{sec:model}, which is based
on the \citet{honnibal:14} model, but makes use of the arc-hybrid transition
system, with a feature set adjusted accordingly. We also train the model to
detect other disfluency types during parsing, by splitting the Edit
transition with different labels.

While \citet{honnibal:14} found that their system achieved slightly higher
accuracy than \citet{qian:13}, our model's accuracy is slightly lower.  It
may be that the Edit transition does not work as well with the arc-hybrid
system as it does with the arc-eager system that \citeauthor{honnibal:14} employ.
Alternatively, our feature set may be less well-tuned for disfluency detection.

Disfluency detection accuracy is substantially lower for all systems when
gold-standard segment boundaries were not available (the \textbf{Unsegmented}
column).  We found the \citet{qian:13} system's drop in accuracy particularly
interesting.  For the parser-based systems, we expected the unsegmented input
to reduce parse quality, leading to lower disfluency scores.  However, the
\citet{qian:13} result suggests that the segment boundaries serve as useful clues
in themselves.


%We first compare our model to previous disfluency detection systems on the
%Switchboard development data, using gold-standard segmentation.  The model
%performs similarly to the state-of-the-art \citet{honnibal:14} system it is based
%on, demonstrating that our adoption of the arc-hybrid transition system did not
%affect accuracy. Both systems perform slightly better than the \citet{qian:13}
%sequence-based model.
%
%\textbf{Unsegmented.}
%We next trained and evaluated the \citet{qian:13} model on unsegmented text.
%The gold-standard segment boundaries turn out to be surprisingly informative
%for the disfluency detection task: without them, accuracy drops from 83.4\% to
%78.2\%.
%
%The drop in accuracy of the \citet{qian:13} system is important context
%for the performance of the joint model on unsegmented text, which see
%a similar drop in accuracy.  Without the Yang and Liu result, it would seem
%that the lack of segmentation disrupted the parse accuracy of the joint model,
%causing its loss of disfluency accuracy.  However, it seems that the gold-standard
%segment boundaries were of direct benefit for disfluency detection.
%
%\textbf{CRF segmentation.} 
%Finally, we compare the fully joint model to a pipeline system, where the input
%is pre-segmented using the \textsc{crf} model described in Section \ref{sec:crf}.
%This system achieved 76.3\% accuracy, slightly below the accuracy of the fully
%joint model.
%
\subsection{Parsing Accuracy}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr}
        \hline
        System & \textsc{uas} & \textsc{las} \\
        \hline \hline
        Gold $\rightarrow$ Parser & 90.9 & 88.0 \\
        \hline
        CRF $\rightarrow$  Parser & 86.2 & 83.4 \\
        Joint (explicit)          & 87.9 & 85.1 \\
        Joint (implicit)          & 88.1 & 85.3 \\
        \hline
    \end{tabular}
    \caption{\small Unlabelled and labelled (parse) attachment scores on the
        development data.
        \label{tab:parse}}
\vspace*{-3em}
\end{table}


Table \ref{tab:parse} shows the labelled (\textsc{las}) and unlabelled (\textsc{uas})
dependency accuracies of the systems, on the development data.  With gold-standard
segmentation (\textbf{Gold $\rightarrow$ Parser}), the system achieves 90.9\%
\textsc{uas}, similar to the result reported by \citet{honnibal:14}.  When
the same system was given input segmented using the \textsc{crf} model described
in Section \ref{sec:crf}, accuracy fell to 86.2\%.

Both of the joint models, \textbf{Joint (explicit)} and \textbf{Joint (implicit)},
achieve substantially higher parse accuracies than the \textsc{crf} system.
The joint models allow the parser to segment the input while assigning the
dependency parse, either by employing an explicit transition, or by simply
attaching segment-governors to the root node.  The two strategies yield similar
accuracy, with a small advantage to the implicit segmentation.

\subsection{Efficiency}

%\textbf{Gold segmentation, gold disfluency detection.}
%We first establish the accuracy of the system with oracle segmentation and
%disfluency detection prior to parsing.  In this condition, the model achieves
%TODO\% \textsc{uas}, similar to the result reported by \citet{honnibal:14}.
%
%\textbf{Gold segmentation, joint disfluency detection.} We then evaluated the
%system using oracle segmentation, but using the Edit transition to do joint
%disfluency detection and parsing.  Again,  results were similar to those reported
%by \citet{honnibal:14}.
%
%\textbf{Unsegmented.}
%
%\textbf{Unsegmented, gold disfluency detection.}  We then trained and evaluated
%the parser on unsegmented input, with disfluencies removed using the gold-standard
%annotation.  The purpose of this experiment was to investigate how much of the
%drop in parse accuracy from unsegmented input was due to the increased difficulty
%of the disfluency detection task.
%
%\textbf{CRF segmentation.}
\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr}
        \hline
        System & Seg. & Disfl. & UAS \\
        \hline \hline
        Qian \& Liu '13          & ---  & 79.2 & --- \\
        CRF $\rightarrow$ Parser & 96.7 & 76.3 & 85.9 \\
        Joint (explicit)         & 97.3 & 76.9 & 87.8 \\
        Joint (implicit)         & 97.2 & 76.7 & 87.9 \\
        \hline
    \end{tabular}
    \caption{\small Final evaluation scores.}
    %\vspace*{-4em}
\end{table}

%\subsection{Disfluencies and Segmentation}

%Table \ref{tab:dfl} compares the accuracy of several systems at detecting speech-repair
%disfluencies on the Switchboard corpus --- the standard evaluation for disfluency
%detection.  The first two systems used gold-standard segment boundaries, followed
%by the joint parser (\textbf{S (Gold) $\rightarrow$ D+P}) and the \citet{qian:13}
%disfluency detection system (\textbf{S (Gold) $\rightarrow$ Q\&L}).  These systems
%roughly repeat the experiment described by \citet{honnibal:14}, with similar
%results.

%The \textbf{S (CRF) $\rightarrow$ D+P} system 

%system used gold-standard segment boundaries,
%before joint parsing and disfluency detection.  This is the experimental
%set-up of \citet{honnibal:14}.  Our system achieves similar
%accuracy to what they report.

%TODO: Gold-standard disfluency, predicted segmentation



\section{Final Evaluation}

\newpage

\section{Related Work}

\begin{itemize}
    \item \citet{honnibal:14}
    \item \citet{rasooli:13}
    \item Brian Roark's reranking-based segmentation system
    \item Qian and Liu (2013) disfluency detection system
    \item Johnson and Charniak systems
    \item RT'04 systems
\end{itemize}

\newpage

\section{Conclusion}

\newpage

\bibliography{main}
\bibliographystyle{aclnat}


\end{document}
