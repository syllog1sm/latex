% vim: set textwidth=78 fo+=t :

\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{amsmath, amsthm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{tikz-dependency}
\usepackage{placeins}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{lcovington}
\usepackage{cancel}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}
\newcommand{\edittrans}{\textsc{edit}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\newcommand{\tuple}[1]{$\langle#1\rangle$}
\newcommand{\maybe}[1]{\textcolor{gray}{#1}}
\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\state}{\mathcal{S}}
\newcommand{\nmae}{\textsc{nmae}\xspace}
\newcommand{\pcfg}{\textsc{pcfg}\xspace}

\newcommand{\szero}{S0\xspace}
\newcommand{\nzero}{N0\xspace}

\newcommand{\szeroH}{S0$_h$\xspace}
\newcommand{\szeroHH}{S0$_{h2}$\xspace}
\newcommand{\szeroL}{S0$_L$\xspace}
\newcommand{\szeroLL}{S0$_{L2}$\xspace}
\newcommand{\szeroR}{S0$_R$\xspace}
\newcommand{\szeroRR}{S0$_{R2}$\xspace}
\newcommand{\szeroLzero}{S0$_{L0}$\xspace}
\newcommand{\szeroRzero}{S0$_{R0}$\xspace}

\newcommand{\nzeroL}{N0$_L$\xspace}
\newcommand{\nzeroLL}{N0$_{LL}$\xspace}
\newcommand{\nzeroLzero}{N0$_{L0}$\xspace}

\newcommand{\szeroRedge}{S0$_{re}$\xspace}
\newcommand{\szeroLedge}{S0$_{le}$\xspace}
\newcommand{\nzeroLedge}{N0$_{le}$\xspace}

\newcommand{\sparseval}{\textsc{sparseval}\xspace}

\title{Incremental Parsing of Disfluent, Unsegmented Speech Transcripts}

\author{
	Anonymous\\
  	Department\\
  	Institution\\
  	Address\\
  {\tt \small email }\\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present a joint transition-based model of segmentation, disfluency detection
and dependency parsing for conversational speech.  We evaluate two ways of encoding
the segmentation decisions into the transition system, and compare against a pipeline
approach, where the input is segmented with a \textsc{crf} model before parsing.
The joint model achieves state-of-the-art accuracy at both parsing and disfluency
detection, outperforming the best stand-alone disfluency detector, and the
\textsc{crf}-based pipeline.
On the parsing evaluation, the joint model achieves 87.6\% \textsc{uas}, 1.5\%
better than the \textsc{crf}-based pipeline.
\end{abstract}

% P1
\section{Introduction}


Previous speech understanding systems have required that the input be
pre-segmented into sentence-like units.  However, the placement of the segment
boundaries is somewhat arbitrary.  For example, the following turn from the
Switchboard conversational speech transcripts is segmented into three utterances,
at the slashes:

\begin{lexample}
\small
uh and really we were really forced into keeping a budget because i 'm i 'm paid
once a month which sort of sort of forces some uh uh restrictions / \\
and you need to make sure all your bills are paid / \\
uh about yourself
\end{lexample}

\noindent An annotator could reasonably omit the first boundary, analysing
the first two units as a single conjoined sentence.  
We suspect this uncertainty in sentence boundary location
might be problematic for syntactic parsing, since the boundaries could
constrain the syntactic analyses in ways that are inconsistent with the parser's
expectations.  This motivated us to explore approaches where the syntactic parser
itself is responsible for identifying the locations of sentence boundaries.  Our
approach here is to associate entire discourse turns with a single syntactic
structure that consists of a sequence of sentences.
While the length of these turns makes conventional $O(n^3)$ parsing algorithms
impractical, incremental transition-based parsers now achieve high accuracy,
while running in time linear in the length of the input string.

We compared two ways of encoding the segmentation decisions into the transition
system against a standard pipeline approach, which used a \textsc{crf} model
to segment the input prior to parsing.  Our model is based on the system described
by \citet{honnibal:14}, who showed that a joint model of disfluency detection
and parsing produced state-of-the-art accuracy on both tasks.
Their experiments follow most previous work in using gold-standard segment boundaries.
In that condition, our model achieved similar parsing and disfluency
detection results: 90.5\% \textsc{uas}, and 84.1\% $F$-measure.
We then repeated the experiment with \textsc{crf}-predicted segmentation,
and found that accuracy dropped to 86.1\% and 76\%.
To test whether the syntactic model was particularly sensitive to segmentation,
we trained and evaluated the \citet{qian:13} \textsc{m3n} disfluency tagger on
unsegmented input, and found that its accuracy dropped by a similar amount.

We then compared the pipeline approach against our two joint models.
Despite slightly lower segmentation accuracy,
the best joint
model achieves 87.6\% parse accuracy, substantially higher than the 86.1\%
accuracy of the pipeline approach.
The two joint models performed similarly, indicating that a variety of 
transition-system encoding strategies could be equally successful.


\section{Spoken Language Understanding}

\begin{figure}
    \begin{tabular}{l}

        A flight to $\underbrace{\mathrm{um}}_\text{FP} \underbrace{\mathrm{Boston}}_\text{RM} \underbrace{\mathrm{I\;mean}}_\text{IM} \underbrace{\mathrm{Denver}}_\text{RP}$ Tuesday\\

\end{tabular}
\caption{\small A sentence with disfluencies annotated in the style of Shriberg (1994) 
    and the Switchboard corpus.
FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair.
We follow previous work in evaluating the system on the accuracy with which
it identifies speech-repairs, marked \emph{reparandum} above.
\label{fig:shriberg}}
\end{figure}


%Unscripted speech has several characteristics that make spoken language understanding
%a distinct problem from understanding written text.  Two such features are that
%it is \emph{continuous}, and that it is frequently \emph{disfluent}.

Well-edited written text can be segmented into sentences easily, using punctuation and
capitalisation.  Sentence boundary detection systems typically achieve accuracies
above 99\%.  For speech, the problem is much more difficult.  Previous work has
explored the idea that pauses could compensate
for the lack of orthographic cues \citep{gregory:04} to improve parse accuracy,
without success.  \citet{liu:05} found that a decision-tree model devoted to prosodic
features could improve a segmentation system, when mixed with a \textsc{crf} model that
considered ngram features.

As well as being continuous, unscripted speech is frequently disfluent.
Figure \ref{fig:shriberg} shows part of an utterance, with a speech-repair annotated
with the scheme devised by \citet{shriberg:94}.  The words marked \emph{reparandum}
are corrected by the speaker, often with an explicit editing term in the
\emph{interregnum}, before being replaced by the text marked \emph{repair}.
Speech repairs are particularly problematic for syntactic
parsers, because of the complicated dependency structures that can arise between the
reparandum, repair and the fluent sentence \citep{Johnson04a}.
In contrast, other types of disfluencies tend to be lexically restricted, and
tend to simply interrupt the fluent sentence, with limited syntactic interaction.

\begin{table}
\centering
\small
\begin{tabular}{l|rr}
    \hline
    Length & Segmented & Unsegmented \\
    \hline \hline
    1-2 & 26,140 & 10,225 \\
    2-5 & 15,727 & 6,914 \\
    5-10 & 23,283 & 6,885 \\
    10-20 & 19,738 &  8,934 \\
    20-50 & 6,406 & 9,765 \\
    50-100 & 161 & 2,106 \\
    100-200 & 0 & 257 \\
    200-422 & 0 & 20 \\
    \hline
    Total & 91,455 & 45,106 \\
    \hline
\end{tabular}
\caption{\small Input lengths in the Switchboard training corpus, with and without
    gold-standard segmentation.  For instance, with utterances pre-segmented,
    there are 161 sentences with between 51 and 100 tokens; in the unsegmented
corpus, there are 2,106 sentences with lengths in that range.
\label{tab:seg_freqs}}
\vspace*{-3em}
\end{table}

\newpage
\subsection{The Switchboard Corpus}
\label{sec:swbd}

The Switchboard portion of the Penn Treebank \citep{marcus:93} contains 1,126
transcripts of telephone calls between strangers, on an assigned topic.
Every file has been annotated for speech-repairs and other disfluencies (filled
pauses, parentheticals, discourse markers, etc); these annotations are provided
in the \textsc{dps} files.  Syntactic brackets (\textsc{mrg} files) are available
for 619,236 of the 1,482,845 words in the training sections of the corpus (2 and 3). 
All of the transcripts have also been annotated for various speech \emph{metadata},
including utterance segmentation, speech repairs per \citet{shriberg:94}, and 
non-repair disfluencies, such as filled pauses.  Table \ref{tab:dfl_freqs} shows the
frequency of these disfluencies in the training corpus.

\begin{table}
    \centering
    \small
    \begin{tabular}{lc|r}
\hline
Disfluency type & Example & Freq. \\
\hline \hline
Reparanda & \emph{to } \textbf{Boston} \emph{uh Denver} & 32,310 \\
Filled pauses    & \textbf{um}, \textbf{uh} & 20,502 \\
Edit terms & \textbf{I mean} & 3,447 \\ 
Discourse  & \textbf{well}, \textbf{you know} & 21,412  \\
Segment Conjunctions & \textbf{and}, \textbf{and so} & 25,624 \\
\hline
\end{tabular}
\caption{\small Frequencies of different disfluency types in Sections 2 and 3 of the
Switchboard \textsc{mrg} files.\label{tab:dfl_freqs}}
\vspace*{-0.5in}
\end{table}

Table \ref{tab:seg_freqs} shows how segmentation affects the length of training
inputs.  Without any utterance segmentation, the inputs consist of whole turns,
using the gold-standard diarisation in the \textsc{mrg} files.
Segmentation doubles the number of inputs (and so halves their length, on average);
i.e., on average each turn contains one sentence-medial segment boundary.

The Switchboard syntactic annotations come in the form of syntactic brackets.
We follow Honnibal and Johnson (2014) in using
the 2013-04-05 version of the Stanford dependency converter \citep{stanford_deps},
using the Basic Dependencies scheme,
which produces strictly projective representations.

We follow previous work on spoken language understanding by lower-casing the text
and removing punctuation and partial words (words tagged XX and words ending in
`-').  However, we depart from Honnibal and Johnson (2014) in not removing one-token
sentences, not removing filled pauses as a pre-process, and not
re-tokenising the common parentheticals \emph{you know} and \emph{i mean}.
We avoid these extra pre-processing steps in favour of extended disfluency processing,
by subtyping the Edit transition with different disfluency labels.

\section{Joint Disfluency Detection and Parsing}

\begin{figure}
    \centering
    \small
    \begin{tabular}{lr}
        \hline
        $(\sigma, i | \beta, A, L) \vdash (\sigma | i, \beta, A, L)$ & S \\
        $(\sigma | i | j, \beta, A, L) \vdash (\sigma | i, \beta, A(j)=i, L(j)=l$ & R$_l$ \\ 
        $(\sigma | i, j | \beta, A, L) \vdash (\sigma, j | \beta, A(i)=j, L(i)=l$ & L$_l$ \\
        $(\sigma | i, j | \beta, A, L) \vdash (\sigma | y, j | \beta, A(y)=y, L(y)=z)$ & E$_l$ \\
        Where \\
        $x_1...x_n$ are the former left children of $i$ \\
        $y = [x_1, x_n]$ \\
        $z = \mathrm{edited}$ \\
        %A \setminus \{(x, y, l) \;\mathrm{or}\; (y, x, l) : \forall x \in [i, j), \forall y \in \mathbb{N} \}$ \\
        %    $A'' = A' \cup \{(-, x, l) : \forall x \in [i, j) \} $ \\
        \hline
\end{tabular}
\caption{\small Transition system for the parser, with $\sigma$ denoting the
    stack, $\beta$ denoting the buffer, $A$ denoting the set of arcs, and $D$ the
    set of disfluent words.  The transitions are the arc-hybrid \textbf{S}hift,
    \textbf{R}ight and \textbf{L}eft, and the Honnibal and Johnson (2014) \textbf{E}dit.
    The R, L and E transitions are parameterised by label, $l$.
\label{fig:trans}}
\vspace*{-3em}
\end{figure}
Our model is based on the Honnibal and Johnson (2014) joint incremental disfluency
detection and parsing system.  This section provides a brief description of
the model, and highlights our departures from it.
Our novel contributions are described from Section 4 onwards.
%we adopt a different
%transition system --- \emph{arc hybrid} \citep{kuhlmann:11}, instead of
%\emph{arc eager} \citep{nivre:03}. 
%
% The change to the arc hybrid
%transition system necessitates some feature engineering, described in Section
%\ref{sec:features}.
%We briefly describe how the system works in this section, before introducing our
%novel contributions from Section 4 onwards.

\subsection{Transition-based Dependency Parsing}

We follow recent work on speech parsing in using an incremental, transition-based
dependency parser \citep{rasooli:13,honnibal:14}.
A transition-based parser \citep{nivre:03} consists of a configuration,
and a set of actions (or `transition system').  Actions are chosen from the
transition-system and applied to the state, until the stack is empty and the
buffer is exhausted.

A configuration $c = (\sigma, \beta, A, L)$,
where $\sigma$ and $\beta$ are disjoint sets of word
indices termed the \emph{stack} and \emph{buffer} respectively, $A$ is a
vector of head indices, and $L$ is a vector of dependency labels.  A dependency
arc from a head $h$ to a dependent $d$ with label $l$ is represented $A(d)=h$,
$L(d)=l$.  A word $d$ is marked disfluent by setting its head to itself, i.e.
$A(d)=d$.  Labels are used to distinguish the different types of dependencies,
e.g. filled pauses, speech repairs, etc.
A vertical bar is used to denote concatenation
to the stack or buffer, e.g. $\sigma | i$ indicates a stack with the topmost
element $i$ and remaining elements $\sigma$.

\subsection{Arc-Hybrid Transition System}

We depart from Honnibal and Johnson (2014) in using the arc-hybrid system \citep{kuhlmann:11},
instead of the arc-eager system \citep{nivre:03}. The two transition systems achieve
comparable accuracy \citep{goldberg:13}, but we find the arc hybrid system slightly
simpler.
%particularly when designing a training oracle for our extended transition
%system (Section \ref{sec:oracle}).
%The increased simplicity was useful for devising the transition strategies for segmentation
%described in Section \ref{sec:break_trans}. 

The arc-hybrid system, shown in Figure \ref{fig:trans}, defines the Left-Arc in
the same way as the \citet{nivre:03}
arc-eager system, but the Right-Arc creates an arc between the top two words of
the stack, following the arc-standard definition.

Unlike the arc-eager system,
arcs are only created when a word is \emph{popped}.
This means that there are never arcs to words on the stack.
The stack, buffer, and the words that have been assigned heads are three disjoint
sets.
%Another convenient property of the transition-system is that the word
%on top of the stack is the potential child of both Right-Arc and Left-Arc
%dependencies.
%In the arc-standard system, the Left-Arc adds an arc to
%the second word on the stack, and in the arc-eager system, the Right-Arc adds
%an arc to the first word of the buffer.

The arc-hybrid system maintains the simplicity advantages of arc-standard, which
have motivated \citet{huang:10} and others to continue working with it; but allows
training oracles to be defined easily, due to the \emph{arc reachability}
property that \citet{goldberg:13} show it shares with the arc-eager system.

\subsection{The Edit Transition}

We employ the Edit transition defined by Honnibal and Johnson (2014), to handle
speech repairs.
The Edit transition marks the word $i$ on top of the stack $\sigma | i$ as
disfluent, along with its rightward descendents --- i.e., all words in the
sequence $i...j-1$, where $j$ is the word at the start of the buffer. It then
restores the words both preceding and formerly governed by $i$ to the stack.

In other words, the word on top of the stack and its \emph{rightward descendents}
are all marked as disfluent, and the stack is popped. Its leftward children are
then restored to the stack, and all dependencies to and from words marked
disfluent are deleted.
\footnote{A word restored to the stack may be popped with either a Left-Arc, or a Right-Arc.
This flexibility is a potential
    advantage of the arc-hybrid system for the Edit transition. With the
arc-eager system, only the Left-Arc could be applied to the replaced words.}

%The arc-hybrid system offers a potential advantage for this definition of the Edit
%transition.  In the arc-eager system, used by \citet{honnibal:14}, when words
%were replaced on the stack, they could only be attached via the Left-Arc transition.
%The arc-hybrid system allows the Right-Arc to apply to these words as well.


\subsection{Training and Decoding}
We follow Honnibal and Johnson (2014) in employing beam-search decoding, and
use their training strategy: the \citet{sun:09} latent-variable
variant of the \citet{collins:02} structured perceptron, with weight updates
calculated from the \citet{huang:12} \emph{maximum violation} strategy.  We
also employ the path-length normalisation technique that Honnibal and Johnson (2014)
recommend, to deal with the variable-length transition histories the Edit transition:
may introduce. 
When calculating the figure-of-merit for the beam, we use the
\emph{mean transition score}, instead of the \emph{total transition score}.



\section{Joint Segmentation and Parsing}

%\begin{figure}
%\centering
%    \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
%    \begin{deptext}
%        Governor \& Governor \& Governor \& \textsc{root} \\
%    \end{deptext}
%    \depedge{4}{1}{}
%    \depedge{4}{2}{}
%    \depedge{4}{3}{}
%\end{dependency}
%\caption{\small Attachment style that arises from the Arc-from-Root segmentation strategy.
%         All segment governors are attached to the \textsc{root} symbol, via the Left-Arc.
%     \label{fig:left_arcs}}
%
%\vspace*{-1em}
%`\end{figure}

\begin{figure}
\begin{dependency}[theme=simple, edge unit distance=1.0ex]
    \begin{deptext}[row sep=2.0ex]
        I \& never \& wear \& heels \& they \& tire \& me \& out \& \textsc{root} \\
        I \& saw \& her \& wear \& heels \& \textsc{root} \\
    \end{deptext}
    \depedge{3}{1}{}
    \depedge{3}{2}{}
    \depedge{3}{4}{}
    \depedge{6}{5}{}
    \depedge{6}{7}{}
    \depedge{6}{8}{}

    \deproot[edge height=0.3cm, ultra thick]{9}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{6}{6}{}
    %\depedge[dashed]{9}{3}{}
    \depedge[dashed]{9}{6}{}


    \depedge[edge below]{2}{1}{}
    \depedge[edge below]{2}{3}{}
    \depedge[edge below]{4}{5}{}
    %\depedge[dashed, edge below]{6}{2}{}
    \depedge[dashed, edge below]{2}{4}{}

    \deproot[edge below, edge height=0.3cm, ultra thick]{6}{}
    \wordgroup{2}{2}{2}{}
    \wordgroup{2}{4}{4}{}
    \end{dependency}
    \vspace*{-2em}
\caption{\small Two parse states, showing a segmentation decision to be made using
    Strategy 1, Arc-from-Root.
    Words remaining on the stack are
    circled, and current arcs are drawn with solid lines.  The arrow indicates
    the start of the buffer.  In the top state, the correct move is a Left-Arc
    as there is no dependency between \emph{wear} and \emph{tired}.  In the
    lower state, the Right-Arc is correct.  The  dependencies that would be
added by these moves are shown with dashed lines.\label{fig:left_state}}
\vspace*{-3em}
\end{figure}

We now describe two ways of encoding utterance segmentation decisions into the
parser's transition system. 
The two strategies are evaluated in Section \ref{sec:results}.
We find that despite being quite different, the two strategies achieve similar
performance, and both substantially out-perform the pipeline approach described
in Section \ref{sec:crf}.  These results suggest that the details of how the
segmentation decisions are encoded into the transition system are less
important than the fact that the decisions are made during parsing, instead of
as a pre-process.

\subsection{Strategy 1: Arc-from-Root}

The first strategy we present uses the Left-Arc to attach each segment-governor
to the \textsc{root} symbol, which we place at the end of the input.
This way of encoding the segmentation decisions has the governors accumulate on
the stack, with the final decision to place segment boundaries between them only
made when the buffer is exhausted.  Because the arc-hybrid system is used, rather
than the arc-eager system, the head of a word is not determined when it is pushed
onto the stack --- only when it is popped.  Thus, even when the buffer is exhausted,
the parser may decide to create a dependency between two governors, instead of
using the Left-Arc to attach them to the \textsc{root} symbol.

Figure \ref{fig:left_state} shows two similar parse states, where this decision
arises.  In the first state, the correct decision is \textbf{Left-Arc}, because
a segment boundary should be inserted between \emph{wear} and \emph{tires}.
In the second state, the correct decision is \textbf{Right-Arc}, because
\emph{wear} is an argument of \emph{saw}.  Such decisions may be difficult,
so delaying them until the potential left and right heads can be compared in the
same state may be advantageous.

\subsection{Strategy 2: the Break Transition}

\begin{figure}
\begin{dependency}[theme=simple, edge unit distance=1.0ex]
    \begin{deptext}[row sep=2.0ex]
        I \& never \& wear \& heels \& they \& tire \& me \& out \& \textsc{root} \\
    \end{deptext}
    \depedge{3}{1}{}
    \depedge{3}{2}{}
    \depedge{3}{4}{}

    \deproot[edge height=0.3cm, ultra thick]{5}{}
    \wordgroup{1}{3}{3}{}
    \depedge[dashed]{9}{3}{}
    \end{dependency}
    \caption{\small Parse state at which a segmentation decision must be made
        using the Break transition.  The governor of the previous segment,
        \emph{wear},  is
        the only word on the stack (circled), and the first word of the
        next segment, \emph{they}, is at the start of the buffer (indicated by
        an arrow). After the Break transition is applied, \emph{wear} is popped
    from the stack, and the dashed arc is added from the \textsc{root} symbol.
    \label{fig:break_state}}
\vspace*{-3em}
\end{figure}
The second strategy we present uses a specific Break transition.  The transition
is designed to be applied early, instead of late: it should be applied
when the first word of the new segment is at the start of the buffer.  This is
achieved by applying a pre-condition to the transition, that prevents it from being
applied if the word at the start of the buffer has any
leftward children, and that the stack must contain exactly one word.

Figure \ref{fig:break_state} shows the state at which the transition should
be applied.  The governor of the previous segment is the only word remaining
on the stack, and the word at the start of the buffer has no children, as it
is the start of a new segment.
The resulting state is shown below.  The governor has been popped from the stack,
and an arc has been added between it and the Root symbol. 

\subsection{Training Oracle}

We follow \citet{honnibal:14} in training our model using the latent-variable
structured perceptron algorithm \citep{sun:09}.  The procedure relies on a training
oracle, which maps a parse-state and a gold-standard analysis to a set of gold-standard
actions.
%A training oracle for the arc-hybrid system was described by
%\citet{goldberg:13}.  We first extend their training oracle for the Edit transition,
%before extending the resulting oracle for our two segmentation strategies.

A transition is considered gold-standard for a given configuration if it does
not render any gold-standard arcs \emph{newly unreachable}.  That is, if the
best parse achievable from the result of applying the action scores no worse
than the best parse achievable before the action was applied. 

\citet{goldberg:13} define exact training oracles for the arc-eager, arc-hybrid,
and easy-first transition systems. The oracles are exact in the sense that they
allow the calculation of a cost, $\mathcal{C}$, of applying any action to any state, where
$\mathcal{C}$ is the difference in the optimal scores reachable before and after the action
was applied.

\citet{honnibal:14} do not give an exact oracle for their Edit transition. Instead,
they describe how they modify the arc-eager oracle defined by \citet{goldberg:12},
to calculate the set of gold-standard actions from a given configuration.  Their
oracle is inexact in the sense that exact costs are not calculated; actions are
simply marked zero-cost or non-zero cost, which is sufficient for the training
algorithm.

We adopt this approximation, and determine the set of gold-standard actions
as follows.  We begin with the arc-hybrid oracle defined by \citet{goldberg:13},
who prove that in the arc-hybrid system, an arc $(h, d)$ is reachable from a
configuration $c$ if one of the following conditions hold:

\begin{enumerate}
    \item $(h, d)$ is already derived (i.e. $A(d)=h$);
    \item $h$ and $d$ are in the buffer;
    \item $h$ is on the stack and $d$ is in the buffer;
    \item $d$ is on the stack and $h$ is in the buffer;
    \item $h$ is immediately behind $d$ on the stack.
\end{enumerate}

\noindent A word is marked as disfluent by setting its head to itself, i.e. it is an arc
$(d, d)$.  
The Edit transition adds the following ways to reach an arc $(h, d)$:

\begin{enumerate}
    \setcounter{enumi}{5}
    \item $h=d$ and $d$ is on the stack;
    \item $h=d$, $d$ is the leftward child of a word $s$, and $s$ is on the stack;
    \item $d$ is the first leftward child of a word $s$, $s$ is the $i$th word
          on the stack, and $h$ is the $i-1$ word on the stack;
    \item $d$ is the $i$th leftward child of a word $s$, $h$ is the $i-1$
          leftward child of $s$, and $s$ is on the stack.
\end{enumerate}

Conditions 6 and 7 are common to the system of \citet{honnibal:14}, while
conditions 8 and 9 are introduced by our use of the arc-hybrid system.  Conditions
7, 8 and 9 all rely on marking a word $s$ as disfluent.  If $s$ is not disfluent
in the gold-standard, then arcs recovered this way will incur additional loss.
\newpage

\section{Experiments}

We use the Switchboard portion of the Penn Treebank \citep{marcus:93}, as
described in Section \ref{sec:swbd}, to train and evaluate our models.  Unfortunately,
this complicates comparison with most of the prior work on utterance segmentation,
which used the RT'04 shared-task data distributed by \textsc{darpa}.  We use
the Switchboard corpus for consistency with previous work on disfluency 
detection and parsing \citep{rasooli:13,honnibal:14,qian:13}.

We follow the pre-processing and dependency conversion steps described in
Section \ref{sec:swbd}: the text was lower-cased, partial words were removed,
and the phrase-structure trees were converted into projective-dependency parses
using the Stanford Dependency Converter \citep{stanford_deps}.
We use the standard train/dev/test split from \citet{Charniak01a}: Sections 2
and 3 for training, and Section 4 divided into three held-out sections, the first
of which is used for final evaluation.
We follow \citet{honnibal:14} in using the \sparseval \citep{sparseval}
metric to evaluate our parser, using their implementation.

We follow \citet{Johnson04a} and others in restricting our disfluency evaluation
to speech repairs, which we identify as words that have a node labelled \textsc{edited}
as an ancestor in the Switchboard phrase-structure trees.  We also follow them
in training only on the \textsc{mrg} files, giving us 619,236 words of training
data instead of the 1,482,845 used by other disfluency detection systems, such
as \citet{qian:13}.

We test for statistical significance in our results by training 20 models for
each experimental configuration, using different random seeds. The random seeds
control how the sentences are shuffled during training, which the perceptron
model is quite sensitive to.  We use the Wilcoxon rank-sums non-parametric test.
The standard deviation in \textsc{uas} for a sample was typically around TODO,
and TODO for disfluency $F$-measure.

The main hyper-parameters of our model are the number of training iterations,
and the beam-width, which controls a speed/accuracy trade-off.  We found that
only small accuracy improvements were obtained with beams wider than 8, and follow
\citet{honnibal:14} in using 15 training iterations.  

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr}
               & \multicolumn{3}{c}{Disfluency} \\
        System & $P$ & $R$ & $F$ \\
        \hline \hline
        S+D+P (Arc-to-Root) & 89.4 & 67.0 & 77.0 \\
        S+D+P (Break) & 88.2 & 68.4 & 76.5 \\
        S (CRF) $\rightarrow$ D+P & 79.5 & 73.4 & 76.3 \\
        \hline
        Q\&L  & & & 78.2 \\
        \hline
        S (Gold) $\rightarrow$ Q\&L & & & 83.4 \\
        S (Gold) $\rightarrow$ D+P & 94.2 & 76.5 & 84.4 \\
        \hline
    \end{tabular}
    \caption{\small Speech-repair disfluency detection precision, recall and
        F-measure on the development data.
        \textbf{S+D+P (Arc-to-Root)}=Joint segmentation, disfluency detection
        and parsing with the Arc-to-Root strategy.
        \textbf{S+D+P (Break)}=Joint
        segmentation, disfluency detection and parsing with the Break strategy.
        \textbf{S (CRF) $\rightarrow$ D+P}=CRF segmentation, followed by joint
        disfluency detection and parsing.
        \textbf{Q\&L}=The Qian and Liu (2013)
        disfluency detection system, on unsegmented input.
        \textbf{S (Gold) $\rightarrow$ Q\&L} The Qian and Liu (2013) disfluency
        detection system, on input with gold-standard segmentation.
        \textbf{S (Gold) $\rightarrow$ D+P} Joint disfluency detection and parsing,
             on input with gold-standard segmentation. \label{tab:dfl}}
\vspace*{-4em}
\end{table}

\subsection{Features}

We base our feature set on the arc-hybrid features used by
\citet{goldberg:13},
%\footnote{\url{http://bitbucket.org/yoavgo/tacl2013dynamicoracles/src/efc0b61c4f45331bf887dd26580465964014c28f/lefttoright/features/extractors.py?at=default}},
with the additional disfluency features used by \citet{honnibal:14}.  The
specific feature templates used are provided in the supplementary materials.

The templates refer to various combinations of the word-form, part-of-speech tag,
and the Brown cluster \citep{brown:92} of various tokens
in the context.  The tokens are the top three words of the stack, the left and
right subtree of the top word of the stack, the first three words of the buffer,
and the left subtree of the first word of the buffer.  Additionally, we follow
\citet{honnibal:14} in using the leftmost and rightmost edge of the top word
of the stack and the first word of the buffer as contextual tokens.

%We used the Brown cluster mapping computed by \citet{liang:05}.
%\footnote{\url{http://www.metaoptimize.com/projects/wordreps}}
%We follow \citet{honnibal:14}
%in using 4- and 6-bit prefixes of the clusters in our feature templates, as
%initially suggested by \citet{koo:10}.

\subsection{CRF Baseline}
\label{sec:crf}

To evaluate our joint model, we prepared a sequence-based segmentation system,
following the approach of \citet{liu:05}.  We used the \textsc{crf} implementation
provided by the Wapiti toolkit \citep{wapiti}.
%\footnote{\url{http://wapiti.limsi.fr/}}
The segmentation problem was modelled as a binary classification task, with
segment-final tokens labelled 1 and all other tokens labelled 0.
Features referred to the word, part-of-speech tag, prefix, and suffix of the
target and surrounding tokens, as well as the previous two labelling decisions.
Weights were learned using the \textsc{l-bfgs} algorithm,
with an elastic-net penalty tuned on the development data.  The pattern file,
which describes the exact feature-templates, is attached in the supporting
materials.

\section{Results}
\label{sec:results}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr}
        System & \textsc{uas} & \textsc{las} & Seg. \\
        \hline \hline
        S+D+P (Arc-to-Root) & 87.7 & & 96.2 \\
        S+D+P (Break) & 87.6 & & 96.2 \\
        \hline
        S (CRF) $\rightarrow$ D+P & 86.1 & & 96.8 \\
        S (Gold) $\rightarrow$ D+P & 90.5 & & 100 \\
        \hline
    \end{tabular}
    \caption{\small Unlabelled and labelled (parse) attachment scores on the
        development data, for two joint segmentation, disfluency detection
        and parsing systems,
        and two systems where the input was pre-segmented.
        \label{tab:parse}}
\end{table}


\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr}
        System & UAS & Disfl. $F$ & Seg. \\
        \hline \hline
        Joint (Arc-to-Root) & 87.7 & 76.6 & 96.2 \\
        S (CRF) $\rightarrow$ D+P & 86.1 & 76.3 & 96.8 \\
        \hline
    \end{tabular}
    \caption{\small Final evaluation scores.}
    \vspace*{-4em}
\end{table}

\subsection{Disfluencies and Segmentation}

The \textbf{S (Gold) $\rightarrow$ D+P} system used gold-standard segment boundaries,
before joint parsing and disfluency detection.  This is the experimental
set-up of \citet{honnibal:14}.  Our system achieves similar
accuracy to what they report.

Disfluency detection accuracy is substantially lower for all systems when
gold-standard segment boundaries are not available.  This could indicate that
the system's disfluency accuracy is highly sensitive to parse quality, which
is reduced when gold-standard segment boundaries are not available.
To investigate this, we trained and evaluated the \citet{qian:13} disfluency
detection system on unsegmented input.  This system uses a cascade of
sequence-tagging models, and does not predict any syntactic information beyond
\textsc{pos} tags.
We found that the \citeauthor{qian:13} system's disfluency $F$-measure dropped
from 83.9\% to 77\% when gold-standard segment boundaries were not available.
This suggests to us that segment boundaries are highly informative
for disfluency detection, which is why our system experiences a similar drop
in accuracy when gold-standard segment boundaries are not available.

\subsection{Segmentation and Parsing Accuracy}

\subsection{Choice of Segmentation Strategy}

\subsection{Comparison with CRF Pipeline}

\section{Analysis}


\section{Related Work}

\section{Conclusion}


\bibliography{main}
\bibliographystyle{aclnat}


\end{document}
