% vim: set textwidth=78 fo+=t :

\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{amsmath, amsthm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{tikz-dependency}
\usepackage{placeins}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{lcovington}
\usepackage{cancel}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}
\newcommand{\edittrans}{\textsc{edit}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\newcommand{\tuple}[1]{$\langle#1\rangle$}
\newcommand{\maybe}[1]{\textcolor{gray}{#1}}
\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\state}{\mathcal{S}}
\newcommand{\nmae}{\textsc{nmae}\xspace}
\newcommand{\pcfg}{\textsc{pcfg}\xspace}

\newcommand{\szero}{S0\xspace}
\newcommand{\nzero}{N0\xspace}

\newcommand{\szeroH}{S0$_h$\xspace}
\newcommand{\szeroHH}{S0$_{h2}$\xspace}
\newcommand{\szeroL}{S0$_L$\xspace}
\newcommand{\szeroLL}{S0$_{L2}$\xspace}
\newcommand{\szeroR}{S0$_R$\xspace}
\newcommand{\szeroRR}{S0$_{R2}$\xspace}
\newcommand{\szeroLzero}{S0$_{L0}$\xspace}
\newcommand{\szeroRzero}{S0$_{R0}$\xspace}

\newcommand{\nzeroL}{N0$_L$\xspace}
\newcommand{\nzeroLL}{N0$_{LL}$\xspace}
\newcommand{\nzeroLzero}{N0$_{L0}$\xspace}

\newcommand{\szeroRedge}{S0$_{re}$\xspace}
\newcommand{\szeroLedge}{S0$_{le}$\xspace}
\newcommand{\nzeroLedge}{N0$_{le}$\xspace}

\newcommand{\sparseval}{\textsc{sparseval}\xspace}

\title{Incremental Parsing of Disfluent, Unsegmented Speech Transcripts}

\author{
	Anonymous\\
  	Department\\
  	Institution\\
  	Address\\
  {\tt \small email }\\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present a joint transition-based model of segmentation, disfluency detection
and dependency parsing.  We evaluate two ways of encoding the segmentation
decisions into the transition system, and compare against a pipeline approach,
where the input is segmented with a \textsc{crf} model before parsing.
We find that the joint model achieves 1.5\% better parse accuracy, and 0.5\% better
disfluency $F$-measure.
Our final unlabelled parse accuracy is 87.6\%, the best score on
unsegmented speech input that we are aware of.
\end{abstract}

% P1
\section{Introduction}

Previous speech understanding systems have required that the input be
pre-segmented into sentence-like units. 
For example, the following turn from one of the Switchboard conversational speech
transcripts is segmented into three utterances, at the slashes:

\begin{lexample}
\small
uh and really we were really forced into keeping a budget because i 'm i 'm paid once a month which sort of sort of forces some uh uh restrictions / \\
and you need to make sure all your bills are paid / \\
uh about yourself
\end{lexample}

\noindent 
The assumption has been that segmentation
is necessary to make parsing tractable, as until recently the most accurate parsing
systems were polynomial time.  Disfluencies such as filled-pauses and speech
errors have also been addressed through pre-processes.
The main strategy of previous work has been to adopt a pipeline
architecture, where the input is progressively transformed to resemble well-edited
written texts.

However, recent work has shown that a pipeline architecture is inferior for detecting
speech-repairs.  \citet{rasooli:13} and Honnibal and Johnson (2014)
describe two joint
transition-based models that detect speech-repair disfluencies at parse-time,
instead of as a pre-process.  Honnibal and Johnson show that their model out-performs
a pipeline of state-of-the-art components.  We adopt a similar model, although
we adopt a transition-system simplification from \citet{kuhlmann:11}, and integrate
the detection of filled-pauses and other non-repair disfluencies into the model.

In this paper, we show that this joint approach can be successfully extended
to segmentation.  We present a joint transition-based model that predicts
segment boundaries, repair and non-repair disfluencies, syntactic heads, and
dependency labels.  The system runs in expected linear-time, and achieves
state-of-the-art accuracy.

For comparison, we prepared a standard \textsc{crf}-based segmentation system,
which achieved 96.5\% accuracy.  Features were based on the word and part-of-speech
tag context.  No phonetic features were used in any of our experiments.

We find that the joint model performs significantly better than the pipeline approach.
We attribute the difference in performance to error-propagation
problems inherent in the pipeline approach.  When the segmenter makes an error,
it leaves the parser with mistaken input, for which the parser may not be able
to find a satisfactory parse.  Jointly determining the segmentation and parsing
solves this problem.  Despite slightly lower segmentation accuracy, the joint
model achieves 87.6\% parse accuracy, substantially higher than the 86.1\%
accuracy of the pipeline approach.

\begin{figure}
    \begin{tabular}{l}

        A flight to $\underbrace{\mathrm{um}}_\text{FP} \underbrace{\mathrm{Boston}}_\text{RM} \underbrace{\mathrm{I\;mean}}_\text{IM} \underbrace{\mathrm{Denver}}_\text{RP}$ Tuesday\\

\end{tabular}
\caption{\small A sentence with disfluencies annotated in the style of Shriberg (1994) 
    and the Switchboard corpus.
FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair.
We follow previous work in evaluating the system on the accuracy with which
it identifies speech-repairs, marked \emph{reparandum} above.
\label{fig:shriberg}}
\end{figure}


\section{Spoken Language Understanding}

Unscripted speech has several characteristics that make spoken language understanding
a distinct problem from understanding written text.  Two such features are that
it is \emph{continuous}, and that it is frequently \emph{disfluent}.

Well-edited text can be segmented into sentences easily, using punctuation and
capitalisation.  Sentence boundary detection systems typically achieve accuracies
above 99\%.  For speech, the problem is much more difficult.  Previous work has
explored the idea that pauses and other phonetic features could compensate
for the lack of orthographic cues \citep{mischa:??}.  However, even the
successful attempts to use phonetic features have yielded only small accuracy
improvements \citep{isi}.

As well as being continuous, unscripted speech is frequently disfluent.
Figure \ref{fig:shriberg} shows part of an utterance, with a speech-repair annotated
with the scheme devised by \citet{shriberg:94}.  The words marked \emph{reparandum}
are corrected by the speaker, often with an explicit editing term in the
\emph{interregnum}, before being replaced by the text marked \emph{repair}.
Speech repairs are a particularly problematic for syntactic
parsers, because of the complicated dependency structures that can arise between the
reparandum, repair and the fluent sentence \citep{Johnson04a}.

\begin{table}
\centering
\small
\begin{tabular}{l|rr}
    \hline
    Length & Segmented & Unsegmented \\
    \hline \hline
    1-2 & 26,140 & 10,225 \\
    2-5 & 15,727 & 6,914 \\
    5-10 & 23,283 & 6,885 \\
    10-20 & 19,738 &  8,934 \\
    20-50 & 6,406 & 9,765 \\
    50-100 & 161 & 2,106 \\
    100-200 & 0 & 257 \\
    200-500 & 0 & 20 \\
    \hline
    Total & 91,455 & 45,106 \\
    \hline
\end{tabular}
\caption{\small Input lengths in the Switchboard training corpus, with and without
    gold-standard segmentation.  For instance, with utterances pre-segmented,
    there are 161 sentences with between 51 and 100 tokens; in the unsegmented
corpus, there are 2,106 sentences with lengths in that range.
\label{tab:seg_freqs}}
\vspace*{-3em}
\end{table}

\subsection{The Switchboard Corpus}

The Switchboard portion of the Penn Treebank \citep{marcus:93} contains 1,126
transcripts of telephone calls between strangers, on an assigned topic.
Every file has been annotated for speech-repairs and other disfluencies (filled
pauses, parentheticals, discourse markers, etc); these annotations are provided
in the \textsc{dps} files.  Syntactic brackets (\textsc{mrg} files) are available
for 619,236 of the 1,482,845 words in the training sections of the corpus (2 and 3). 
All of the transcripts have also been annotated for various speech \emph{metadata},
including utterance segmentation, speech repairs per \citet{shriberg:94}, and 
non-repair disfluencies, such as filled pauses.  Table \ref{tab:dfl_freqs} shows the
frequency of these disfluencies in the training corpus.

\begin{table}
    \centering
    \small
    \begin{tabular}{lc|r}
\hline
Disfluency type & Example & Freq. \\
\hline \hline
Reparanda & \emph{to } \textbf{Boston} \emph{uh Denver} & 32,310 \\
Filled pauses    & \textbf{um}, \textbf{uh} & 20,502 \\
Edit terms & \textbf{I mean} & 3,447 \\ 
Discourse  & \textbf{well}, \textbf{you know} & 21,412  \\
Segment Conjunctions & \textbf{and}, \textbf{and so} & 25,624 \\
\hline
\end{tabular}
\caption{\small Frequencies of different disfluency types in Sections 2 and 3 of the
Switchboard \textsc{mrg} files.\label{tab:dfl_freqs}}
\vspace*{-0.5in}
\end{table}

Table \ref{tab:seg_freqs} shows how segmentation affects the length of training
inputs.  Without any utterance segmentation, the inputs consist of whole turns,
using the gold-standard diarisation in the \textsc{mrg} files.
Segmentation doubles the number of inputs (and so halves their length, on average);
i.e., on average each turn contains one segment boundary.

The Switchboard syntactic annotations come in the form of syntactic brackets.
We follow Honnibal and Johnson (2014) in using
the 2013-04-05 version of the Stanford dependency converter \citep{stanford_deps},
using the Basic Dependencies scheme,
which produces strictly projective representations.

We follow previous work on disfluency detection by lower-casing the text and
removing punctuation and partial words (words tagged XX and words ending in
`-').  However, we depart from Honnibal and Johnson (2014) in not removing one-token
sentences, and not removing filled pauses as a pre-process, and not
re-tokenising the common parentheticals \emph{you know} and \emph{i mean}.
We avoid these extra pre-processing steps in favour of extended disfluency processing,
using a new transition we dub \textbf{Delete}.  This transition is described in
Section \ref{sec:delete}.

\section{Joint Disfluency Detection and Parsing}

Our model is based on the joint incremental disfluency detection and parsing
system described by Honnibal and Johnson (2014), although we adopt a different
transition system --- \emph{arc hybrid} \citep{kuhlmann:11}, instead of
\emph{arc eager} \citep{nivre:03}.  The two transition systems achieve comparable
accuracy, but we find the arc hybrid system slightly simpler.  The increased
simplicity was useful for devising the transition strategies for segmentation
described in Section \ref{sec:break_trans}.  The change to the arc hybrid
transition system necessitates some feature engineering, described in Section
\ref{sec:features}.
We briefly describe how the system works in this section, before introducing our
novel contributions from Section 4 onwards.

\subsection{Transition-based Dependency Parsing}

A transition-based parser consists of a configuration (or `state'), and a set
of actions (or `transition system').  Actions are chosen from the transition-system
and applied to the state, until a termination condition is reached.

For us, a state is a 4-tuple $c = (\sigma, \beta, A, L)$, where $\sigma$ and $\beta$
are disjoint sets of word indices termed the \emph{stack} and \emph{buffer}
respectively, and $A$ is a vector of head indices representing the dependency parse,
and $L$ is a vector of dependency labels.  Parsing terminates when the stack is
empty, and the buffer is exhausted.

\subsection{Arc-Hybrid Transition System}

We depart from Honnibal and Johnson (2014) in using the arc-hybrid system \citep{kuhlmann:11},
instead of the arc-eager system \citep{nivre:03}.
The arc-hybrid transition system \citep{kuhlmann:11} consists of three types of actions:

\begin{itemize}
    \item Shift. Push the first word of the buffer onto the stack, removing it
          from the buffer.
      \item Left$_L$. Set the head of the word on top of the stack to be the
            word at the start of the buffer, and assign it the label $L$. Pop
            the top word from the stack.
      \item Right$_L$. Set the head of the word on top of the stack to be the
            word immediately behind it on the stack, and assign it the label $L$.
            Pop the top word from the stack.
\end{itemize}

The actions are summarised in Figure \ref{fig:arc_hybrid}.  We find it easier
to reason about transition systems based on the arc-hybrid system, because it has
several convenient properties.

First, the set of words in the stack, and the set
of words that have been assigned a head, are disjoint in the arc-hybrid
system.  In the arc-eager system, words on the stack may or may not
have a head set.
Another convenient property is thatbBoth the Left-Arc and the Right-Arc assign
a head to the word on top of the stack.  In contrast, in the arc-standard system,
the Left-Arc assigns a head to the \emph{second} word on the stack,
while in the arc-eager system the Right-Arc assigns a head to the
first word of the buffer.  Because of this,
both the Left-Arc and the Right-Arc
\emph{pop} the stack. In the arc-eager system, Right-Arc pushes the stack.

The convenient properties of the arc-hybrid system removes the need for the
complicated pre-conditions that the arc-eager system requires to ensure
parsing is monotonic \citep{honnibal:13}.  It is also easier to reason about
which actions will make some arcs newly unreachable.  \citet{goldberg:13}
give a \emph{dynamic oracle} for the arc-hybrid system, which we use as
part of our training algorithm.


\subsection{The Edit Transition}

We employ the Edit transition defined by Honnibal and Johnson (2014), to handle
speech repairs.
The Edit transition marks the word $i$ on top of the stack $\sigma | i$ as
disfluent, along with its rightward descendents --- i.e., all words in the
sequence $i...j-1$, where $j$ is the word at the start of the buffer. It then
restores the words both preceding and formerly governed by $i$ to the stack.

In other words, the word on top of the stack and its \emph{rightward descendents}
are all marked as disfluent, and the stack is popped. We then restore its
leftward children to the stack, and
all dependencies to and from words marked disfluent are deleted. 
The words restored to the stack may
become rightward children of the word behind them on the stack, or leftward
children of a word from the buffer.

\subsection{Training and Decoding}

We follow Honnibal and Johnson (2014) in employing beam-search decoding, and
use their training strategy, which employs the \citet{sun:09} latent-variable
variant of the \citet{collins:02} structured perceptron, with weight updates
calculated from the \citet{huang:12} \emph{maximum violation} strategy.  We
also employ the path-length normalisation technique that Honnibal and Johnson (2014)
recommend: when calculating the figure-of-merit for the beam, we use the
\emph{mean transition score}, instead of the \emph{total transition score}.
The adjustment is necessary because variable length transition histories are
possible when the Edit transition is employed.

\subsection{Features}
\label{sec:features}

\subsection{POS Tagging}

\clearpage

\section{Joint Segmentation and Parsing}

\subsection{Segment (leaf)}

The ``leaf'' version of the Segment transition, abbreviated S$_L$, is applied when
the last word of a segment is on the stack, and the first word of the next segment
is at the start of the buffer.

Additionally, the Shift, Left and Edit moves are updated with a additional
pre-condition, that stipulates they cannot be applied when S0 and N0 have
been assigned different sentence IDs.  The transition increments the sentence
ID of N0, which acts as a flag that blocks these transitions until the stack is clear.
While the parser is in this `break mode', the only transitions available are
Right and Filler.

The pre-conditions on the transition stipulate that it can only be applied when
S0 has no right children, and N0 has no left children.  This ensures the segment
boundary is inserted between two leaves of the tree.

\subsection{Segment (governors)}

In this segmentation strategy, the governors are accumulated on the stack, and
popped once the parser reaches the end of the buffer.  The segment boundary is
inserted between their rightmost and leftmost leaf words.
The transition is stated:

\begin{eqnarray}
    (\sigma | i | j, \emptyset , A, D) \vdash (\sigma, | i, \emptyset, A, D, S_i \ne S_j ) & \mathrm{S}_G
\end{eqnarray}

That is, if there are at least two words $i$ and $j$ on the stack, and the buffer
is empty, pop $j$ and assert that $i$ and $j$ do not belong to the same segment.

\subsection{Segment between governor and leaf}

In this segmentation strategy, the governors are popped from the stack one at
a time, when the first word of the next segment is at the start of the buffer.

sentence-final punctuation prediction method described by \citet{zhang:13},
in that it 

\clearpage
\section{Delete Transition}


\clearpage
\section{Baseline CRF System}

\section{Experiments}

\section{Results}

\subsection{Choice of Segmentation Strategy}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr}
        System & Parse & Disfl. & Segments \\
        \hline \hline
        S$_G$ & & & \\
        S$_{GL}$  & & & \\
        S$_L$  & & & \\
        \hline
    \end{tabular}
    \caption{\small Comparison of segmentation strategies.}
\end{table}


\subsection{Comparison with Pipeline}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr}
        System & Parse & Disfl. & Segments \\
        \hline \hline
        Seg. $\rightarrow$ Disfl. + Parse & & & \\
        Seg. + Disfl. + Parse  & & & \\
        \hline
        O.Seg. $\rightarrow$ Disfl. + Parse & & & \\
        O.Seg. $\rightarrow$ O.Disfl. $\rightarrow$ Parse & & & \\
        \hline

    \end{tabular}
    \caption{\small Comparison between pipeline, joint, and oracle-pipeline
             approaches.}
\end{table}


\subsection{Test Set Evaluation}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr}
        System & Parse & Disfl. & Seg. \\
        \hline \hline
        Seg. $\rightarrow$ Disfl. + Parse & & & \\
        Seg. + Disfl. + Parse  & & & \\
    \end{tabular}
    \caption{\small Final evaluation.}
\end{table}

%\clearpage

%\section{Related Work}

%\section{Conclusion}
\bibliography{main}
\bibliographystyle{aclnat}


\end{document}
