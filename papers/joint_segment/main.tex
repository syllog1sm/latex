% vim: set textwidth=78 fo+=t :

\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{amsmath, amsthm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{tikz-dependency}
\usepackage{placeins}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{lcovington}
\usepackage{cancel}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}
\newcommand{\edittrans}{\textsc{edit}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\newcommand{\tuple}[1]{$\langle#1\rangle$}
\newcommand{\maybe}[1]{\textcolor{gray}{#1}}
\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\state}{\mathcal{S}}
\newcommand{\nmae}{\textsc{nmae}\xspace}
\newcommand{\pcfg}{\textsc{pcfg}\xspace}

\newcommand{\szero}{S0\xspace}
\newcommand{\nzero}{N0\xspace}

\newcommand{\szeroH}{S0$_h$\xspace}
\newcommand{\szeroHH}{S0$_{h2}$\xspace}
\newcommand{\szeroL}{S0$_L$\xspace}
\newcommand{\szeroLL}{S0$_{L2}$\xspace}
\newcommand{\szeroR}{S0$_R$\xspace}
\newcommand{\szeroRR}{S0$_{R2}$\xspace}
\newcommand{\szeroLzero}{S0$_{L0}$\xspace}
\newcommand{\szeroRzero}{S0$_{R0}$\xspace}

\newcommand{\nzeroL}{N0$_L$\xspace}
\newcommand{\nzeroLL}{N0$_{LL}$\xspace}
\newcommand{\nzeroLzero}{N0$_{L0}$\xspace}

\newcommand{\szeroRedge}{S0$_{re}$\xspace}
\newcommand{\szeroLedge}{S0$_{le}$\xspace}
\newcommand{\nzeroLedge}{N0$_{le}$\xspace}

\newcommand{\sparseval}{\textsc{sparseval}\xspace}

\title{Incremental Parsing of Disfluent, Unsegmented Speech Transcripts}

\author{
	Anonymous\\
  	Department\\
  	Institution\\
  	Address\\
  {\tt \small email }\\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Conversational speech turns are often long, motivating the use of a segmentation
model to pre-process the input into shorter units.  We instead propose a
transition-based model that processes entire turns, jointly performing dependency
parsing and disfluency detection.  Segment boundaries can then be recovered
from the syntactic structure if they are required.

We compare the system against a pipeline approach, where the input is segmented
with a \textsc{crf} model before parsing.  We find that the pipeline approach
suffers from error-propagation, while the joint model achieves
state-of-the-art accuracies.
It achieves 87.9\% parse accuracy, a 2.0\% improvement over the pipeline approach.
\end{abstract}

% P1
\section{Introduction}


Previous speech understanding systems have required that the input be
pre-segmented into sentence-like units.  However, sentence boundaries are not
always clear in the speech input, which lacks case distinctions and punctuation,
and is not reliably segmented by pauses.
For example, the following turn from the
Switchboard conversational speech transcripts is segmented into three utterances,
at the slashes:

\begin{lexample}
\small
uh and really we were really forced into keeping a budget because i 'm i 'm paid
once a month which sort of sort of forces some uh uh restrictions / \\
and you need to make sure all your bills are paid / \\
uh about yourself
\end{lexample}

\noindent An annotator could reasonably omit the first boundary, analysing
the first two units as a single conjoined sentence.  
We suspect this arbitrariness in sentence boundary location
might be problematic for syntactic parsing, since the boundaries could
accidentally constrain the syntactic analyses in ways that are inconsistent with
the parser's expectations.  This motivated us to explore approaches where the
syntactic parser itself is responsible for identifying the locations of sentence
boundaries.  Our approach here is to associate entire discourse turns with a
single syntactic structure that consists of a sequence of sentences.
While the length of these turns makes conventional $O(n^3)$ parsing algorithms
impractical, incremental transition-based parsers now achieve high accuracy,
while running in time linear in the length of the input string.
Our parsing model is based on the system described
by \citet{honnibal:14}, who showed that a joint model of disfluency detection
and parsing produced state-of-the-art accuracy on both tasks, when gold-standard
segment boundaries are available.

We tried two ways of parsing unsegmented inputs.  Our first approach introduces
a new transition, Break, to explicitly predict segment boundaries during parsing.
The second approach leaves the segmentation implicit until
parsing is complete, at which point the boundaries can be read-off from the
predicted parse.

We find that both ways of parsing unsegmented input result in superior accuracy
to a pipeline approach, where the input is pre-segmented by a \textsc{crf} model.
The joint models prove advantageous at all three evaluations: segmentation,
disfluency detection and parsing.
The improvement is particularly substantial for parsing, where the
pipeline approach results in 85.9\% \textsc{uas}, while the best joint model
achieves 87.9\%, a new state-of-the-art.  
Our model is also highly efficient, processing approximately 900 words per second.


\section{Spoken Language Understanding}

\begin{figure}
    \begin{tabular}{l}
        \small

        A flight to $\underbrace{\mathrm{um}}_\text{FP} \underbrace{\mathrm{Boston}}_\text{RM} \underbrace{\mathrm{I\;mean}}_\text{IM} \underbrace{\mathrm{Denver}}_\text{RP}$ Tuesday\\

\end{tabular}
\caption{\small A sentence with disfluencies annotated in the style of Shriberg (1994) 
    and the Switchboard corpus.
FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair.
%We follow previous work in evaluating the system on the accuracy with which
%it identifies speech-repairs, marked \emph{reparandum} above.
\label{fig:shriberg}}
\end{figure}

\begin{table}
    \centering
    \small
    \begin{tabular}{lc|r}
\hline
Disfluency type & Example & Freq. \\
\hline \hline
Speech-repair & \emph{to } \textbf{Boston} \emph{uh Denver} & 32,310 \\
Filled pauses    & \textbf{um}, \textbf{uh} & 20,502 \\
Edit terms & \textbf{I mean} & 3,447 \\ 
Discourse  & \textbf{well}, \textbf{you know} & 21,412  \\
Segment Conjunctions & \textbf{and}, \textbf{and so} & 25,624 \\
\hline
\end{tabular}
\caption{\small Frequencies of different disfluency types in Sections 2 and 3 of the
Switchboard \textsc{mrg} files.\label{tab:dfl_freqs}}
\vspace*{-4em}
\end{table}


A verbatim, unpunctuated speech transcript has very different linguistic characteristics
from well-edited written text, even in the absence of speech-recognition errors.
Speech transcripts pose
two challenges for natural language understanding technologies in particular.
The \textbf{segmentation} problem arises because the speech stream is continuous,
and  pauses are less syntactically informative than careful punctuation \citep{gregory:04}.
The \textbf{disfluency} problem arises due to language performance problems ---
speakers \emph{um} and \emph{uh}, edit their utterances on the fly, and
frequently insert parentheticals.

%Unscripted speech has several characteristics that make spoken language understanding
%a distinct problem from understanding written text.  Two such features are that
%it is \emph{continuous}, and that it is frequently \emph{disfluent}.

Well-edited written text can be segmented into sentences easily, using punctuation and
capitalisation.  Sentence boundary detection systems typically achieve accuracies
above 99\% on clean text input, while state-of-the-art speech segmentation systems
achieve only 96-97\% accuracy (i.e. a 3-times higher error-rate).
Segmentation is typically a \emph{pre-}process for parsing, limiting segmentation
systems to ngram-based features, and acoustic cues --- which have proven difficult
to utilise \citep{liu:05}.  These local features cannot accurately identify
segment boundaries, if segments are identified with tensed clauses, since they can
contain multiple long-range dependencies.

As well as being continuous, unscripted speech is frequently disfluent.
Figure \ref{fig:shriberg} shows part of a disfluent utterance, annotated according to
\citet{shriberg:94}.
%The words marked \emph{reparandum}
%are corrected by the speaker, often with an explicit editing term in the
%\emph{interregnum}, before being replaced by the text marked \emph{repair}.
Speech repairs are particularly problematic for syntactic
parsers, because of the complicated dependency structures that can arise between the
reparandum, repair and the fluent sentence \citep{Johnson04a}.
%In contrast, other types of disfluencies tend to be lexically restricted, and
%tend to simply interrupt the fluent sentence, with limited syntactic interaction.

\begin{table}
\centering
\small
\begin{tabular}{l|rr}
    \hline
    Length & Segmented & Unsegmented \\
    \hline \hline
    1-2 & 26,140 & 10,225 \\
    2-5 & 15,727 & 6,914 \\
    5-10 & 23,283 & 6,885 \\
    10-20 & 19,738 &  8,934 \\
    20-50 & 6,406 & 9,765 \\
    50-100 & 161 & 2,106 \\
    100-200 & 0 & 257 \\
    200-422 & 0 & 20 \\
    \hline
    Total & 91,455 & 45,106 \\
    \hline
\end{tabular}
\caption{\small Input lengths in the Switchboard training corpus, with and without
    gold-standard segmentation.  For instance, with utterances pre-segmented,
    there are 161 sentences with between 51 and 100 tokens; in the unsegmented
corpus, there are 2,106 sentences with lengths in that range.
\label{tab:seg_freqs}}
\vspace*{-2em}
\end{table}

\newpage
\subsection{The Switchboard Corpus}
\label{sec:swbd}

The Switchboard portion of the Penn Treebank \citep{marcus:93} contains 1,126
transcripts of telephone calls between strangers on an assigned topic.
Every file has been annotated for speech-repairs and other disfluencies (filled
pauses, parentheticals, discourse markers, etc); these annotations are provided
in the \textsc{dps} files.  Syntactic brackets (\textsc{mrg} files) are available
for 619,236 of the 1,482,845 words in the training sections of the corpus (2 and 3). 
All of the transcripts have also been annotated for various speech \emph{metadata},
including utterance segmentation, speech repairs per \citet{shriberg:94}, and 
non-repair disfluencies, such as filled pauses.  Table \ref{tab:dfl_freqs} shows the
frequency of these disfluencies in the training corpus.

Table \ref{tab:seg_freqs} shows how segmentation affects the length of training
inputs.  Without any utterance segmentation, the inputs consist of whole turns,
using the gold-standard diarisation in the \textsc{mrg} files.
Segmentation doubles the number of segments (and so halves their length, on average);
i.e., on average each turn contains one sentence-medial segment boundary.

%The Switchboard syntactic annotations come in the form of syntactic brackets.
%We follow Honnibal and Johnson (2014) in using
%the 2013-04-05 version of the Stanford dependency converter \citep{stanford_deps},
%using the Basic Dependencies scheme,
%which produces strictly projective representations.

We follow previous work on spoken language understanding by lower-casing the text
and removing punctuation and partial words (words tagged XX and words ending in
`-').  However, we depart from Honnibal and Johnson (2014) in not removing one-token
sentences, not removing filled pauses as a pre-process, and not
re-tokenising the common parentheticals \emph{you know} and \emph{i mean}.
We avoid these extra pre-processing steps in favour of extended disfluency processing,
by subtyping the Edit transition with different disfluency labels, as described
in Section \ref{sec:edit}.

\section{Joint Disfluency Detection and Parsing}
\label{sec:model}


\begin{figure}
    \centering
    \small
    \begin{tabular}{lr}
        \hline
        $(\sigma, i | \beta, \mathbf{A}, \mathbf{L}) \vdash (\sigma | i, \beta, \mathbf{A}, \mathbf{L})$ & S \\
        $(\sigma | i | j, \beta, \mathbf{A}, \mathbf{L}) \vdash (\sigma | i, \beta, \mathbf{A}(j)=i, \mathbf{L}(j)=\ell)$ & R$_\ell$ \\ 
        $(\sigma | i, j | \beta, \mathbf{A}, \mathbf{L}) \vdash (\sigma, j | \beta, \mathbf{A}(i)=j, \mathbf{L}(i)=\ell)$ & L$_\ell$ \\
        $(\sigma | i, \beta, \mathbf{A}, \mathbf{L}) \vdash (\sigma | x_1 | ... | x_n, \beta, \mathbf{A}(\gamma)=\gamma, \mathbf{L}(\gamma)=\ell)$ & E$_\ell$ \\
        Where \\
        $x_1...x_n$ are the former left children of $i$ \\
        $\gamma$ is $i$ and its rightward subtree \\
        %A \setminus \{(x, y, l) \;\mathrm{or}\; (y, x, l) : \forall x \in [i, j), \forall y \in \mathbb{N} \}$ \\
        %    $A'' = A' \cup \{(-, x, l) : \forall x \in [i, j) \} $ \\
        \hline
\end{tabular}
\caption{\small Transition system for the parser, with $\sigma$ denoting the
    stack, $\beta$ denoting the buffer, $\mathbf{A}$ denoting a vector of head
    indices, and $\mathbf{L}$ a vector of arc labels.
    The transitions are the arc-hybrid \textbf{S}hift,
    \textbf{R}ight and \textbf{L}eft, and the Honnibal and Johnson (2014) \textbf{E}dit.
    The R, L and E transitions are parameterised by label, $\ell$.
\label{fig:trans}}
\vspace*{-3em}
\end{figure}
Our model is based on the Honnibal and Johnson (2014) joint incremental disfluency
detection and parsing system.  This section provides a brief description of
the model, and highlights our departures from it.
Our novel contributions are described from Section 4 onwards.
%we adopt a different
%transition system --- \emph{arc hybrid} \citep{kuhlmann:11}, instead of
%\emph{arc eager} \citep{nivre:03}. 
%
% The change to the arc hybrid
%transition system necessitates some feature engineering, described in Section
%\ref{sec:features}.
%We briefly describe how the system works in this section, before introducing our
%novel contributions from Section 4 onwards.

\subsection{Transition-based Dependency Parsing}

We follow recent work on speech parsing in using an incremental, transition-based
dependency parser \citep{rasooli:13,honnibal:14}.
A transition-based parser \citep{nivre:03} consists of a configuration,
and a set of actions (or `transition system').  Actions are chosen from the
transition-system and applied to the state, until a terminal configuration is
reached.

A configuration $c = (\sigma, \beta, \mathbf{A}, \mathbf{L})$,
where $\sigma$ and $\beta$ are disjoint sets of word
indices termed the \emph{stack} and \emph{buffer} respectively, $\mathbf{A}$ is a
vector of head indices, and $\mathbf{L}$ is a vector of dependency labels.  A
dependency arc from a head $h$ to a dependent $d$ with label $\ell$ is represented
$\mathbf{A}(d)=h$, $\mathbf{L}(d)=\ell$.  A word $d$ is marked disfluent by setting
its head to itself, i.e. $\mathbf{A}(d)=d$.  Labels are used to distinguish the
different types of disfluencies, e.g. filled pauses, speech repairs, etc.
A vertical bar is used to denote concatenation
to the stack or buffer, e.g. $\sigma | i$ indicates a stack with the topmost
element $i$ and remaining elements $\sigma$.

\subsection{Arc-Hybrid Transition System}

We depart from Honnibal and Johnson (2014) in using the arc-hybrid system \citep{kuhlmann:11},
instead of the arc-eager system \citep{nivre:03}. The two transition systems achieve
comparable accuracy \citep{goldberg:13}, but we find the arc hybrid system slightly
simpler.
%particularly when designing a training oracle for our extended transition
%system (Section \ref{sec:oracle}).
%The increased simplicity was useful for devising the transition strategies for segmentation
%described in Section \ref{sec:break_trans}. 

The arc-hybrid system, shown in Figure \ref{fig:trans}, defines the Left-Arc in
the same way as the \citet{nivre:03}
arc-eager system, but the Right-Arc creates an arc between the top two words of
the stack, following the arc-standard definition.

Unlike the arc-eager system,
arcs are only created when a word is \emph{popped}.
This means that there are never arcs to words on the stack.
The stack, buffer, and the words that have been assigned heads are three disjoint
sets.
%Another convenient property of the transition-system is that the word
%on top of the stack is the potential child of both Right-Arc and Left-Arc
%dependencies.
%In the arc-standard system, the Left-Arc adds an arc to
%the second word on the stack, and in the arc-eager system, the Right-Arc adds
%an arc to the first word of the buffer.

The arc-hybrid system maintains the simplicity advantages of arc-standard, which
have motivated \citet{huang:10} and others to continue working with it; but allows
training oracles to be defined easily, due to the \emph{arc decomposable}
property that \citet{goldberg:13} show it shares with the arc-eager system.

\subsection{The Edit Transition}
\label{sec:edit}
We employ the Edit transition defined by Honnibal and Johnson (2014), to handle
speech repairs.
The Edit transition marks the word $i$ on top of the stack $\sigma | i$ as
disfluent, along with its rightward descendents --- i.e., all words in the
sequence $i...j-1$, where $j$ is the leftmost edge of the word at the start of
the buffer. It then restores the words both preceding and formerly governed by
$i$ to the stack.

%The leftward children are restored so that the parser can more easily process
%sentences where the head of a constituent is disfluent, but its leftward children
%are not. For instance, in \emph{pass the red square uh rectangle}, the parser
%might decide to attach the fluent words \emph{the} and \emph{red} to the disfluent
%word \emph{square}.  Because these words will be returned to the stack when
%the Edit transition is employed, they can later be attached correctly.

\citet{honnibal:14} only apply the Edit transition to speech-repair disfluencies.
They pre-process the input to remove \emph{uh} and \emph{um} tokens, and merge
the common parenthetical \emph{you know} into a single token.
We instead extend the Edit transition to the other disfluency types described
in Table \ref{tab:dfl_freqs}, with the exception of segment conjunctions, which
are simply conjunctions that occur at the beginning of a segment.

\subsection{Training and Decoding}

We follow Honnibal and Johnson (2014) in employing beam-search decoding, and
use their training strategy: the \citet{sun:09} latent-variable
variant of the \citet{collins:02} structured perceptron, with weight updates
calculated with the \citet{huang:12} \emph{maximum violation} strategy.  We
also employ the path-length normalisation technique that Honnibal and Johnson (2014)
recommend, to deal with the variable-length transition histories the Edit transition
may introduce: 
when calculating the figure-of-merit for the beam, we use the
\emph{mean transition score}, instead of the \emph{total transition score}.


\section{Joint Segmentation and Parsing}

%\begin{figure}
%\centering
%    \begin{dependency}[theme=simple, segmented edge, edge unit distance=1.0ex]
%    \begin{deptext}
%        Governor \& Governor \& Governor \& \textsc{root} \\
%    \end{deptext}
%    \depedge{4}{1}{}
%    \depedge{4}{2}{}
%    \depedge{4}{3}{}
%\end{dependency}
%\caption{\small Attachment style that arises from the Arc-from-Root segmentation strategy.
%         All segment governors are attached to the \textsc{root} symbol, via the Left-Arc.
%     \label{fig:left_arcs}}
%
%\vspace*{-1em}
%`\end{figure}

\begin{figure}
\begin{dependency}[theme=simple, edge unit distance=1.0ex]
    \begin{deptext}[row sep=2.0ex]
        I \& never \& wear \& heels \& they \& tire \& me \& out \& \textsc{root} \\
        I \& saw \& her \& wear \& heels \& \textsc{root} \\
    \end{deptext}
    \depedge{3}{1}{}
    \depedge{3}{2}{}
    \depedge{3}{4}{}
    \depedge{6}{5}{}
    \depedge{6}{7}{}
    \depedge{6}{8}{}

    \deproot[edge height=0.3cm, ultra thick]{9}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{6}{6}{}
    %\depedge[dashed]{9}{3}{}
    \depedge[dashed]{9}{6}{}


    \depedge[edge below]{2}{1}{}
    \depedge[edge below]{2}{3}{}
    \depedge[edge below]{4}{5}{}
    %\depedge[dashed, edge below]{6}{2}{}
    \depedge[dashed, edge below]{2}{4}{}

    \deproot[edge below, edge height=0.3cm, ultra thick]{6}{}
    \wordgroup{2}{2}{2}{}
    \wordgroup{2}{4}{4}{}
    \end{dependency}
    \vspace*{-2em}
\caption{\small Two parse states, showing a segmentation decision to be made using
    Strategy 1, implicit segmentation.
    Words remaining on the stack are
    circled, and current arcs are drawn with solid lines.  The arrow indicates
    the start of the buffer.  In the top state, the correct move is a Left-Arc
    as there is no dependency between \emph{wear} and \emph{tired}.  In the
    lower state, the Right-Arc is correct.  The  dependencies that would be
added by these moves are shown with dashed lines.\label{fig:left_state}}
%\vspace*{-3em}
\end{figure}

We now describe two ways of encoding utterance segmentation decisions into the
parser's transition system.  In the first strategy, the segmentation is determined
from the parse structure; in the second strategy, a distinct transition, Break,
is added to the transition system to insert segment boundaries.
The two strategies are evaluated in Section \ref{sec:results}.
We find that despite being quite different, the two strategies achieve similar
performance.

\subsection{Strategy 1: Implicit segmentation}

The first strategy we present uses the Left-Arc to attach each segment-governor
to the \textsc{root} symbol, which we place at the end of the input.
This way of encoding the segmentation decisions has the governors accumulate on
the stack, with the final decision to place segment boundaries between them only
made when the buffer is exhausted.  Because the arc-hybrid system is used, rather
than the arc-eager system, the governor of a word is not determined when it is pushed
onto the stack --- only when it is popped.  Thus, even when the buffer is exhausted,
the parser may decide to create a dependency between two words, instead of
using the Left-Arc to attach them to the \textsc{root} symbol.

Figure \ref{fig:left_state} shows two similar parse states, where this decision
arises.  In the first state, the correct decision is \textbf{Left-Arc}, because
a segment boundary should be inserted between \emph{wear} and \emph{tires}.
In the second state, the correct decision is \textbf{Right-Arc}, because
\emph{wear} is an argument of \emph{saw}.  Such decisions may be difficult,
so delaying them until the potential left and right heads can be compared in the
same state may be advantageous.

\subsection{Strategy 2: Explicit Break transition}
\label{sec:break}
\begin{figure}
\begin{dependency}[theme=simple, edge unit distance=1.0ex]
    \begin{deptext}[row sep=2.0ex]
        I \& never \& wear \& heels \& they \& tire \& me \& out \& \textsc{root} \\
    \end{deptext}
    \depedge{3}{1}{}
    \depedge{3}{2}{}
    \depedge{3}{4}{}

    \deproot[edge height=0.3cm, ultra thick]{5}{}
    \wordgroup{1}{3}{3}{}
    \depedge[dashed]{9}{3}{}
    \end{dependency}
    \caption{\small A parse state illustrating segmentation using Strategy 2,
        the Break transition.
        The governor of the previous segment,
        \emph{wear},  is
        the only word on the stack (circled), and the first word of the
        next segment, \emph{they}, is at the start of the buffer (indicated by
        an arrow). After the Break transition is applied, \emph{wear} is popped
    from the stack, and the dashed arc is added from the \textsc{root} symbol.
    \label{fig:break_state}}
%\vspace*{-3em}
\end{figure}
The second strategy we present uses a specific Break transition.  
The design of the transition was inspired by the work of \citet{dongzhang:13},
who describe a joint transition-based model of dependency parsing and punctuation
prediction.  The task of predicting sentence-final punctuation, and our task of
utterance segmentation, are closely related, so we experimented with their
approach.

\citet{dongzhang:13} train their model to insert sentence-final punctuation when
the stack is fully connected and the first word of a new sentence is at the start
of the buffer. Our Break transition operates in a similar fashion.
The transition is applied when there is exactly one word on the stack, and the
word at the start of the buffer has no leftward children.  The word on the stack
is arced to the \textsc{root} symbol, and the stack is popped:
\vspace*{-1em}
\begin{center}
    \begin{tabular}{l}
        $(i, j | \beta | n, \mathbf{A}, \mathbf{D}) \vdash (\emptyset, j | \beta | n, \mathbf{A}(i)=n, \mathbf{L}(i)=\textsc{r}) $ \\
    Where $\{x, ..., j : \mathbf{A}(x)=j \} = \emptyset$ \\
    \end{tabular}
\end{center}
The transition
is designed to be applied early, instead of late: it should be applied
when the first word of the new segment is at the start of the buffer.  This is
guaranteed by the pre-condition, which prevents it from being applied if the word at the start of the buffer has any leftward children.

Figure \ref{fig:break_state} shows a state at which the transition should
be applied.  The governor of the previous segment is the only word remaining
on the stack, and the word at the start of the buffer has no children, as it
is the start of a new segment.  In the resulting state (shown below), the governor
has been popped from the stack (it is no longer circled), and it has been arced
to the \textsc{root} symbol.

\subsection{Training Oracle}

We follow \citet{honnibal:14} in training our model using the latent-variable
structured perceptron algorithm \citep{sun:09}.  As each training example is
received, we search for the highest-scoring transition sequence, and the highest-scoring
\emph{gold} transition sequence.  The search for the best gold-standard sequence
requires a training oracle, which maps a parse-state and a gold-standard derivation
to a set of gold-standard actions.  An action is considered gold-standard if
it is a step towards the best possible continuation, i.e. the
transition sequence that will yield the highest-scoring analysis reachable from
the current configuration.

\citet{goldberg:13} give a training oracle for the arc-hybrid system.  The oracle
amounts to the following rules, where $\beta_0$ refers to the first word of the
buffer, $\sigma_0$ refers to the top word of the stack, and $\sigma_1$ refers
to the second word on the stack.  We denote the vector of gold-standard
head indices $\mathbf{G}$, with
$\mathbf{G}(d)=h$ asserting that an arc from $h$
to $d$ is in the gold-standard.  The arc-hybrid training oracle consists of
the following rules, which determine which actions (abbreviated S, R and L here)
are gold-standard:

\begin{enumerate}
    \item If the stack is empty, S is the only gold action;
    \item If $\mathbf{G}(\beta_0)=\sigma_0$, S is a gold action; 
    \item If there are any other arcs between $\beta_0$ and the stack, S is 
          not a gold-standard action;
      \item If $\mathbf{G}(\sigma_0)=\sigma_1$, R is a gold action;
      \item If $\mathbf{G}(\sigma_0)=\beta_0$, L is a gold action;
    \item If there are any other arcs between $\sigma_0$ and the buffer, neither
          L nor R are gold actions.
\end{enumerate}

\noindent The \citet{honnibal:14} Edit transition adds the following rules to the oracle
above. Recall that $\mathbf{G}(d)=d$ asserts that a word $d$ is disfluent in the
gold-standard:

\begin{enumerate}
    \item If $\mathbf{G}(\sigma_0) = \sigma_0$, E is a gold action;
    \item If $\mathbf{G}(\sigma_0) \not= \sigma_0$, E is not a gold action;
    \item If $\mathbf{G}(\beta_0) = \beta_0$, both L and S are gold actions;
    \item If $\mathbf{G}(\sigma_1)= \sigma_1$ and $\mathbf{G}(\sigma_0) = \sigma_0$, R
        is a gold action;
    \item If $\mathbf{G}(\sigma_0) = \sigma_0$ but $\mathbf{G}(\beta_0) \not= \beta_0$,
          L is not a gold action;
      \item If $\mathbf{G}(\sigma_0) = \sigma_0$ but $\mathbf{G}(\sigma_1) \not= \sigma_1$,
          R is not a gold action.
\end{enumerate}

\noindent In the implicit segmentation strategy, the segment boundaries are inserted
via the standard Left-Arc transition, so no adjustment to the training oracle
is required.  In the explicit strategy, the new Break transition is used to insert the
segment boundaries. 

The Break transition is gold-standard if and only if its pre-conditions
are met, and the word on top of the stack and the word at the start of the buffer
do not belong to the same segment.  If this is the case, B is the only gold-standard
action.

\section{Experiments}

We use the Switchboard portion of the Penn Treebank \citep{marcus:93}, as
described in Section \ref{sec:swbd}, to train and evaluate our models.  Unfortunately,
this complicates comparison with most of the prior work on utterance segmentation,
which used the RT'04 shared-task data distributed by \textsc{darpa}.
The RT'04 data covers part of the Switchboard corpus text, but was reanalysed
for segmentation and disfluency detection, with slightly different annotation
conventions.  Previous work has found it difficult to reconcile the RT annotations
with those provided by the Penn Treebank \citep{bies:06}.
We use
the Switchboard corpus for consistency with previous work on disfluency 
detection and parsing \citep{qian:13,rasooli:13,honnibal:14}.

We follow the pre-processing and dependency conversion steps described in
Section \ref{sec:swbd}: the text was lower-cased, partial words were removed,
and the phrase-structure trees were converted into projective-dependency parses
using the Stanford Dependency Converter \citep{stanford_deps}.
We use the standard train/dev/test split from \citet{Charniak01a}.
%Sections 2
%and 3 for training, and Section 4 divided into three held-out sections, the first
%of which is used for final evaluation.
We follow \citet{honnibal:14} in using the \sparseval \citep{sparseval}
metric to evaluate our parser, which measures the dependency
accuracy of words marked fluent in the gold-standard.
%Fluent words that the parser
%incorrectly marks as disfluent are considered parse errors.

We follow \citet{Johnson04a} and others in restricting our disfluency evaluation
to speech repairs, which we identify as words that have a node labelled \textsc{edited}
as an ancestor in the Switchboard phrase-structure trees.  We also follow them
in training only on the \textsc{mrg} files, giving us 619,236 words of training
data instead of the 1,482,845 used by other disfluency detection systems, such
as \citet{qian:13}.

We test for statistical significance in our results by training 20 models for
each experimental configuration, using different random seeds. The random seeds
control how the sentences are shuffled during training, which the perceptron
model is quite sensitive to.  We use the Wilcoxon rank-sums non-parametric test.
%The standard deviation in \textsc{uas} for a sample was typically around TODO,
%and TODO for disfluency $F$-measure.

The main hyper-parameter of our model is the beam-width.
Our beam is notably narrower than \citet{honnibal:14}, who employ a beam-width
of 64. We found that
only small accuracy improvements were obtained with beams wider than 8.
We use a beam-width of 12.

\subsection{Features}

We base our feature set on the arc-hybrid features used by
\citet{goldberg:13},
with the additional disfluency features used by \citet{honnibal:14}.  Our
specific feature templates are provided in the supplementary materials.

The templates refer to various combinations of the word-form, part-of-speech tag,
and the Brown cluster \citep{brown:92} of various tokens
in the context.  The tokens are the top three words of the stack, the left and
right subtree of the top word of the stack, the first three words of the buffer,
and the left subtree of the first word of the buffer.  Additionally, we follow
\citet{honnibal:14} in using the leftmost and rightmost edge of the top word
of the stack and the first word of the buffer as contextual tokens.
We used the Brown cluster mapping computed by \citet{liang:05}.
We follow \citet{honnibal:14}
in using 4- and 6-bit prefixes of the clusters in our feature templates, as
initially suggested by \citet{koo:10}.

\subsection{Qian and Liu (2013) Disfluency Detector}

Our parser performs disfluency detection jointly during parsing, using the Edit
transition described by \citet{honnibal:14}.  For comparison, we also trained
and evaluated the disfluency detection system of \citet{qian:13} on both
segmented and unsegmented input.

The \citet{qian:13} system uses a cascade of \textsc{m3n} sequence-tagging
models.  The first pass detects filler-words, the next pass uses the filler-word
predictions as features, and two subsequent passes detect disfluencies.
The system is a good comparison point because it achieves
equivalent, state-of-the-art accuracy to the \citet{honnibal:14} system,
with a very different algorithm.

Because the system does not require syntactically-annotated training data, it
can be trained from the \textsc{dps} files, which make more training data
available than the syntactically-annotated \textsc{mrg} files.  \citet{qian:13}
train and evaluate their model from the \textsc{dps} annotations.
However, to promote direct comparison against our syntactic disfluency detectors,
we trained the system from the \textsc{mrg} files, and followed the syntactic
disfluency annotations (i.e., words
were marked disfluent if they were part of the yield of an \textsc{edited} node).
Interestingly, this resulted in slightly higher accuracy than 
\citet{qian:13} report.  It seems that the \textsc{mrg} annotated speech repairs
are easier to detect than the \textsc{dps} annotations.

\subsection{CRF Segmenter}
\label{sec:crf}

To evaluate our joint model, we prepared a sequence-based segmentation system,
following the approach of \citet{liu:05}.  We used the \textsc{crf} implementation
provided by the Wapiti toolkit \citep{wapiti}.
%\footnote{\url{http://wapiti.limsi.fr/}}
The segmentation problem was modelled as a binary classification task, with
segment-initial
%\footnote{We
%found segment-initial encoding slightly better, due to the high frequency of
%conjunctions at the start of segments.}
tokens labelled 1 and all other tokens labelled 0.
Features referred to the word, part-of-speech tag, prefix, and suffix of the
target and surrounding tokens, as well as the previous two labelling decisions.
Weights were learned using the \textsc{l-bfgs} algorithm,
with an elastic-net penalty tuned on the development data.  The pattern file,
which describes the exact feature-templates, is attached in the supporting
materials.

\section{Development Results}

\label{sec:results}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrrr}
        \hline
               & \multicolumn{3}{c}{Segmentation} \\
               & Acc. & $P$ & $R$ & $F$ \\
        \hline
        CRF              & 96.6 & 87.4 & 78.0 & 82.4 \\
        Joint (explicit) & 96.9 & 84.0 & 85.9 & 84.9 \\
        Joint (implicit) & 96.9 & 83.9 & 86.2 & 85.0 \\
        \hline
    \end{tabular}
    \caption{\small Segmentation evaluation on the development set, for
    the CRF model and our joint approaches.\label{tab:sbd_eval}}
    \vspace*{-4em}
\end{table}

\subsection{Segmentation Accuracy}

Table \ref{tab:sbd_eval} compares the per-token segmentation accuracy of the
three systems, along with their precision, recall and $F$-measure at identifying
segment-initial words.  Low recall indicates under-segmentation, while low precision
indicates over-segmentation.  Our segmentation evaluation excludes disfluent words.
When a disfluent word begins a segment, we move its segment label back to the first
fluent word.

%We evaluate segmentation accuracy as a binary classification task, where each
%word is assigned a label
%indicating whether it is the first word in a segment.  The \textbf{CRF}
%model learns these labels explicitly.  The \textbf{Joint (explicit)} model
%learns to insert segment boundaries as part of the transition system. In
%the \textbf{Joint (implicit)} model, the segmentation is derived deterministically
%from the predicted dependency analysis.

The two syntactic systems achieve equivalent accuracy, with similar precision/recall
bias.  Because the parser is incremental, it is easy to capture the information
available to the \textsc{crf} system, with the added advantage of long-range
syntactic features.  This gives the parsing models a small but statistically
significant accuracy advantage.

\subsection{Disfluency Detection Accuracy}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr|rrr}
        & \multicolumn{3}{c|}{Segmented}  & \multicolumn{3}{c}{Unsegmented} \\
        System & $P$ & $R$ & $F$ & $P$ & $R$ & $F$ \\
        \hline \hline
        Qian \& Liu '13         & 90.6 & 80.7  & 85.3  & 83.8 & 76.5 & 80.0 \\
        Pipeline                & 92.4 & 76.8  & 84.1  & 81.9 & 73.6 & 77.6 \\ 
        Joint (explicit)        & \multicolumn{3}{c|}{n/a} & 88.4 & 68.6 & 77.2 \\
        Joint (implicit)        & \multicolumn{3}{c|}{n/a} & 88.3 & 68.9 & 77.5 \\
        \hline
   \end{tabular}
    \caption{\small Disfluency detection evaluation on the development set,
             with and without gold segment boundaries.
         \label{tab:dfl}}
%\vspace*{-4em}
\end{table}

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrr}
        \hline
        System & \textsc{uas} & \textsc{las} \\
        \hline \hline
        Gold $\rightarrow$ Parser & 90.9 & 88.0 \\
        \hline
        CRF $\rightarrow$  Parser & 86.2 & 83.4 \\
        Joint (explicit)          & 87.9 & 85.1 \\
        Joint (implicit)          & 88.1 & 85.3 \\
        \hline
    \end{tabular}
    \caption{\small Unlabelled and labelled (parse) attachment scores on the
        development data.
        \label{tab:parse}}
\vspace*{-4em}
\end{table}



Table \ref{tab:dfl} shows the accuracy of the systems at detecting speech-repair
disfluencies.  We first follow previous work in evaluating our model given
gold-standard segment boundaries (\textbf{Segmented}).  The \textbf{Pipeline}
system is the parsing model described in Section \ref{sec:model}, which is based
on the \citet{honnibal:14} model, but makes use of the arc-hybrid transition
system, with a feature set adjusted accordingly. We also train the model to
detect other disfluency types during parsing, by splitting the Edit
transition with different labels.

While \citet{honnibal:14} found that their system achieved slightly higher
accuracy than \citet{qian:13}, our model's accuracy is slightly lower.  It
may be that the Edit transition does not work as well with the arc-hybrid
system as it does with the arc-eager system that \citeauthor{honnibal:14} employ.
Alternatively, our feature set may be less well-tuned for disfluency detection.

When the gold-standard segment boundaries were replaced with predictions from the
\textsc{crf} model, precision fell substantially, with only a small decrease
in recall.  It seems that the parser dealt with segmentation errors by applying
the Edit transition, since the input was ungrammatical.  This suggests that there
may be a way to train the model to adjust to segmentation mistakes, should a
pipeline architecture prove desirable.

The two joint models achieved similar disfluency $F$-measure to the
pipeline system, but with higher precision, and lower recall.  
The differences in $F$-measure were not statistically significant. 
We suspect that
the syntactic disfluency models are sensitive to parse accuracy, which is reduced
in the absence of gold-standard segment boundaries.  However, the drop in performance
of the \citet{qian:13} system on unsegmented input, from 85.3 to 80.0 $F$-measure
suggests that the segment boundaries are also informative clues for disfluency
detection in their own right.

%We first compare our model to previous disfluency detection systems on the
%Switchboard development data, using gold-standard segmentation.  The model
%performs similarly to the state-of-the-art \citet{honnibal:14} system it is based
%on, demonstrating that our adoption of the arc-hybrid transition system did not
%affect accuracy. Both systems perform slightly better than the \citet{qian:13}
%sequence-based model.
%
%\textbf{Unsegmented.}
%We next trained and evaluated the \citet{qian:13} model on unsegmented text.
%The gold-standard segment boundaries turn out to be surprisingly informative
%for the disfluency detection task: without them, accuracy drops from 83.4\% to
%78.2\%.
%
%The drop in accuracy of the \citet{qian:13} system is important context
%for the performance of the joint model on unsegmented text, which see
%a similar drop in accuracy.  Without the Yang and Liu result, it would seem
%that the lack of segmentation disrupted the parse accuracy of the joint model,
%causing its loss of disfluency accuracy.  However, it seems that the gold-standard
%segment boundaries were of direct benefit for disfluency detection.
%
%\textbf{CRF segmentation.} 
%Finally, we compare the fully joint model to a pipeline system, where the input
%is pre-segmented using the \textsc{crf} model described in Section \ref{sec:crf}.
%This system achieved 76.3\% accuracy, slightly below the accuracy of the fully
%joint model.
%
\subsection{Parsing Accuracy}


Table \ref{tab:parse} shows labelled (\textsc{las}) and unlabelled
(\textsc{uas}) dependency accuracies on the development data.  With gold-standard
segmentation, the system achieves 90.9\%
\textsc{uas}, matching the state-of-the-art accuracy
reported by \citet{honnibal:14}.
When the same system was given input segmented by the \textsc{crf} model
described in Section \ref{sec:crf}, accuracy fell to 86.2\%.

Both of the joint models, \textbf{Joint (explicit)} and \textbf{Joint (implicit)},
achieve substantially higher parse accuracies than the pipeline system.
We attribute the pipeline system's loss of accuracy to error-propagation problems:
segmentation errors mean that the parser will be supplied ungrammatical input,
leading to low quality parses.
The joint models allow the parser to segment the input while assigning the
dependency parse, either by employing an explicit transition, or by simply
attaching segment-governors to the root node.  The two strategies yield similar
accuracy, with a small but statistically significant advantage to the implicit
segmentation.

\section{Final Evaluation}

Table \ref{tab:test} shows the final evaluation, using unsegmented inputs from
the test set.  We compare the implicit and explicit strategies for joint
segmentation and parsing against the pipeline system on segmentation,
speech-repair disfluency detection, and unlabelled parse accuracy.  
We also evaluate the state-of-the-art \citet{qian:13} disfluency detection system
on unsegmented input, and evaluate the efficiency of the systems, measured in
words per second.\footnote{All systems were run on a 2.4GHz Intel Xeon, with a
single thread.}

The two joint strategies performed similarly on all evaluations.
The difference in segmentation accuracy was statistically significant, while
the differences in parse accuracy (\textsc{uas}) and disfluency detection $F$-measure
were not.
We conclude that the way in which the segmentation decisions are encoded into the transition
system is relatively unimportant, so long as the decisions are made jointly during
parsing.

\begin{table}
    \centering
    \small
    \begin{tabular}{l|rrrr}
        \hline
        System & Seg. & Disfl. & \textsc{uas} & w/s \\
        \hline \hline
        Qian \& Liu '13          & ---  & 79.2 & ---  & 1,161 \\
        CRF $\rightarrow$ Parser & 96.7 & 76.4 & 85.9 & 948   \\
        Joint (explicit)         & 97.3 & 76.9 & 87.8 & 890   \\
        Joint (implicit)         & 97.2 & 76.7 & 87.9 & 893   \\
        \hline
    \end{tabular}
    \caption{\small Final evaluation scores, for segmentation, disfluency detection,
    parsing and efficiency, on unsegmented input.\label{tab:test}}
    \vspace*{-4em}
\end{table}



The advantage of joint modelling over the pipeline approach
(\textbf{CRF $\rightarrow$ Parser}) is clear, particularly
on the parsing evaluation, where the \textbf{Joint (implicit)} model achieved
2.0\% higher \textsc{uas}.

Interestingly, the joint models' small advantage in segmentation
accuracy over the \textsc{crf} system widened on the test data: on the development
data, the joint models scored 96.9\%, while the \textsc{crf} model scored 96.6\%.
On the test data, the best joint model segmented with 97.3\% accuracy, while the
\textsc{crf} model scored 96.7\%.  Both differences were statistically significant.

The differences in disfluency detection $F$-measure between the three syntactic
systems were not statistically significant.  The state-of-the-art \citet{qian:13}
system achieved significantly better accuracy.  We attribute this to the loss
of parse quality for the syntactic models on unsegmented input, which is linked
to their disfluency detection accuracy due to their joint approach.

Finally, we note that there is little loss in efficiency from parsing the unsegmented
input, and that the joint models are almost as efficient as the \citet{qian:13}
sequence-tagging system, which only performs disfluency detection.

\section{Related Work}

Our model draws directly on the work of \citet{honnibal:14}, who showed that
a joint transition-based model achieved superior results to a pipeline approach
for disfluency detection and parsing.  We describe a similar model, which we
extend to utterance segmentation.  They report 90.9\% \textsc{uas} and 85.8
disfluency detection $F$-measure on the development data, given gold-standard
segmentation.  Our model achieves 90.9\% and 84.1 on these evaluations.
We attribute the difference in disfluency detection to a lack of feature tuning
in our model.

\citet{rasooli:13} and \citet{rasooli:14} also describe joint transition-based
models of dependency parsing and disfluency detection.  However, their system
is limited to greedy search, and their disfluency detection transition operates
slightly differently.  The \citet{rasooli:14} system achieves 88.4\% \textsc{uas}
and 82.6\% disfluency $F$-measure, given gold-standard segmentation and \textsc{pos}
tags.  Although their system differs in several minor ways, it seems likely that
the biggest factor in their lower accuracy is the lack of beam-search.

%\citet{zhang_pos:11} show that the generalised perceptron with beam-search
%architecture could be successfully applied to a range of tasks, including
%joint word segmentation and \textsc{pos} tagging for Chinese.  Their results
%anticipate the recent interest in joint transtion-based dependency parsing
%models, such as the work of \citet{dongzhang:13}, who describe a joint
%transition-based model of dependency parsing and punctuation prediction.  We
%based our Break transition, described in Section \ref{sec:break}, on their
%work.  Although our models are similar, there are no directly comparable
%results, as \citet{dongzhang:13} did not investigate the effect of segmentation
%on parse accuracy, and did not apply their model to conversational speech.

%The main body of work on parsing unsegmented and disfluent speech data was
%conducted as part of the \textsc{darpa} \textsc{ears} rich transcription program,
%particularly the fall rich transcription (\textsc{rt04}) workshop.  The shared-task
%development and evaluation data were drawn from the
%Switchboard corpus, but were re-annotated for disfluencies and utterance segmentation.
%The \textsc{rt04} annotations proved
%difficult to reconcile with the Switchboard syntactic brackets \citep{bies:06}.
%These annotation difficulties contributed to low scores for the systems participating
%in the evaluation.

The impact of utterance segmentation on parse quality was investigated by
\citet{kahn:04}, in the context of a \textsc{pcfg} parsing model.  They compared
the effect on parse accuracy of three sentence segmentation systems: oracle segmentation,
an \textsc{hmm} system, and naive pause-based segmentation.  They showed that
the \textsc{hmm} segmentation system achieved a 7\% improvement in bracket
precision and recall over the baseline segmenter, and scored 5\% below the
oracle segmenter.

A weakness of the pipeline architecture employed by \citet{kahn:04} and others
is error-propagation, which arises because the earlier components forward only
a single hypothesis.  One way of mitigating this problem is re-ranking, which
\citet{Johnson04a,Johnson04b} used to improve disfluency detection, and
\citet{roark:06} used to improve utterance segmentation.  The re-ranking architecture
allows subsequent models to search some of the hypothesis space from earlier in the
pipeline, i.e. a disfluency or segmentation analysis can be selected in light
of the parse structure it permits.  We instead adopt a fully joint approach,
made tractable by recent advances in incremental parsing.

\section{Conclusion}

Segmentation and disfluency detection make speech parsing particularly
difficult, relative to understanding written text.  Both disfluency detection
and segmentation require syntactic features, but standard polynomial-time
parsing algorithms cannot be applied accurately before segmentation and disfluency
detection have been conducted.
We have shown that recent advances in transition-based parsing offer a solution
to this chicken-and-egg problem.  We model all three problems
jointly, so that the combined problem-space can be searched for a good
hypothesis. 

We demonstrate a 2\% improvement in dependency parse accuracy over a pipeline
approach.  Our results are the first to be published on this task, as the recent
work on speech parsing has used inputs with gold segmentation.  
Our model currently makes use of no acoustic features, which would be interesting
to explore for future work.  It would also be interesting to connect the model
to speech recognition output, to evaluate the effect of recognition errors on
parse accuracy, and to use the parser as a conditional language model, to reduce
 word error rates.

\bibliography{main}
\bibliographystyle{aclnat}


\end{document}
